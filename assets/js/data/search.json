[
  
  {
    "title": "Do We Really Have Free Will? A Journey Through Mind, Matter, and Meaning",
    "url": "/posts/do-we-really-have-free-will/",
    "categories": "Philosophy, Neuroscience",
    "tags": "free-will, philosophy, neuroscience, determinism, consciousness, Eastern-philosophy, Western-philosophy, compatibilism, cognitive-science, ethics",
    "date": "2025-10-12 14:30:00 +0545",
    





    
    "snippet": "Do We Really Have Free Will? A Journey Through Mind, Matter, and MeaningIntroduction: The Timeless QuestionFor centuries, philosophers, theologians, and scientists have wrestled with one of humanit...",
    "content": "Do We Really Have Free Will? A Journey Through Mind, Matter, and MeaningIntroduction: The Timeless QuestionFor centuries, philosophers, theologians, and scientists have wrestled with one of humanity’s most profound questions: Do we truly have free will? Are our choices genuinely our own, or are they merely inevitable outcomes of a vast chain of prior causes beyond our control?This isn’t just an abstract philosophical puzzle. The answer shapes how we think about responsibility, justice, morality, and what it means to be human. In modern times, advances in neuroscience and physics have added compelling new dimensions to this ancient debate, forcing us to reconsider what “freedom” really means.What Is Free Will?At its core, free will is the ability to make choices that aren’t predetermined by prior causes or external constraints. But as with most profound questions, the devil is in the details.Libertarian free will claims that humans possess genuine freedom—that in any given moment, we could have chosen differently. Our decisions aren’t merely the product of physics and chemistry, but of something more.Determinism takes the opposite view: every event, including our thoughts and decisions, follows inevitably from prior states of the universe. According to this view, if you could rewind time and replay a decision with everything exactly the same, you’d make the same choice every time.Compatibilism, championed by philosophers like David Hume and Daniel Dennett, offers a middle path. It suggests that free will can exist within a deterministic universe—as long as our actions align with our internal desires and reasoning, we’re free enough (Dennett, 1984; Hume, 1748).Philosophical Perspectives Across CulturesWestern Philosophy: The Struggle Between Freedom and FateWestern philosophical thought has long been dominated by the tension between freedom and determinism. Ancient Greek philosophers laid the groundwork for this debate, with some emphasizing rational choice and others pointing to the inexorable chain of causation.The Stoic philosophers developed a nuanced view: while they believed the universe unfolds according to rational necessity, they argued that humans can achieve freedom through understanding and accepting this order. True freedom, in their view, comes not from changing external circumstances but from mastering our internal responses (Epictetus, c. 135 CE).During the Enlightenment, the debate intensified. Philosophers like Spinoza argued for strict determinism—that everything, including human thought, follows necessarily from the nature of reality (Spinoza, 1677). Meanwhile, Kant proposed that humans exist in two realms: the phenomenal world of cause and effect, and the noumenal realm where free will operates beyond physical causation (Kant, 1785).Existentialist thinkers like Sartre took freedom to its extreme, arguing that humans are “condemned to be free”—that we have no fixed essence and must create ourselves through our choices, bearing the full weight of responsibility (Sartre, 1943).Eastern Philosophy: The Illusion of the Separate SelfEastern philosophical traditions approach free will from a fundamentally different angle, often questioning the very notion of an independent “self” that could possess free will.Many Eastern schools of thought emphasize the interconnectedness of all things. The concept of dependent origination suggests that every event arises from countless prior conditions, creating an intricate web of causation where nothing exists independently (Nāgārjuna, c. 150-250 CE). From this perspective, the question isn’t whether “you” have free will, but whether there’s a separate “you” at all.The doctrine of karma introduces another layer: actions create consequences that shape future circumstances, creating patterns that can feel deterministic. However, this isn’t fatalism—conscious awareness and deliberate action can gradually reshape these patterns. Liberation comes not from asserting free will, but from recognizing the processes that bind us (Vasubandhu, c. 4th-5th century CE).Zen traditions often sidestep the free will debate entirely, viewing it as a conceptual trap. The emphasis is on direct experience rather than philosophical analysis. When the illusion of a separate self dissolves through practice and insight, the question of whether “you” have free will becomes meaningless—there’s simply action arising naturally in response to circumstances (Dōgen, 1233).Both traditions ultimately suggest that ordinary human experience of “choosing” is more complex than it appears, though they arrive at this conclusion through different paths—Western philosophy through rigorous logical analysis, Eastern philosophy through contemplative insight into the nature of self and reality.The Case Against Free WillNeuroscientific EvidenceIn the 1980s, neuroscientist Benjamin Libet conducted experiments that shook the foundations of how we think about decision-making. His team detected brain activity—the “readiness potential”—occurring milliseconds before participants consciously decided to move their hands (Libet et al., 1983).The unsettling implication? Your brain appears to “decide” before “you” consciously do.Critics are quick to point out limitations: these experiments measured only simple motor movements, not the complex moral reasoning and deliberation that characterize our most meaningful choices. But the findings remain provocative and have spawned decades of follow-up research.The Problem of DeterminismIf every physical event in the universe follows naturally from the one before—like an unbroken chain of falling dominoes—then your decision to read this article was set in motion long before you were born. Under strict determinism, human choices are simply results of physical and chemical processes, no different from water flowing downhill (Laplace, 1814).This mechanistic view leaves little room for the kind of freedom we intuitively believe we possess.The Case for Free Will—or Something Like ItQuantum IndeterminacySome physicists have found hope in an unlikely place: quantum mechanics. At the subatomic level, the universe appears fundamentally random and unpredictable (Heisenberg, 1927). If not everything is predetermined, perhaps there’s room for genuine freedom to emerge.However, this argument has a critical weakness: randomness alone doesn’t equal free will. Random quantum fluctuations in your neurons wouldn’t make your choices any more “yours”—they’d just replace deterministic inevitability with chaotic unpredictability.The Conscious VetoInterestingly, even Libet offered a potential escape hatch. While unconscious brain activity may initiate decisions, he found evidence that we can consciously veto them before they’re executed (Libet, 1999). He called this “free won’t”—the capacity to stop an initiated action in its tracks.This small window of conscious control might be where human agency truly resides. We may not freely initiate all our impulses, but we can choose which ones to follow through on.Compatibilism: Redefining FreedomPerhaps we’ve been asking the wrong question all along. Compatibilists argue that free will isn’t about escaping causality—it’s about acting according to your own reasons, values, and desires (Dennett, 2003).If you did what you wanted to do, and could have done otherwise if you had wanted to, that’s sufficient for meaningful freedom. You don’t need to be uncaused; you just need to be self-caused in the right way.Why the Debate Never EndsThe free will debate persists because it sits at the intersection of science, philosophy, and lived experience, creating tensions that may never be fully resolved:Complexity: Consciousness, neuroscience, and quantum physics interact in ways we’re only beginning to understand. Each new discovery opens as many questions as it answers.Subjectivity: We feel like we make choices. That first-person experience is powerful and immediate, even when scientific evidence suggests it might be illusory.Ethics: Our entire system of morality, justice, and personal responsibility rests on the assumption that people make meaningful choices. If no one chooses freely, what justifies punishment or praise? What does personal growth even mean?My Perspective: A Middle GroundAfter exploring both sides, I find myself drawn to a nuanced position: perhaps absolute, metaphysical free will is an illusion—but functional free will is real enough to matter.Yes, we’re shaped by genetics, upbringing, brain chemistry, and countless factors beyond our control. But within those constraints, we still reason, reflect, deliberate, and choose. Our decisions may be influenced by the past, but they’re not simply dictated by it.In a world where the universe sets the stage, we still write our lines—even if the ink was mixed before we picked up the pen.ConclusionWhether free will is a comforting illusion or an emergent property of sufficiently complex brains, grappling with the question forces us to reflect deeply on what it means to be human. Perhaps freedom isn’t about escaping causality entirely—but about understanding it well enough to navigate it consciously and deliberately.The debate will continue, and that’s as it should be. Some questions are too important to ever fully answer.Suggested Reading  Dennett, D. C. (1984). Elbow Room: The Varieties of Free Will Worth Wanting. MIT Press.  Dennett, D. C. (2003). Freedom Evolves. Viking Press.  Dōgen, Z. (1233/2010). Shōbōgenzō: The Treasury of the True Dharma Eye. Numata Center for Buddhist Translation and Research.  Epictetus (c. 135 CE/1995). The Discourses. Everyman.  Harris, S. (2012). Free Will. Free Press.  Hume, D. (1748). An Enquiry Concerning Human Understanding.  Kant, I. (1785/1997). Groundwork of the Metaphysics of Morals. Cambridge University Press.  Laplace, P. S. (1814). A Philosophical Essay on Probabilities.  Libet, B. (2004). Mind Time: The Temporal Factor in Consciousness. Harvard University Press.  Libet, B. (1999). Do We Have Free Will? Journal of Consciousness Studies, 6(8-9), 47-57.  Libet, B., et al. (1983). Time of Conscious Intention to Act in Relation to Onset of Cerebral Activity (Readiness-Potential). Brain, 106(3), 623-642.  Nāgārjuna (c. 150-250 CE/1995). The Fundamental Wisdom of the Middle Way. Oxford University Press.  Sapolsky, R. M. (2017). Behave: The Biology of Humans at Our Best and Worst. Penguin Press.  Sartre, J. P. (1943/2003). Being and Nothingness. Routledge.  Spinoza, B. (1677/2000). Ethics. Oxford University Press.  Vasubandhu (c. 4th-5th century CE/1991). Abhidharmakośabhāṣyam (Vol. 1-4). Asian Humanities Press."
  },
  
  {
    "title": "The Science and Practice of Flow State for Software Developers",
    "url": "/posts/flow-state-for-developers-science-and-practice/",
    "categories": "Productivity, Psychology",
    "tags": "flow-state, deep-work, productivity, cognitive-science, developer-mindset",
    "date": "2025-09-06 06:10:00 +0545",
    





    
    "snippet": "The concept of “flow state”—that elusive mental zone where time seems to disappear and productivity soars—has fascinated psychologists and knowledge workers for decades. For software developers, ac...",
    "content": "The concept of “flow state”—that elusive mental zone where time seems to disappear and productivity soars—has fascinated psychologists and knowledge workers for decades. For software developers, achieving flow can mean the difference between a day of frustration and a day of breakthrough innovation. This article explores the neuroscience behind flow state, its particular relevance to software development, and practical techniques for cultivating it in our increasingly distracted world.Understanding Flow State: The Science Behind the MagicFlow state was first identified and named by psychologist Mihaly Csikszentmihalyi in the 1970s through his research examining people who performed activities for pure enjoyment, without external rewards. He described flow as a state of complete absorption where people experience being “in the zone” or “in the groove” - a highly focused mental state conducive to productivity. In the decades since its initial discovery, neuroscience has revealed what actually happens in our brains during flow.The Neurochemistry of FlowWhen we enter flow state, our brains undergo several significant changes:      Transient Hypofrontality: The prefrontal cortex—responsible for self-criticism, doubt, and self-consciousness—temporarily downregulates. This silencing of the inner critic allows us to act more intuitively and take creative risks.        Neurochemical Cascade: The brain releases a potent cocktail of performance-enhancing chemicals:          Dopamine improves pattern recognition and increases focus      Norepinephrine sharpens attention and speeds up reaction time      Anandamide elevates mood and enhances lateral thinking      Serotonin generates feelings of wellbeing      Endorphins mask physical discomfort and extend stamina            Alpha-Theta Wave Shift: EEG studies show that flow is characterized by a shift from fast-moving beta waves (normal waking consciousness) to the border between alpha waves (relaxed focus) and theta waves (dreamlike state), creating an optimal zone for creative problem-solving.  The Flow CycleResearch has revealed that flow typically follows a four-stage cycle:      Struggle: The initial phase of learning and information gathering, often accompanied by frustration and confusion.        Release: A period of mental relaxation where you step away from the problem, allowing your subconscious to work.        Flow: The state of peak performance where solutions emerge and implementation feels effortless.        Recovery: The necessary downtime after flow, as the neurochemicals that facilitate flow are depleted and need time to replenish.  Understanding this cycle is crucial—many developers try to force themselves directly into flow without going through the necessary struggle and release phases.Why Flow Matters Especially for DevelopersSoftware development presents a unique combination of challenges and characteristics that make flow state particularly valuable:Complexity ManagementModern software development requires holding complex mental models in working memory. Flow state enhances working memory capacity and pattern recognition, allowing developers to navigate complex codebases and architectural decisions more effectively.In flow state, a developer can more easily hold the entire execution context in mind, tracking variables and execution paths that would otherwise require constant reference to documentation or debuggers.The High Cost of Context SwitchingStudies have shown that it takes an average of 23 minutes to fully regain focus after an interruption. For developers, the cost is even higher due to the complex state that must be rebuilt in working memory.A 2021 study by the University of Zurich found that software developers lose up to 30% more productivity from interruptions compared to other knowledge workers, and require 10-15 minutes of refocusing time even after brief interruptions.Debugging and Problem-SolvingDebugging complex issues often requires following intricate causal chains across multiple systems. Flow state enhances pattern recognition and intuitive leaps that can lead to breakthrough moments in troubleshooting.Learning AccelerationThe rapid evolution of technologies, frameworks, and languages requires continuous learning. Flow state has been shown to accelerate skill acquisition by up to 490% according to research from the Flow Research Collective.The Flow State Prerequisites for DevelopersCsikszentmihalyi identified several conditions necessary for flow to occur. Here’s how they apply specifically to software development:1. Clear GoalsFlow thrives on clarity. Vague requirements or shifting priorities make flow nearly impossible to achieve.Developer Application: Break down complex tasks into clear, achievable units of work. Use techniques like:  Writing detailed task descriptions before coding  Creating a checklist of acceptance criteria  Setting specific implementation goals for each coding session2. Immediate FeedbackFlow requires knowing how you’re performing in real-time.Developer Application:  Use test-driven development to get immediate feedback on code correctness  Set up continuous integration for rapid feedback on integration issues  Leverage static analysis tools and linters for instant code quality feedback3. Balance Between Challenge and SkillFlow occurs in the sweet spot where the task is challenging enough to engage but not so difficult that it causes anxiety.Developer Application:  Deliberately take on tasks slightly beyond your current skill level  Break down complex problems into manageable components  Use the “Pomodoro Flow” technique: 25 minutes of focused work on a challenging but achievable task4. Deep ConcentrationFlow requires uninterrupted focus for extended periods.Developer Application:  Create a “focus environment” free from distractions  Use noise-canceling headphones and ambient sound  Block distracting websites and notifications during coding sessions  Schedule “no-meeting” blocks of at least 2-3 hoursPractical Techniques for Inducing Flow StateBased on the latest research and developer experiences, here are practical techniques to cultivate flow state in your development work:1. Environment DesignPhysical Environment:  Create a dedicated coding space that your brain associates with focused work  Ensure ergonomic comfort to minimize physical distractions  Consider using a standing desk to maintain energy levels  Use plants, natural light, and proper temperature control to optimize cognitive functionDigital Environment:  Customize your IDE to minimize cognitive load  Create project-specific profiles that load relevant tools and references  Use full-screen mode to eliminate visual distractions  Set up keyboard shortcuts for common operations to maintain flow2. Ritual and RoutineNeuroscience research shows that consistent pre-work rituals can trigger flow state more reliably:  Develop a consistent “pre-coding” ritual (e.g., brewing coffee, reviewing tasks, 5 minutes of meditation)  Schedule coding sessions at the same time each day to leverage your circadian rhythm  Use a specific playlist or ambient sound that your brain associates with flow3. The Flow Toolkit for DevelopersStruggle Phase Tools:  Mind mapping software for problem exploration  Rubber duck debugging to clarify thinking  Time-boxed research sprints (25-45 minutes)Release Phase Tools:  Physical activity (short walk, stretching)  Diffuse mode thinking activities (shower, dishes)  Meditation or breathwork (4-7-8 breathing technique)Flow Phase Tools:  Pomodoro technique modified for flow (45 minutes on, 10 minutes off)  IDE extensions that minimize disruption  Ambient background noise (rainy mood, coffee shop sounds)Recovery Phase Tools:  Journaling to capture insights  Proper hydration and nutrition  Brief social interactions4. Flow-Based Development MethodologyIntegrate flow principles into your development methodology:Flow-Based Development Cycle:1. Planning (Pre-Flow)   - Clear task definition   - Gather necessary resources   - Set specific success criteria2. Struggle (15-45 minutes)   - Research and exploration   - Problem definition   - Initial attempts3. Release (15-30 minutes)   - Physical movement   - Different mental activity   - No problem-solving4. Flow Session (60-90 minutes)   - Uninterrupted implementation   - No context switching   - Capture side thoughts without pursuing5. Recovery (30 minutes)   - Document progress   - Commit and push code   - Light review   - Physical and mental reset6. Repeat or ConcludeFlow Blockers in Modern Development EnvironmentsCertain aspects of modern development culture actively work against flow state. Recognizing and mitigating these blockers is essential:1. The Slack/Teams TrapInstant messaging platforms create an expectation of immediate response, fragmenting attention and preventing deep work.Solution: Implement communication protocols that respect focus time:  Set status to “In deep work” or “Coding session”  Batch communications during designated periods  Use asynchronous communication by default  Negotiate response time expectations with your team2. Meeting OverloadThe proliferation of meetings in agile environments can make sustained flow impossible.Solution: Protect your calendar ruthlessly:  Block 3-4 hour chunks for deep work  Push for “No Meeting Wednesdays” or similar team policies  Decline meetings without clear agendas or objectives  Suggest asynchronous alternatives when appropriate3. Notification AddictionThe dopamine hit from notifications creates a psychological dependency that disrupts flow.Solution: Implement notification hygiene:  Disable all non-critical notifications during flow sessions  Use “Do Not Disturb” mode on all devices  Create a separate user profile on your computer for deep work  Use tools like Freedom or Cold Turkey to block distracting sites4. Technical Debt and Codebase FrictionPoorly maintained codebases with high technical debt create constant cognitive friction that prevents flow.Solution: Systematically reduce friction:  Schedule regular refactoring sessions  Improve documentation and code organization  Invest in better tooling and faster tests  Address the most disruptive technical debt firstMeasuring and Tracking FlowTo improve your flow state capacity, consider tracking these metrics:  Flow Frequency: How often you achieve flow state (days per week)  Flow Duration: How long you maintain flow once achieved  Recovery Time: How long it takes to re-enter flow after interruption  Flow Triggers: Which activities or environments most reliably induce flowTools for tracking flow:  Flow journals (manual tracking)  Time tracking apps with focus labels (Toggl, RescueTime)  Productivity analytics (WakaTime for coding)  EEG headbands for advanced users (Muse, Neurosity)Flow State in Team ContextsWhile flow is often discussed as an individual phenomenon, it can also be cultivated at the team level:Team Flow Practices      Synchronized Deep Work: Schedule team-wide deep work sessions where everyone focuses on their tasks without interruption.        Communication Protocols: Establish clear guidelines for when and how to interrupt colleagues.        Flow-Friendly Meetings: Design meetings to respect and enhance flow:          Schedule meetings at the edges of the day (early morning or late afternoon)      Use agendas and timeboxing rigorously      Implement “No Laptop” policies to ensure full engagement            Collaborative Flow Sessions: For pair programming or collaborative problem-solving, use techniques like:          Pomodoro pairing (25 minutes of focused collaboration, 5-minute break)      Designated roles (driver/navigator) to maintain focus      Shared focus environment (same room, minimal distractions)      The Future of Flow in Software DevelopmentAs our understanding of neuroscience advances and development practices evolve, several trends are emerging in how developers approach flow state:1. Flow-Optimized ToolsDevelopment tools are increasingly being designed with flow state in mind:  IDEs with distraction-free modes and focus-enhancing features  AI pair programmers that maintain context without interruption  Flow-aware notification systems that adapt to your cognitive state2. Neurofeedback TrainingEmerging technologies allow developers to train their brains for faster flow state entry:  Consumer EEG devices that provide real-time brain state feedback  Neurofeedback apps designed specifically for knowledge workers  Flow state training programs based on brainwave entrainment3. Flow-Centric WorkplacesForward-thinking companies are redesigning workplaces and policies around flow:  Office designs with dedicated flow zones  Meeting policies that protect large blocks of focus time  Performance metrics that value deep work over activity metricsConclusion: Cultivating a Flow-Based Development PracticeFor software developers, flow state isn’t just a pleasant experience—it’s a competitive advantage in a field that demands creative problem-solving and complex cognitive work. By understanding the neuroscience of flow, designing your environment to support it, and systematically removing blockers, you can dramatically increase both your productivity and your enjoyment of coding.The most effective developers aren’t those who work the longest hours, but those who can reliably enter flow state and harness its cognitive benefits. As the pace of technological change continues to accelerate, the ability to achieve deep focus and creative flow will only become more valuable.Start by implementing one or two flow-enhancing practices from this article, track your results, and gradually build a development workflow that optimizes for this powerful mental state. Your code, your career, and your wellbeing will all benefit from the practice of flow.REFERENCEShttps://www.sciencedirect.com/topics/psychology/flow-theoryhttps://en.wikipedia.org/wiki/Mihaly_Csikszentmihalyihttps://positivepsychology.com/mihaly-csikszentmihalyi-father-of-flow/"
  },
  
  {
    "title": "Simplifying AI Integration in Ruby: Exploring the ruby_llm Gem",
    "url": "/posts/simplifying-ai-integration-ruby-llm-gem-guide/",
    "categories": "Ruby, LLM",
    "tags": "ruby, llm, ai, generative_ai",
    "date": "2025-08-01 12:13:00 +0545",
    





    
    "snippet": "Simplifying AI Integration in Ruby: Exploring the ruby_llm GemAs AI-powered applications become increasingly popular, developers are constantly looking for simple, unified ways to integrate Large L...",
    "content": "Simplifying AI Integration in Ruby: Exploring the ruby_llm GemAs AI-powered applications become increasingly popular, developers are constantly looking for simple, unified ways to integrate Large Language Models (LLMs) into their applications. If you’re a Ruby developer who’s tired of juggling multiple API clients for different AI providers, the ruby_llm gem might be exactly what you’ve been searching for.What is ruby_llm?The ruby_llm gem is a unified Ruby interface that allows you to interact with multiple AI providers through a single, consistent API. Instead of learning different SDKs for OpenAI, Anthropic, Google, and other providers, you can use one gem to rule them all.Supported ProvidersThe gem supports a wide range of AI providers, including:  OpenAI (GPT-3.5, GPT-4, GPT-4o)  Anthropic (Claude models)  Google (Gemini)  Cohere  Hugging Face  Ollama (for local models)  And many more!Getting StartedSetting up ruby_llm is refreshingly simple. Add it to your Gemfile:gem 'ruby_llm'Basic UsageHere’s how easy it is to get started with the gem:require 'ruby_llm'# Initialize a chat instancechat = RubyLLM.chat# Ask more complex questionsresponse = chat.ask(\"Hello, world! It's me Siv.\")puts response.content# =&gt; \"Hello, Siv! Great to hear from you. How can I assist you today?\"Advanced FeaturesProvider ConfigurationYou can easily switch between different AI providers:# Using OpenAI (default)chat = RubyLLM.chat(provider: :openai, model: 'gpt-4')# Using Anthropic's Claudechat = RubyLLM.chat(provider: :anthropic, model: 'claude-3-sonnet')# Using Google's Geminichat = RubyLLM.chat(provider: :google, model: 'gemini-pro')Code Generation ExampleOne of the impressive features I tested was code generation. Here’s an example:chat = RubyLLM.chatresponse = chat.ask(\"generate sample fibonacci ruby code example\")puts response.contentThe gem returned a complete, well-documented Fibonacci implementation:# Ruby program to generate Fibonacci sequence up to a certain number of termsdef fibonacci(n)  sequence = []  a, b = 0, 1  n.times do    sequence &lt;&lt; a    a, b = b, a + b  end  sequenceend# Specify the number of Fibonacci numbers to generatenum_terms = 10puts \"Fibonacci sequence with #{num_terms} terms:\"puts fibonacci(num_terms).join(', ')Configuration with RailsIf you’re using Rails, you can easily configure the gem with your API credentials stored in Rails credentials:Step 1: Add your API key to Rails credentialsEDITOR=\"nano\" rails credentials:editAdd your OpenAI API key:OPENAI_API_KEY: your_api_key_hereStep 2: Configure in an initializerCreate config/initializers/ruby_llm.rb:RubyLLM.configure do |config|  config.openai_api_key = Rails.application.credentials.OPENAI_API_KEY  config.anthropic_api_key = Rails.application.credentials.ANTHROPIC_API_KEY  # Add other provider keys as neededendReal-World Use Cases1. Content Generationchat = RubyLLM.chatblog_post = chat.ask(\"Write a technical blog post introduction about Ruby on Rails\")2. Code Review Assistantcode_to_review = \"def calculate(a, b); a + b; end\"review = chat.ask(\"Review this Ruby code and suggest improvements: #{code_to_review}\")3. Data Analysis Helperdata_question = \"Explain the trends in this CSV data: #{csv_data}\"analysis = chat.ask(data_question)4. Customer Support Automationcustomer_query = \"How do I reset my password?\"support_response = chat.ask(\"Provide a helpful customer support response: #{customer_query}\")Why Choose ruby_llm?1. Unified InterfaceNo need to learn multiple APIs. One consistent interface works across all providers.2. Easy Provider SwitchingTest different models and providers without rewriting your code.3. Ruby-First DesignBuilt specifically for Ruby developers, following Ruby conventions and best practices.4. Minimal DependenciesLightweight and doesn’t bloat your application.5. Production ReadyHandles errors gracefully and includes proper logging.Best Practices1. Environment-Specific Configuration# config/environments/development.rbconfig.ruby_llm_provider = :openai# config/environments/production.rb  config.ruby_llm_provider = :anthropic2. Response Handlingbegin  response = chat.ask(\"Your question here\")  if response.success?    puts response.content  else    puts \"Error: #{response.error}\"  endrescue =&gt; e  Rails.logger.error \"LLM Error: #{e.message}\"end3. Caching Responsesdef cached_ai_response(question)  Rails.cache.fetch(\"ai_response_#{Digest::MD5.hexdigest(question)}\", expires_in: 1.hour) do    chat = RubyLLM.chat    chat.ask(question).content  endend"
  },
  
  {
    "title": "Rails Query Optimization guide",
    "url": "/posts/rails-query-optimization-guide/",
    "categories": "Ruby on Rails, Performance",
    "tags": "ruby on rails, optimization, query, performance, profiling, benchmark",
    "date": "2025-07-31 05:31:00 +0545",
    





    
    "snippet": "Rails Query Optimization: Complete Guide to Database PerformanceDatabase query optimization is the cornerstone of building high-performance Ruby on Rails applications. Poor query patterns can trans...",
    "content": "Rails Query Optimization: Complete Guide to Database PerformanceDatabase query optimization is the cornerstone of building high-performance Ruby on Rails applications. Poor query patterns can transform a responsive application into a sluggish, resource-hungry system that frustrates users and drains infrastructure budgets. This comprehensive guide explores proven techniques, advanced strategies, and modern tools to optimize your Rails queries for maximum performance.Why Query Optimization MattersPerformance ImpactOptimized queries directly translate to faster response times, reduced server load, and improved user experience. A single poorly written query can increase page load times from milliseconds to seconds.Cost EfficiencyEfficient queries minimize database resource consumption, reducing infrastructure costs. By specifying the exact columns you require, you minimize the data transferred between the database and your Ruby on Rails application, leading to significant cost savings in cloud environments.Scalability FoundationWell-optimized queries ensure your application scales gracefully as data volume grows, preventing performance degradation that often forces expensive infrastructure upgrades.Understanding Active Record Query FundamentalsActive Record provides an elegant interface for database operations, but its convenience can mask performance implications. Understanding how your Ruby code translates to SQL is essential for optimization.The Hidden Cost of Convenience# This innocent-looking code can be expensiveusers = User.allusers.each { |user| puts user.name }This pattern loads all user records into memory, which becomes problematic as your user base grows.Core Optimization Techniques1. Selecting Only Required DataOne of the most impactful optimizations is retrieving only necessary columns.Problem:# Loads all columns, including potentially large text fieldsusers = User.allusers.each { |user| puts user.name }Solution:# Loads only the name column, reducing memory usage significantlynames = User.pluck(:name)# For multiple columns while maintaining Active Record objectsusers = User.select(:id, :name, :email)When to use .pluck vs .select:  Use .pluck when you need raw values and don’t require Active Record methods  Use .select when you need Active Record objects but want to limit columns2. Mastering Eager Loading to Eliminate N+1 QueriesN+1 queries are among the most common performance killers in Rails applications.The N+1 Problem:# This creates 1 + N queries (1 for users, N for each user's posts)users = User.allusers.each do |user|  puts \"#{user.name} has #{user.posts.count} posts\"endSolutions:Using .includes (Smart Loading)# Rails chooses the best loading strategy automaticallyusers = User.includes(:posts)users.each do |user|  puts \"#{user.name} has #{user.posts.size} posts\" # Uses .size, not .countendUsing .preload (Separate Queries)# Forces separate queries - better for memory usageusers = User.preload(:posts)Using .eager_load (LEFT JOIN)# Forces a single LEFT JOIN query - better when you need to add conditionsusers = User.eager_load(:posts).where(posts: { published: true })Key Differences:  preload initiates two queries, the first to fetch the primary model and the second to fetch associated models whereas eager_load does a left join which initiates one query to fetch both primary and associated models  includes - By default works like preload, but in some cases it will behave like eager_load, normally when you are also adding some conditions to the query3. Efficient Batch ProcessingWhen processing large datasets, loading all records into memory can cause performance issues and memory exhaustion.Problem:# Loads all users into memory at onceUser.all.each do |user|  UserMailer.newsletter(user).deliver_nowendSolution:# Processes users in batches of 1000 (default)User.find_each do |user|  UserMailer.newsletter(user).deliver_nowend# Custom batch sizeUser.find_each(batch_size: 500) do |user|  # Process userend# For batch processing with array accessUser.find_in_batches(batch_size: 1000) do |batch|  batch.each { |user| process_user(user) }end4. Smart Existence ChecksChecking for record existence efficiently can significantly impact performance.Inefficient:# Loads the entire object just to check existenceif User.where(email: 'test@example.com').present?  # Handle existing userendEfficient:# Only checks existence without loading the objectif User.where(email: 'test@example.com').exists?  # Handle existing userend5. Strategic Database IndexingIndexes are crucial for query performance, especially on frequently queried columns.Basic Index Creation:# In a migrationclass AddIndexToUsers &lt; ActiveRecord::Migration[7.0]  def change    add_index :users, :email    add_index :users, :created_at  endendComposite Indexes:# For queries that filter on multiple columnsadd_index :users, [:active, :created_at]add_index :posts, [:user_id, :published_at]Partial Indexes:# Index only active usersadd_index :users, :email, where: \"active = true\"When to Add Indexes:  Columns used in WHERE clauses  Foreign keys for associations  Columns used in ORDER BY and GROUP BY  Frequently joined columnsAdvanced Optimization Strategies1. Query Caching with Rails.cacheCaching expensive query results can dramatically improve performance for frequently accessed data.Basic Query Caching:def featured_posts  Rails.cache.fetch('featured_posts', expires_in: 1.hour) do    Post.includes(:author, :tags)        .where(featured: true)        .order(created_at: :desc)        .limit(10)        .to_a  endendCache Invalidation:class Post &lt; ApplicationRecord  after_save :clear_featured_cache  after_destroy :clear_featured_cache  private  def clear_featured_cache    Rails.cache.delete('featured_posts') if featured?  endendDynamic Cache Keys:def user_posts(user_id)  cache_key = \"user_posts/#{user_id}/#{User.find(user_id).updated_at.to_i}\"  Rails.cache.fetch(cache_key, expires_in: 30.minutes) do    User.find(user_id).posts.published.includes(:tags).to_a  endend2. Memoization for Request-Level CachingMemoization prevents redundant queries within a single request cycle.class PostsController &lt; ApplicationController  private  def featured_posts    @featured_posts ||= Post.featured.includes(:author).limit(5)  end  def popular_tags    @popular_tags ||= Tag.joins(:posts)                         .group('tags.id')                         .order('COUNT(posts.id) DESC')                         .limit(10)  endend3. Counter Caches for Association CountsCounter caches eliminate the need for expensive COUNT queries.Setup:# In the migrationclass AddPostsCountToUsers &lt; ActiveRecord::Migration[7.0]  def change    add_column :users, :posts_count, :integer, default: 0        # Reset existing counts    User.reset_counters(:posts)  endend# In the modelclass Post &lt; ApplicationRecord  belongs_to :user, counter_cache: trueendclass User &lt; ApplicationRecord  has_many :postsendUsage:# Instead of this expensive queryuser.posts.count # Triggers COUNT query# Use the cached counteruser.posts_count # No database queryCustom Counter Cache Names:class Comment &lt; ApplicationRecord  belongs_to :post, counter_cache: :comments_countend4. Understanding Joins vs IncludesDifferent loading strategies serve different purposes and have distinct performance characteristics.Inner Join with .joins:# Only loads users who have posts, single queryusers_with_posts = User.joins(:posts).distinctLeft Join with .left_joins:# Loads all users, including those without postsall_users = User.left_joins(:posts).distinctCombining Strategies:# Efficient: Join to filter, then eager load associationsactive_users_with_data = User.joins(:posts)                             .where(posts: { published: true })                             .includes(:profile, :posts)                             .distinctPerformance Monitoring and Detection Tools1. Development ToolsBullet Gem - N+1 Query Detection# Gemfilegroup :development do  gem 'bullet'end# config/environments/development.rbconfig.after_initialize do  Bullet.enable = true  Bullet.alert = true  Bullet.bullet_logger = true  Bullet.rails_logger = trueendProsopite - Lightweight Alternative# Gemfilegroup :development do  gem 'prosopite'end# ConfigurationProsopite.finish = trueProsopite.rails_logger = trueRack Mini Profiler - Real-time Performance# Gemfilegem 'rack-mini-profiler'# Shows performance overlay in browser# Identifies slow queries and memory usage automatically2. Production MonitoringScoutAPM - Comprehensive Performance Tracking# Gemfilegem 'scout_apm'# Provides:# - SQL query breakdown with execution times# - Historical performance trends  # - Memory leak detection# - Endpoint-specific analysisSkylight - Query Performance Insights# Gemfile  gem 'skylight'# Features:# - Endpoint-specific query analysis# - Detailed timing breakdowns# - Historical performance metrics3. Query Analysis and Memory ProfilingEXPLAIN for Query Analysis# Analyze execution plansputs User.joins(:posts).where(active: true).explain# Look for:# - \"Using index\" (good) vs \"Using filesort\" (add index)# - High row counts (optimize query)# - \"Using temporary\" (refactor complex queries)Memory Profiler for Leak Detection# Gemfilegem 'memory_profiler'# Usagerequire 'memory_profiler'report = MemoryProfiler.report do  Post.includes(:comments, :tags).limit(100).to_aendreport.pretty_print# Analyze output for high Active Record allocationsBatch Processing for Memory Efficiency# Bad - loads all into memoryUser.includes(:posts).each { |user| process_user(user) }# Good - processes in batchesUser.includes(:posts).find_in_batches(batch_size: 100) do |batch|  batch.each { |user| process_user(user) }end# Even better - select only needed columnsUser.select(:id, :name).find_each { |user| process_user(user) }Advanced Indexing Strategies1. Composite IndexesMulti-column indexes optimize queries filtering on multiple columns. Place the most selective column first.# Migrationadd_index :orders, [:user_id, :created_at]add_index :posts, [:user_id, :status, :published_at]# Optimizes queries like:Order.where(user_id: 1).order(created_at: :desc)Post.where(user_id: 1, status: 'published')2. Partial IndexesIndex only rows matching specific conditions to reduce index size and improve write performance.# Index only active usersadd_index :users, :email, unique: true, where: \"active = true\"# Index only published postsadd_index :posts, :created_at, where: \"status = 'published'\"# Usage - index is used automaticallyUser.where(active: true, email: \"user@example.com\")3. JSONB Indexes (PostgreSQL)For applications using JSON data, JSONB indexes dramatically improve query performance.# GIN index for JSONB columnsadd_index :products, :metadata, using: :gin# Expression index for specific JSON keysadd_index :products, \"(metadata-&gt;&gt;'category')\", using: :btree# UsageProduct.where(\"metadata-&gt;&gt;'category' = ?\", 'Electronics')Product.where(\"metadata @&gt; ?\", { brand: 'Apple' }.to_json)4. Covering IndexesInclude all columns needed for a query to avoid table lookups.# Covers SELECT id, title WHERE user_id = ? ORDER BY created_atadd_index :posts, [:user_id, :created_at], include: [:id, :title]# Query uses index-only scanPost.select(:id, :title)    .where(user_id: 1)    .order(:created_at)Advanced Query Refactoring1. Efficient SubqueriesUse subqueries to filter data before expensive operations.# Instead of joining large tablesUser.joins(:orders).where(orders: { status: 'completed' })# Use subquery to pre-filtercompleted_orders = Order.select(:user_id).where(status: 'completed')User.where(id: completed_orders)2. Query Merging for DRY CodeReuse scope logic with .merge() for maintainable queries.class Comment &lt; ApplicationRecord  scope :approved, -&gt; { where(approved: true) }  scope :recent, -&gt; { where('created_at &gt; ?', 1.week.ago) }end# Instead of duplicating conditionsPost.joins(:comments).where(comments: { approved: true })# Reuse existing scopesPost.joins(:comments).merge(Comment.approved.recent)3. Optimized Column SelectionAlways specify needed columns to reduce memory usage and transfer time.# Bad - loads all columnsposts = Post.where(published: true)titles = posts.map(&amp;:title)# Good - loads only needed datatitles = Post.where(published: true).pluck(:title)# For multiple columns maintaining objectsposts = Post.select(:id, :title, :excerpt).where(published: true)Advanced Database Strategies1. Database Views with Scenic GemCreate reusable complex queries as database views using the Scenic gem.# Gemfilegem 'scenic'# Generate viewrails generate scenic:view active_post_stats# db/views/active_post_stats_v01.sqlSELECT p.id,       p.title,       p.user_id,       COUNT(c.id) as comments_count,       AVG(c.rating) as avg_ratingFROM posts pLEFT JOIN comments c ON c.post_id = p.idWHERE p.status = 'published'GROUP BY p.id, p.title, p.user_id;# Use in Railsclass ActivePostStat &lt; ApplicationRecord  self.table_name = 'active_post_stats'  # Read-only modelend# Query like any modelpopular_posts = ActivePostStat.where('comments_count &gt; 10')2. Materialized Views for Heavy ComputationsStore expensive query results physically for better performance.# Create materialized viewclass CreateUserStatsView &lt; ActiveRecord::Migration[7.0]  def up    execute &lt;&lt;-SQL      CREATE MATERIALIZED VIEW user_engagement_stats AS      SELECT u.id,             u.name,             COUNT(DISTINCT p.id) as posts_count,             COUNT(DISTINCT c.id) as comments_count,             AVG(p.views_count) as avg_post_views      FROM users u      LEFT JOIN posts p ON p.user_id = u.id      LEFT JOIN comments c ON c.user_id = u.id      GROUP BY u.id, u.name;            CREATE UNIQUE INDEX ON user_engagement_stats (id);    SQL  end  def down    execute \"DROP MATERIALIZED VIEW user_engagement_stats\"  endend# Refresh periodically (in background job)class RefreshStatsJob &lt; ApplicationJob  def perform    ActiveRecord::Base.connection.execute(      \"REFRESH MATERIALIZED VIEW user_engagement_stats\"    )  endend2. Scopes for Reusable Query LogicCreate maintainable, reusable query patterns with scopes.class Post &lt; ApplicationRecord  scope :published, -&gt; { where(published: true) }  scope :recent, -&gt; { where('created_at &gt; ?', 1.week.ago) }  scope :popular, -&gt; { where('views_count &gt; ?', 100) }  scope :by_author, -&gt;(author) { where(author: author) }    # Chainable scopes  scope :trending, -&gt; { published.recent.popular }end# Usagetrending_posts = Post.trending.includes(:author, :comments)3. Raw SQL for Complex QueriesSometimes raw SQL is more efficient than Active Record.class AnalyticsQuery  def self.user_engagement_stats    sql = &lt;&lt;-SQL      SELECT         DATE_TRUNC('day', created_at) as date,        COUNT(*) as posts_count,        COUNT(DISTINCT user_id) as active_users      FROM posts       WHERE created_at &gt;= ?       GROUP BY DATE_TRUNC('day', created_at)      ORDER BY date DESC    SQL        ActiveRecord::Base.connection.exec_query(      sql,       'User Engagement Stats',       [30.days.ago]    )  endendQuery Optimization ChecklistPre-Development Setup  Install Bullet gem for N+1 detection  Set up Prosopite for lightweight query monitoring  Configure Rack Mini Profiler for development insightsCode Review Checklist  Check for N+1 queries - use .includes, .preload, or .eager_load  Verify selective column loading with .select or .pluck  Add indexes for WHERE, JOIN, ORDER BY columns  Implement counter caches for association counts  Use .find_each for large dataset processing  Cache expensive queries with appropriate expiration  Utilize scopes for reusable query logicDatabase Optimization  Add composite indexes for multi-column queries  Create partial indexes for conditional queries  Consider JSONB indexes for JSON column queries  Implement covering indexes for select-heavy queries  Use database views for complex, repeated queries  Set up materialized views for expensive aggregationsProduction Monitoring  Implement APM tools (ScoutAPM, Skylight, or similar)  Monitor slow query logs regularly  Set up database performance alerts  Track query performance trends over time  Configure query timeout settings  Regular EXPLAIN analysis of complex queriesPerformance Testing  Benchmark queries with realistic data volumes  Profile memory usage during peak loads  Test with production-like database sizes  Validate caching strategies under load  Measure response times before/after optimizationsPerformance Testing StrategiesBenchmarking Queriesrequire 'benchmark'def benchmark_query_optimization  Benchmark.bm(20) do |x|    x.report(\"Without includes:\") do      users = User.limit(100)      users.each { |user| user.posts.count }    end        x.report(\"With includes:\") do      users = User.includes(:posts).limit(100)      users.each { |user| user.posts.size }    end        x.report(\"With counter cache:\") do      users = User.limit(100)      users.each { |user| user.posts_count }    end  endendProduction-Like Testing# Use production database seeds for realistic testingclass DatabaseSeeder  def self.seed_realistic_data    User.create_batch(10_000) do |user, index|      {        name: \"User #{index}\",        email: \"user#{index}@example.com\",        posts_count: rand(0..50)      }    end  endendCommon Anti-Patterns to Avoid1. Query in Loops# Bad: N+1 queriesusers.each do |user|  puts User.find(user.id).name # Unnecessary queryend# Good: Use loaded objectsusers.each do |user|  puts user.name # No additional queryend2. Loading Unnecessary Associations# Bad: Loads all associationsusers = User.includes(:posts, :comments, :profile)users.each { |user| puts user.name } # Only using name# Good: Load only what you needusers = User.select(:name)3. Inefficient Counting# Bad: Loads all records to countUser.where(active: true).to_a.count# Good: Database-level countingUser.where(active: true).countConclusionQuery optimization is an ongoing process that requires understanding your application’s data access patterns, monitoring performance metrics, and applying the right techniques for each scenario. Start with the fundamentals—eliminating N+1 queries, adding strategic indexes, and selecting only necessary data. Then gradually implement advanced techniques like caching, counter caches, and raw SQL optimization where appropriate.Remember that premature optimization can be counterproductive. Make sure to evaluate the real performance bottlenecks and optimize where it matters most. Regularly monitoring and profiling your Rails application, along with familiarizing yourself with traffic metrics, can help you identify the areas that will provide the greatest performance improvements.The key to successful query optimization lies in continuous monitoring, testing, and iterative improvement. Build performance considerations into your development workflow, and your Rails applications will scale efficiently while providing excellent user experiences."
  },
  
  {
    "title": "Building Modern Rails Apps in 2025: Evolving with the Rails 8 Ecosystem",
    "url": "/posts/building-modern-rails-apps-2025-ecosystem/",
    "categories": "Ruby on Rails",
    "tags": "ruby on rails, webapp, rails8",
    "date": "2025-07-31 05:31:00 +0545",
    





    
    "snippet": "🚀 Building Modern Rails Apps in 2025: Evolving with the Rails 8 EcosystemRails 8 isn’t just another version update—it’s a thoughtful evolution that keeps the framework’s beloved simplicity while em...",
    "content": "🚀 Building Modern Rails Apps in 2025: Evolving with the Rails 8 EcosystemRails 8 isn’t just another version update—it’s a thoughtful evolution that keeps the framework’s beloved simplicity while embracing modern web development realities. Let’s explore how Rails continues to deliver developer happiness in 2025.1. From Request to Response: Rails 8 in MotionThe journey of a request through Rails 8 remains elegantly straightforward:Browser → Router → Controller → Model → Views/Turbo/API → ResponseThis familiar flow still leverages the time-tested MVC architecture, but now it’s supercharged with Hotwire and Turbo rendering capabilities. The beauty? Your mental model stays the same, but your apps get significantly more powerful.2. Still MVC at the Core — But Sharper Than EverRails doubles down on MVC because it works. This isn’t stubbornness—it’s wisdom gained from years of building maintainable applications:  Model: Your data and business logic sanctuary (hello, ActiveRecord!)  View: Clean HTML/JSON output via ERB, HAML, or the new Turbo Streams  Controller: The diplomatic coordinator handling requests with graceExample: Basic PostsController# app/controllers/posts_controller.rbclass PostsController &lt; ApplicationController  def index    @posts = Post.all  endendSimple, readable, and it just works. Sometimes the old ways are the best ways.3. UI Revolution: Hotwire + Turbo in ActionHere’s where Rails 8 gets exciting. Remember when building interactive UIs meant drowning in JavaScript? Those days are over.Turbo (Frames + Streams) delivers reactive interfaces using server-rendered HTML. It’s like having your cake and eating it too—rich interactivity without the client-side complexity headaches.Example: Live Updating with Turbo Stream&lt;!-- app/views/posts/index.turbo_stream.erb --&gt;&lt;%= turbo_stream.append \"posts\", partial: \"posts/post\", locals: { post: @post } %&gt;This single line delivers real-time UI updates. No React boilerplate, no state management nightmares—just elegant server-driven interactivity.Pro Tip: The Turbo-First MindsetBefore reaching for that JavaScript framework, ask yourself: “Can Turbo handle this?” Nine times out of ten, the answer is yes.4. Autoloading Magic: Zeitwerk by DefaultGone are the days of manual require statements cluttering your code. Rails 8’s full adoption of Zeitwerk means your classes load automatically based on file naming conventions.Example: Deep Folder Structure Autoload# app/services/user/notifier.rbmodule User  class Notifier    def self.send_welcome_email(user)      # Your logic here    end  endendRails finds it, loads it, and gets out of your way. It’s like having a helpful assistant who never asks for recognition.5. Backend Muscle: Background Jobs SimplifiedEmail sending, data processing, third-party API calls—these shouldn’t block your users. ActiveJob with adapters like Sidekiq or GoodJob handles async work seamlessly.Example: Newsletter Job# app/jobs/send_newsletter_job.rbclass SendNewsletterJob &lt; ApplicationJob  queue_as :default  def perform(user)    NewsletterMailer.weekly(user).deliver_later  endendCritical tasks stay fast, heavy lifting happens in the background. Your users stay happy, your servers stay responsive.6. Minimal by Default: Rails as an API ServerBuilding a mobile app or need a backend for your React frontend? Rails 8 has you covered:rails new myapp --apiThis streamlined setup gives you:  Lightweight middleware stack  JSON-first rendering  No unnecessary cookies or session managementIt’s Rails, but dressed for API duty.7. Scaling Up: Multi-DB SupportWhen your app grows beyond a single database, Rails 8 doesn’t make you jump through hoops.Example: database.yml for Multiple Rolesproduction:  primary:    database: app_primary  replica:    database: app_replicaSwitch database contexts with ease:ActiveRecord::Base.connected_to(role: :reading) do  Post.firstendRead replicas, multiple databases, database sharding—Rails 8 scales with your ambitions.8. Fortified Secrets: Encrypted Per-Environment CredentialsSecurity isn’t an afterthought in Rails 8. Per-environment encrypted credentials keep your secrets actually secret:EDITOR=\"code --wait\" bin/rails credentials:edit --environment productionAccess them securely:Rails.application.credentials.dig(:aws, :access_key_id)No more .env files accidentally committed to Git. Security by design, not by accident.9. Architecture Guidelines: The Smart Practices of 2025Modern Rails teams embrace these patterns for better maintainability:  ✅ Service Objects – Business logic deserves its own home  ✅ Form Objects – Complex forms need structure  ✅ Presenters/Decorators – Keep view logic organized  ✅ ViewComponents – Reusable, testable UI building blocks  ✅ Turbo-first – Default to server-rendered interactivityWhy These Patterns MatterThey’re not just trendy—they solve real problems. Service objects prevent fat controllers, ViewComponents make testing UI logic possible, and Turbo-first keeps your JavaScript bundle lean.10. The Three S’s: Queue, Cache, CableEvery resilient Rails 8 application rests on these foundational pillars:  Solid Queue → Reliable background job processing with ActiveJob + Sidekiq/GoodJob  Solid Cache → Smart caching with Redis/Memory, featuring Russian Doll and fragment caching  Solid Cable → Real-time features via ActionCable and Turbo StreamsTogether, these create the infrastructure for modern, responsive applications that users love.📦 Why Rails Still Wins in 2025Rails 8 proves that good ideas don’t go out of style—they just get better:  🧠 Clean architecture (MVC continues to deliver)  ⚡ Hotwire magic (Interactive UIs without JavaScript complexity)  🔄 Real-time built-in (Live updates feel natural)  🧰 Intelligent autoloading (Zeitwerk just works)  🧵 Background job mastery (Async processing made simple)  🔐 Security by default (Encrypted credentials protect you)  🏗️ Scale when ready (Multi-DB support and API-only mode)Rails 8 isn’t trying to be everything to everyone. Instead, it’s perfecting what it does best: helping developers build amazing web applications quickly, safely, and joyfully. In a world obsessed with the next shiny framework, Rails 8 proves that thoughtful evolution beats revolutionary chaos every time."
  },
  
  {
    "title": "From Server Headaches to Serverless Success: Building APIs That Scale with Lambda and API Gateway",
    "url": "/posts/serverless-compute-aws-lambda-and-api-gateway/",
    "categories": "Serverless, AWS, API, LambdaFunction",
    "tags": "serverless, aws, api-gateway, api, lambda",
    "date": "2025-07-28 22:45:00 +0545",
    





    
    "snippet": "Serverless computing has revolutionized how we build and deploy applications. AWS Lambda, combined with API Gateway, creates a powerful duo that eliminates server management while delivering scalab...",
    "content": "Serverless computing has revolutionized how we build and deploy applications. AWS Lambda, combined with API Gateway, creates a powerful duo that eliminates server management while delivering scalable, cost-effective solutions.Why Serverless MattersGone are the days of provisioning servers, managing infrastructure, or worrying about scaling. With Lambda, you write code, deploy it, and AWS handles everything else. You only pay for what you use - down to the millisecond.The Perfect PartnershipAWS Lambda executes your code in response to events, while API Gateway acts as the front door, handling HTTP requests and routing them to your Lambda functions. Together, they create REST APIs that can handle thousands of concurrent requests without breaking a sweat.Real-World Example: Text Analyzer APII recently built a Ruby-based text analyzer that demonstrates this partnership perfectly:require 'json'def lambda_handler(event:, context:)  begin    # Parse the incoming request    body = parse_request_body(event)    text = body['text'] || ''        # Validate input    if text.empty?      return error_response(400, \"Text input is required\")    end        if text.length &gt; 5000      return error_response(400, \"Text too long (max 5000 characters)\")    end        # Perform text analysis    analysis = analyze_text(text)        # Return successful response    {      statusCode: 200,      headers: {        'Content-Type' =&gt; 'application/json',        'Access-Control-Allow-Origin' =&gt; '*',        'Access-Control-Allow-Headers' =&gt; 'Content-Type',        'Access-Control-Allow-Methods' =&gt; 'POST, OPTIONS'      },      body: JSON.generate({        success: true,        input_text: text[0..100] + (text.length &gt; 100 ? '...' : ''),        analysis: analysis,        processed_at: Time.now.utc.iso8601      })    }      rescue JSON::ParserError    error_response(400, \"Invalid JSON format\")  rescue =&gt; e    puts \"Error: #{e.message}\"    error_response(500, \"Internal server error\")  endendprivatedef parse_request_body(event)  # Handle different API Gateway integration types  if event['body']    # API Gateway proxy integration    body_content = event['isBase64Encoded'] ? Base64.decode64(event['body']) : event['body']    JSON.parse(body_content)  elsif event['text']    # Direct invocation or test event    event  else    {}  endenddef analyze_text(text)  {    word_analysis: word_analysis(text),    sentiment_analysis: sentiment_analysis(text),    email_extraction: extract_emails(text),    readability: readability_score(text),    text_stats: text_statistics(text)  }enddef word_analysis(text)  words = text.downcase.gsub(/[^\\w\\s]/, '').split(/\\s+/)  word_freq = words.each_with_object(Hash.new(0)) { |word, hash| hash[word] += 1 }    {    total_words: words.length,    unique_words: word_freq.keys.length,    most_common: word_freq.sort_by { |k, v| -v }.first(5).to_h,    average_word_length: words.empty? ? 0 : (words.map(&amp;:length).sum.to_f / words.length).round(2)  }enddef sentiment_analysis(text)  positive_words = %w[good great excellent amazing wonderful fantastic happy joy love like enjoy success positive beautiful]  negative_words = %w[bad terrible awful horrible sad angry hate dislike failure negative ugly disappointing]  neutral_words = %w[okay fine normal average standard regular typical usual ordinary common]    words = text.downcase.gsub(/[^\\w\\s]/, '').split(/\\s+/)    positive_count = words.count { |word| positive_words.include?(word) }  negative_count = words.count { |word| negative_words.include?(word) }  neutral_count = words.count { |word| neutral_words.include?(word) }    total_sentiment_words = positive_count + negative_count + neutral_count    if total_sentiment_words == 0    sentiment = 'neutral'    confidence = 0.0  else    if positive_count &gt; negative_count &amp;&amp; positive_count &gt; neutral_count      sentiment = 'positive'      confidence = (positive_count.to_f / total_sentiment_words * 100).round(1)    elsif negative_count &gt; positive_count &amp;&amp; negative_count &gt; neutral_count      sentiment = 'negative'      confidence = (negative_count.to_f / total_sentiment_words * 100).round(1)    else      sentiment = 'neutral'      confidence = (neutral_count.to_f / total_sentiment_words * 100).round(1)    end  end    {    overall_sentiment: sentiment,    confidence_percentage: confidence,    positive_words_found: positive_count,    negative_words_found: negative_count,    neutral_words_found: neutral_count  }enddef extract_emails(text)  email_regex = /\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b/  emails = text.scan(email_regex).uniq    {    emails_found: emails,    count: emails.length,    domains: emails.map { |email| email.split('@').last }.uniq  }enddef readability_score(text)  sentences = text.split(/[.!?]+/).reject(&amp;:empty?)  words = text.gsub(/[^\\w\\s]/, '').split(/\\s+/)    return { score: 0, level: 'N/A' } if sentences.empty? || words.empty?    avg_sentence_length = words.length.to_f / sentences.length  avg_word_length = words.map(&amp;:length).sum.to_f / words.length    # Simplified readability score (0-100)  score = [100 - (avg_sentence_length * 2) - (avg_word_length * 5), 0].max.round(1)    level = case score          when 90..100 then 'Very Easy'          when 80..89 then 'Easy'          when 70..79 then 'Fairly Easy'          when 60..69 then 'Standard'          when 50..59 then 'Fairly Difficult'          when 30..49 then 'Difficult'          else 'Very Difficult'          end    {    score: score,    level: level,    avg_sentence_length: avg_sentence_length.round(1),    avg_word_length: avg_word_length.round(1)  }enddef text_statistics(text)  {    character_count: text.length,    character_count_no_spaces: text.gsub(/\\s/, '').length,    sentence_count: text.split(/[.!?]+/).reject(&amp;:empty?).length,    paragraph_count: text.split(/\\n\\s*\\n/).reject(&amp;:empty?).length,    line_count: text.split(/\\n/).length  }enddef error_response(status_code, message)  {    statusCode: status_code,    headers: {      'Content-Type' =&gt; 'application/json',      'Access-Control-Allow-Origin' =&gt; '*'    },    body: JSON.generate({      success: false,      error: message,      timestamp: Time.now.utc.iso8601    })  }endKey Benefits I’ve Experienced  Zero Infrastructure Management: Deploy and forget  Automatic Scaling: Handles traffic spikes seamlessly  Cost Efficiency: Free tier covers 1M requests monthly  Lightning Fast: Cold starts under 100ms for Ruby functions  Built-in Monitoring: CloudWatch logs everythingGetting Started is Simple  Write your function code  Deploy using Serverless Framework or AWS SAM  API Gateway automatically creates your endpoints  Test and iterate rapidlyThe serverless paradigm isn’t just a trend - it’s the future of application development. Start small, experiment, and watch your ideas scale effortlessly.AWS Lambda and API Gateway in Action: A Visual Walkthrough"
  },
  
  {
    "title": "Gemini CLI with MCP Integration: AI Powered Terminal Coding Assistant and AI Development Workflow",
    "url": "/posts/gemini-cli-with-git-mcp-integration/",
    "categories": "Artificial Intelligence (AI), gemini-cli, MCP, mcp_server_git",
    "tags": "AI, gemini-cli, ai-coding-assistant, developer-tools, cli-tools, llm, claude-code-alternative, mcp, mcp_server_git",
    "date": "2025-07-27 08:20:00 +0545",
    





    
    "snippet": "Gemini CLI: Google’s AI Coding Assistant with MCP IntegrationGoogle’s Gemini CLI brings the power of Gemini 2.5 Pro directly into your terminal, offering code understanding, file manipulation, and ...",
    "content": "Gemini CLI: Google’s AI Coding Assistant with MCP IntegrationGoogle’s Gemini CLI brings the power of Gemini 2.5 Pro directly into your terminal, offering code understanding, file manipulation, and intelligent automation. With its 1-million-token context window, it can handle large codebases and complex development tasks with ease.Key Features  Large Codebase Analysis: Handle projects beyond typical token limits  Multimodal Capabilities: Generate code from PDFs, sketches, and documents  Workflow Automation: Streamline development tasks and troubleshooting  MCP Integration: Connect to external tools like GitHub, Git, and databasesInstallation and SetupInstall Gemini CLInpm install -g @google/gemini-cliORbrew install gemini-cligeminiAuthentication OptionsOption 1: Google Account (Personal Use)gemini authGrants 60 model requests/minute and 1,000 model requests/day.Option 2: API Key (Production)  Get API key from Google AI Studio  Set environment variable:    export GEMINI_API_KEY=\"your_api_key_here\"# Make persistentecho 'export GEMINI_API_KEY=\"your_key_here\"' &gt;&gt; ~/.bashrc      Verify Installationgemini --version# 0.1.14gemini --helpBasic Usage ExamplesCode Generationgemini \"Create a React todo app with local storage\"gemini \"Write a Python class for CSV file operations\"Code Analysisgemini \"Review this code for performance issues: [paste code]\"gemini \"Debug this function: [paste code and error]\"File Operationsgemini \"Create a Node.js project structure with package.json\"gemini \"Generate README.md for this project\"MCP Server IntegrationWhat is MCP?Model Context Protocol (MCP) enables AI tools to connect to external data sources and tools. It’s like a USB-C port for AI applications, providing standardized access to GitHub, Git, databases, and more.Install Git MCP ServersGitHub MCP Server:npm install -g @modelcontextprotocol/server-githubexport GITHUB_PERSONAL_ACCESS_TOKEN=\"your_github_token\"Local Git MCP Server:npm install -g @cyanheads/git-mcp-serverMCP Inspector (for debugging):npm install -g @modelcontextprotocol/inspectorMCP ConfigurationCreate mcp-config.json:{  \"servers\": {    \"github\": {      \"command\": \"node\",      \"args\": [\"path/to/github-mcp-server\"],      \"env\": {        \"GITHUB_PERSONAL_ACCESS_TOKEN\": \"your_token\"      }    },    \"git\": {      \"command\": \"git-mcp-server\",      \"args\": [\"stdio\"],      \"cwd\": \"/path/to/git/repository\"    }  }}OR, In CLINE, Go to MCP sections,  install Git Tools.MCP + Gemini CLI ExamplesGitHub Operations:gemini \"Show all open issues in this repository\"gemini \"Create a GitHub issue for the API bug\"gemini \"Review the latest 3 pull requests\"Git Operations:gemini \"Create feature branch 'user-auth' and switch to it\"gemini \"Show diff for last 3 commits and explain changes\"gemini \"Generate release notes since last tag\"Repository Analysis:gemini \"Analyze git history and find top contributors\"gemini \"Review uncommitted changes before committing\"gemini \"Audit repository for security issues\"Best PracticesEffective Prompting  Be specific with requirements and context  Include relevant code snippets and error messages  Use follow-up questions to refine resultsMCP Security  Store tokens in environment variables  Use minimal necessary permissions  Rotate access tokens regularlyPerformance Tips  Start sessions in project root for full context  Combine related questions in single prompts  Use /clear to reset context when switching projectsTroubleshootingAuthentication Issues# Reset authenticationgemini auth --reset# Check environment variablesecho $GEMINI_API_KEYMCP Server Issues# Debug MCP serversmcp-inspector github-mcp-server# Test connectionsgemini \"List available MCP resources and tools\"Quick ReferenceEssential Commands# Installationnpm install -g @google/gemini-clinpm install -g @modelcontextprotocol/server-githubnpm install -g @cyanheads/git-mcp-server# Authenticationgemini authexport GEMINI_API_KEY=\"your_key\"# Usagegemini                           # Interactive sessiongemini \"your prompt\"            # One-shot commandgemini --help                   # HelpKey Prompts  \"Analyze this codebase and suggest improvements\"  \"Create tests for this function: [code]\"  \"Show all GitHub issues and categorize by priority\"  \"Review latest commits and suggest optimizations\"  \"Generate project documentation\"ConclusionGemini CLI with MCP integration provides a powerful, context-aware development experience. The combination of Google’s AI capabilities, extensive context window, and standardized protocol integration makes it an excellent choice for modern development workflows.Gemini-Cli in Action: A Visual WalkthroughTo demonstrate Gemini-Cli’s capabilities in practice, here’s a step-by-step visual guide showing the feature:"
  },
  
  {
    "title": "Ollama-Powered Qwen Code: Privacy-First AI Coding",
    "url": "/posts/qwen-code-with-ollama/",
    "categories": "Artificial Intelligence (AI), Qwen3, Ollama",
    "tags": "AI, qwen-code, qwen3, ollama, ai-coding-assistant, open-source-ai, local-ai, developer-tools, cli-tools, llm, claude-code-alternative, privacy-first",
    "date": "2025-07-27 05:33:00 +0545",
    





    
    "snippet": "Qwen Code: Your Local AI Coding Assistant - A Powerful Alternative to Claude CodeThe world of AI-powered coding assistants is rapidly evolving, and while Anthropic’s Claude Code has been making wav...",
    "content": "Qwen Code: Your Local AI Coding Assistant - A Powerful Alternative to Claude CodeThe world of AI-powered coding assistants is rapidly evolving, and while Anthropic’s Claude Code has been making waves in the developer community, there’s an exciting open-source alternative that deserves your attention: Qwen Code. This command-line AI workflow tool brings the power of advanced coding assistance directly to your local development environment.What is Qwen Code?Qwen Code is a command-line AI coding agent developed by the Qwen team at Alibaba Cloud. Built as an adaptation of the Gemini CLI framework, it’s specifically optimized for Qwen3-Coder models and offers enhanced parser support and comprehensive tool integration. Like Claude Code, Qwen Code is designed to understand, edit, and automate work across large codebases, making it an invaluable companion for modern software development.The tool leverages the powerful Qwen3-Coder models, with the flagship Qwen3-Coder-480B-A35B-Instruct being a 480-billion parameter Mixture-of-Experts model that supports up to 256K tokens natively and can be extended to 1M tokens. This massive context window allows for unprecedented understanding of large codebases and complex programming tasks.Key Features and CapabilitiesCode Understanding &amp; EditingQwen Code excels at querying and editing large codebases that go beyond traditional token limits. It can analyze complex project structures, understand dependencies, and make intelligent modifications across multiple files.Workflow AutomationThe tool can automate various development workflows including:  Handling pull requests (PRs)  Complex variable base operations  Generating JSDoc comments  Creating unit tests  Producing API documentationMulti-Language SupportQwen3-Coder models demonstrate excellent performance across more than 40 programming languages, with particularly impressive results in languages like Haskell, Racket, and Python. The latest Qwen2.5-Coder-32B scores 65.9 on McEval, showcasing its broad language capabilities.Setting Up Qwen Code with OllamaOne of the most appealing aspects of Qwen Code is its ability to run locally using Ollama, giving you complete control over your development environment and ensuring your code never leaves your machine.Prerequisites  Ollama installed on your system  Node.js and npm for the Qwen Code CLI  Sufficient system resources (the larger models benefit from more RAM and GPU memory)Installation Steps  Install Ollama Model    # Pull a Qwen3 model (adjust size based on your system capabilities)ollama pull qwen3:30b-a3b# Or try other variants like:# ollama pull qwen2.5-coder:32b# ollama pull qwen2.5-coder:7b        Install Qwen Code CLI    npm install -g qwen-code        Configure API EndpointSet up Qwen Code to use your local Ollama instance running on localhost:11434/api/v1.Running Qwen CodeThe setup requires two terminal sessions:Terminal 1 - Start Ollama Server:ollama serveTerminal 2 - Run Qwen Code:qwen --help  # View all available options and commandsqwen         # Start interactive sessionQwen Code vs Claude Code: Key DifferencesWhile both tools serve similar purposes as AI coding assistants, there are several important distinctions:Deployment Model  Qwen Code: Runs entirely locally with Ollama, ensuring complete privacy and no dependency on external APIs  Claude Code: Connects to Anthropic’s cloud-based Claude modelsCost Structure  Qwen Code: Free to use once set up locally (only hardware costs)  Claude Code: Requires API credits and usage-based pricingCustomization  Qwen Code: Open-source nature allows for extensive customization and fine-tuning  Claude Code: Limited customization options as a proprietary toolPerformance CharacteristicsBoth tools may issue multiple API calls per cycle, resulting in higher token usage for complex tasks. However, Qwen Code’s local execution means you’re only limited by your hardware rather than API rate limits.Best Practices and TipsModel SelectionChoose your Qwen model based on your system capabilities:  qwen3:7b - Good for basic coding tasks on lower-end hardware  qwen3:30b-a3b - Balanced performance for most development tasks  qwen2.5-coder:32b - Excellent coding performance with strong multi-language supportSystem RequirementsFor optimal performance, consider:  GPU: 24GB+ VRAM for larger models  RAM: 128-256GB for the largest models  Context Length: Use 65,536 tokens as recommended (can be increased based on needs)Configuration Tips  Set appropriate temperature settings (0.6-0.7 for coding tasks)  Use TopP=0.8-0.95 depending on your model  Avoid greedy decoding to prevent performance degradationGetting StartedTo begin using Qwen Code effectively:  Start with smaller models to test your setup  Experiment with different prompting strategies  Leverage the tool’s ability to understand large codebases  Use it for automated documentation and testing  Explore workflow automation features for repetitive tasksThe Future of Local AI CodingQwen Code represents a significant step toward democratizing AI-powered development tools. By offering a powerful, local alternative to cloud-based solutions, it addresses key concerns around privacy, cost, and dependency on external services.The open-source nature of the Qwen ecosystem also means rapid innovation and community contributions, potentially leading to specialized variants optimized for specific programming languages or development workflows.ConclusionWhile Claude Code has pioneered the command-line AI coding assistant space, Qwen Code offers a compelling alternative that brings similar capabilities to your local environment. With its powerful models, extensive language support, and cost-effective local deployment, it’s worth exploring for developers who value privacy, customization, and independence from cloud services.Whether you’re working on personal projects or enterprise applications, Qwen Code provides a robust foundation for AI-assisted development that grows with your needs. Try it out with Ollama today and experience the future of local AI coding assistance.Qwen-Code in Action: A Visual WalkthroughTo demonstrate Qwen-Code’s capabilities in practice, here’s a step-by-step visual guide showing the feature:"
  },
  
  {
    "title": "The Complete Guide to Terraform",
    "url": "/posts/terraform/",
    "categories": "cloud devops aws gcp azure docker terraform",
    "tags": "aws, gcp, azure, cloud, devops, deployment, docker, terraform",
    "date": "2025-07-16 04:58:00 +0545",
    





    
    "snippet": "Getting Started with Terraform: Infrastructure as Code Made SimpleIntroductionTerraform is a powerful Infrastructure as Code (IaC) tool developed by HashiCorp that allows developers and DevOps engi...",
    "content": "Getting Started with Terraform: Infrastructure as Code Made SimpleIntroductionTerraform is a powerful Infrastructure as Code (IaC) tool developed by HashiCorp that allows developers and DevOps engineers to define, provision, and manage infrastructure resources using declarative configuration files. Instead of manually clicking through cloud consoles or writing complex scripts, Terraform enables you to describe your desired infrastructure state in human-readable configuration files and automatically creates, updates, or destroys resources to match that state.Think of Terraform as a blueprint for your infrastructure. Just as an architect creates blueprints before building a house, Terraform lets you define your infrastructure before deploying it. This approach brings software development best practices to infrastructure management, making it more reliable, repeatable, and maintainable.What is Infrastructure as Code?Infrastructure as Code (IaC) is the practice of managing and provisioning computing infrastructure through machine-readable configuration files, rather than through physical hardware configuration or interactive configuration tools. With IaC, you can:  Version control your infrastructure - Track changes, rollback if needed  Reproduce environments - Create identical dev, staging, and production environments  Collaborate effectively - Share infrastructure configurations with team members  Automate deployments - Integrate with CI/CD pipelinesHow Terraform WorksTerraform follows a simple workflow:  Write - Define infrastructure in .tf configuration files  Plan - Preview changes before applying them  Apply - Create, update, or destroy infrastructure  Manage - Track infrastructure state and manage lifecycleKey Concepts  Providers - Plugins that interact with APIs (AWS, Azure, GCP, Docker, etc.)  Resources - Infrastructure components (servers, databases, networks)  State - Terraform’s record of managed infrastructure  Modules - Reusable configuration packagesUsage ExamplesBasic Example: Docker ContainerHere’s a simple example that creates an nginx container using Docker:terraform {  required_providers {    docker = {      source = \"kreuzwerker/docker\"    }  }}provider \"docker\" {}resource \"docker_image\" \"nginx\" {  name = \"nginx:latest\"}resource \"docker_container\" \"web\" {  image = docker_image.nginx.name  name  = \"terraform-nginx\"  ports {    internal = 80    external = 8080  }}  You might need to modify existing configuration with correct docker host path.terraform {  required_providers {    docker = {      source = \"kreuzwerker/docker\"    }  }}provider \"docker\" {  host = \"unix:///Users/siv/.docker/run/docker.sock\"}resource \"docker_image\" \"nginx\" {  name = \"nginx:latest\"  keep_locally = true}resource \"docker_container\" \"web\" {  image = docker_image.nginx.name  name  = \"terraform-nginx\"  ports {    internal = 80    external = 8080  }}  Run following command on each config edit. Make sure your Docker is restarted/stopped-startedterraform init# Terraform has been successfully initialized!terraform validate# Success! The configuration is valid.terraform apply# Apply complete! Resources: 1 added, 1 changed, 0 destroyed.terraform destroy# Destroy complete! Resources: 2 destroyed.  Possible error looks like:Error: Error pinging Docker server, please make sure that unix:///var/run/docker.sock is reachable and has a  '_ping' endpoint. Error: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?││   with provider[\"registry.terraform.io/kreuzwerker/docker\"],│   on main.tf line 9, in provider \"docker\":│    9: provider \"docker\" {}  See Docker Contextdocker context lsdocker context use desktop-linuxNAME              DESCRIPTION                               DOCKER ENDPOINT                             ERRORdefault           Current DOCKER_HOST based configuration   unix:///var/run/docker.sockdesktop-linux *   Docker Desktop                            unix:///Users/siv/.docker/run/docker.sockAWS EC2 Instance Exampleterraform {  required_providers {    aws = {      source  = \"hashicorp/aws\"      version = \"~&gt; 5.0\"    }  }}provider \"aws\" {  region = \"us-west-2\"}resource \"aws_instance\" \"web_server\" {  ami           = \"ami-0c02fb55956c7d316\"  instance_type = \"t2.micro\"    tags = {    Name = \"MyWebServer\"  }}Essential Commands# Initialize Terraform (download providers)terraform init# Check configuration syntaxterraform validate# Preview changesterraform plan# Apply changesterraform apply# Destroy infrastructureterraform destroy# Show current stateterraform show# List managed resourcesterraform state listPros and ConsAdvantages🎯 Declarative Approach  Describe what you want, not how to build it  Terraform figures out the steps to reach desired state🔄 Idempotent Operations  Safe to run multiple times  Only makes necessary changes☁️ Multi-Cloud Support  Works with AWS, Azure, GCP, and 100+ providers  Avoid vendor lock-in📋 State Management  Tracks current infrastructure state  Enables safe updates and rollbacks🔒 Plan Before Apply  Preview changes before execution  Reduces risk of unintended modifications📦 Modularity  Reusable modules for common patterns  Promote best practices and consistency👥 Team Collaboration  Version control integration  Consistent environments across teamsDisadvantages📚 Learning Curve  HCL syntax to master  Understanding of underlying cloud services required🗃️ State File Management  Critical state file needs secure storage  Corruption can cause issues🔄 Provider Dependencies  Relies on third-party providers  Updates may introduce breaking changes🔧 Limited Logic  Not a full programming language  Complex conditional logic can be challenging💰 Cost Considerations  Easy to accidentally create expensive resources  Need proper cost monitoring🐛 Debugging Challenges  Error messages can be cryptic  Troubleshooting requires deep understandingDeployment Options1. Local DevelopmentBest for: Learning, small projects, testing# Simple local workflowterraform initterraform planterraform applyConsiderations:  State stored locally  No collaboration features  Good for experimentation2. Remote State with Cloud StorageBest for: Team collaboration, production environmentsterraform {  backend \"s3\" {    bucket = \"my-terraform-state\"    key    = \"prod/terraform.tfstate\"    region = \"us-west-2\"  }}Benefits:  Shared state across team  State locking prevents conflicts  Backup and versioning3. Terraform CloudBest for: Enterprise teams, advanced workflowsFeatures:  Remote execution  Private module registry  Policy as code  Cost estimation  Team management4. CI/CD IntegrationBest for: Automated deployments# GitHub Actions examplename: Terraformon: [push]jobs:  terraform:    runs-on: ubuntu-latest    steps:      - uses: actions/checkout@v2      - name: Setup Terraform        uses: hashicorp/setup-terraform@v1      - name: Terraform Plan        run: terraform plan      - name: Terraform Apply        run: terraform apply -auto-approve5. Multi-Environment SetupBest for: Managing dev/staging/prod environmentsproject/├── modules/│   ├── vpc/│   ├── ec2/│   └── rds/├── environments/│   ├── dev/│   ├── staging/│   └── prod/└── global/Best Practices1. Project Structure  Use modules for reusable components  Separate environments  Keep configurations DRY (Don’t Repeat Yourself)2. State Management  Use remote state for team projects  Enable state locking  Regular state backups3. Security  Never commit sensitive data  Use variables for secrets  Implement proper IAM policies4. Version Control  Tag releases  Use meaningful commit messages  Review changes before merging5. Testing  Validate configurations regularly  Test in lower environments first  Use terraform plan extensivelyCommon Use Cases1. Cloud Migration  Migrate existing infrastructure to cloud  Ensure consistency across environments  Gradual migration strategies2. Disaster Recovery  Quickly recreate infrastructure  Multi-region deployments  Automated backup strategies3. Development Environments  Spin up/down dev environments  Consistent developer setups  Cost optimization4. Compliance and Governance  Standardized configurations  Policy enforcement  Audit trailsGetting Started Tips  Start Small - Begin with simple resources like Docker containers  Use Official Providers - Stick to verified providers from HashiCorp  Read Documentation - Provider docs are your best friend  Practice Locally - Use Docker or local providers for learning  Join Community - Engage with Terraform community for supportConclusionTerraform has revolutionized how we approach infrastructure management by bringing software development practices to infrastructure provisioning. Its declarative approach, multi-cloud support, and strong ecosystem make it an essential tool for modern DevOps practices.While there’s a learning curve, the benefits of using Terraform far outweigh the initial investment in time and effort. Whether you’re managing a single server or a complex multi-cloud architecture, Terraform provides the tools and flexibility needed to manage infrastructure efficiently and reliably.Error and FixesRails + Nginx + Terraform Docker SetupProblemI had a Rails app running in Docker at localhost:3000 and an nginx container managed by Terraform at localhost:8080. I wanted to serve the Rails app through nginx instead of having separate ports.Ways to Do: Multiple Containers vs Single ContainerInitially, I was confused why I couldn’t run Rails inside the nginx container like traditional deployments:Traditional Server (what I expected):Server├── nginx (web server)├── Rails app (process)└── DatabaseDocker Way (what actually happens):Container 1: nginxContainer 2: Rails appContainer 3: DatabaseKey insight: Docker follows “one process per container” principle. Containers communicate over Docker’s internal network, not as processes in the same system.Solution: Nginx Reverse Proxy1. Create nginx.confserver {    listen 80;    location / {        proxy_pass http://host.docker.internal:3000;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;        proxy_set_header X-Forwarded-Proto $scheme;    }}2. Update Terraform main.tfresource \"docker_image\" \"nginx\" {  name = \"nginx:latest\"  keep_locally = true}resource \"docker_container\" \"web\" {  image = docker_image.nginx.name  name  = \"terraform-nginx\"  ports {    internal = 80    external = 8080  }  volumes {    host_path = \"${path.cwd}/nginx.conf\"    container_path = \"/etc/nginx/conf.d/default.conf\"  }}3. Test and Apply# Test nginx config syntaxdocker run --rm -v $(pwd)/nginx.conf:/etc/nginx/conf.d/default.conf nginx nginx -t# Apply changesterraform apply# Test the setupcurl http://localhost:8080Result  localhost:8080 now serves Rails app through nginx  nginx acts as reverse proxy to Rails container  Clean separation of concerns with Docker containers"
  },
  
  {
    "title": "The Cost of Perfectionism in Dogfooding",
    "url": "/posts/the-cost-of-perfectionism-in-dogfooding/",
    "categories": "tech_culture",
    "tags": "product_development, leadership, iteration, feedback",
    "date": "2025-07-01 08:00:00 +0545",
    





    
    "snippet": "The Freedom of Imperfection: Why ‘Low-Shame’ Dogfooding Drives Better ResultsIn many organizations, sharing unfinished work feels risky. Employees are conditioned to believe that anything less than...",
    "content": "The Freedom of Imperfection: Why ‘Low-Shame’ Dogfooding Drives Better ResultsIn many organizations, sharing unfinished work feels risky. Employees are conditioned to believe that anything less than polished perfection invites criticism—so they overprepare, obsess over edge cases, and delay sharing until every possible “What if?” has been addressed. This makes sense for high-stakes, customer-facing releases, but it’s counterproductive for internal processes like dogfooding (using your own product during development).When teams fear imperfection, they waste time perfecting ideas that might be fundamentally flawed. By the time they finally share their work, course corrections become expensive and demoralizing. The feedback that could have saved them months of effort never arrives—because no one saw the work early enough to give it.The Cost of Perfectionism in DogfoodingDogfooding is meant to be an iterative learning process. But when shame clings to unfinished work:  Teams delay sharing prototypes, fearing judgment.  They overinvest in flawed ideas, making pivots painful.  They miss rapid improvements that could have emerged from early feedback.The result? Slower progress, higher costs, and products that fail to meet real needs—precisely what dogfooding is meant to prevent.The “Low-Shame” AntidoteTo fix this, teams need a “low level of shame” culture when working internally. This means:  Celebrating rough drafts—Prototypes and half-baked ideas are shared proudly, not hidden.  Prioritizing speed over polish—Feedback is sought early, even if the work is messy.  Failing fast and cheap—Small, incremental corrections replace late-stage overhauls.The key insight: In low-risk internal contexts, the cost of being wrong early is trivial compared to the cost of being wrong late. By embracing imperfection, teams uncover problems sooner, iterate faster, and build better products.Beyond Product DevelopmentThis principle isn’t just for engineers. Any collaborative work—design, strategy, even writing—benefits from low-shame iteration. When psychological safety replaces perfectionism, teams stop wasting cycles on “defensive preparation” and start making progress.The lesson? Don’t wait. Share early, correct often, and let dogfooding do its job.Why This Works  Engaging hook (“The Freedom of Imperfection”) draws readers in.  Problem/Solution structure makes the argument clear and actionable.  Bold highlights and lists improve readability.  Universal application (beyond product teams) broadens appeal."
  },
  
  {
    "title": "PowerBI data analysis and visualization",
    "url": "/posts/powerbi-datavisualization/",
    "categories": "data_visualization, powerbi, ai",
    "tags": "data_visualization, powerbi, ai",
    "date": "2025-06-06 11:00:00 +0545",
    





    
    "snippet": "Power BI: AI-Powered Data Visualization for Modern BusinessesMicrosoft Power BI has transformed from a simple reporting tool into an intelligent analytics platform that combines data visualization ...",
    "content": "Power BI: AI-Powered Data Visualization for Modern BusinessesMicrosoft Power BI has transformed from a simple reporting tool into an intelligent analytics platform that combines data visualization with artificial intelligence. This evolution makes advanced analytics accessible to organizations without requiring extensive technical expertise.Key AI Features That Drive IntelligenceSmart Narratives and Auto-Generated InsightsPower BI automatically generates written summaries of your data, explaining key trends and outliers in plain English. Complex visualizations become digestible insights that stakeholders can quickly understand and act upon.Q&amp;A Natural Language ProcessingUsers interact with data using everyday language, asking questions like “What were our sales last quarter?” The AI interprets these queries and generates appropriate visualizations instantly.Anomaly Detection and ForecastingMachine learning algorithms continuously monitor data streams to identify unusual patterns while built-in forecasting capabilities predict future trends using historical data.Real-World Impact Across IndustriesOrganizations across sectors leverage Power BI’s AI capabilities for various applications: retailers analyze customer behavior and predict demand, healthcare providers monitor patient outcomes and operational efficiency, financial institutions detect fraud and assess risks, while manufacturers optimize production schedules and predict maintenance needs.Core Benefits of AI-Enhanced VisualizationDemocratized Analytics: Intuitive interface and natural language capabilities make advanced analytics accessible to non-technical users.Faster Decision-Making: Automated insight generation accelerates the journey from raw data to actionable intelligence.Improved Accuracy: AI algorithms process vast amounts of data with consistency, reducing human error and bias.Scalable Intelligence: AI capabilities scale automatically as organizations grow and data volumes increase.Implementation Best PracticesSuccess with Power BI begins with clean, well-structured data and clear business objectives. The platform includes data preparation tools and suggests optimal visualization types based on your data characteristics. Regular model updates and user training maximize the value of AI-driven insights.The Future of Intelligent AnalyticsPower BI continues evolving with enhanced natural language processing, sophisticated predictive models, and integration with other Microsoft AI services. This combination of human creativity with machine learning capabilities enables a new level of data-driven decision making.ConclusionPower BI’s AI-enhanced visualization capabilities transform how businesses interact with data by automating insight discovery and providing predictive analytics. As data complexity grows, these intelligent tools become essential for competitive advantage, enabling organizations to make faster, more informed decisions regardless of their size or technical expertise.Power BI in Action: A Visual WalkthroughTo demonstrate Power BI’s capabilities in practice, here’s a step-by-step visual guide showing the platform’s key features and interface elements:"
  },
  
  {
    "title": "Ruby MultiThreading",
    "url": "/posts/ruby-multithreading/",
    "categories": "Ruby, multithreading",
    "tags": "ruby, multithreading",
    "date": "2025-06-05 11:00:00 +0545",
    





    
    "snippet": "Ruby Multithreading: A Practical Guide for Better PerformanceRuby’s threading capabilities allow developers to write concurrent programs that can improve performance and responsiveness. While Ruby’...",
    "content": "Ruby Multithreading: A Practical Guide for Better PerformanceRuby’s threading capabilities allow developers to write concurrent programs that can improve performance and responsiveness. While Ruby’s Global Interpreter Lock (GIL) presents some limitations, understanding multithreading is crucial for building efficient applications, especially for I/O-bound operations.Understanding Ruby ThreadsRuby threads are lightweight units of execution that run concurrently within a single process. They’re particularly effective for I/O-bound tasks like file operations, network requests, and database queries, where threads can work while others wait for external resources.Basic Thread Creation# Simple thread creationthread = Thread.new do  puts \"Hello from thread #{Thread.current.object_id}\"  sleep(2)  puts \"Thread finished\"endputs \"Main thread continues...\"thread.join  # Wait for thread to completeputs \"All done!\"Practical Example 1: Concurrent Web ScrapingOne of the most common use cases for threading is making multiple HTTP requests simultaneously:require 'net/http'require 'uri'require 'json'class ConcurrentWebScraper  def initialize(urls)    @urls = urls    @results = {}    @mutex = Mutex.new  end  def scrape_sequentially    start_time = Time.now        @urls.each do |url|      response = fetch_url(url)      @results[url] = response    end        puts \"Sequential execution: #{Time.now - start_time} seconds\"    @results  end  def scrape_concurrently    start_time = Time.now    threads = []        @urls.each do |url|      threads &lt;&lt; Thread.new(url) do |current_url|        response = fetch_url(current_url)                # Thread-safe access to shared data        @mutex.synchronize do          @results[current_url] = response        end      end    end        # Wait for all threads to complete    threads.each(&amp;:join)        puts \"Concurrent execution: #{Time.now - start_time} seconds\"    @results  end  private  def fetch_url(url)    uri = URI(url)    response = Net::HTTP.get_response(uri)    {      status: response.code,      length: response.body.length,      title: extract_title(response.body)    }  rescue =&gt; e    { error: e.message }  end  def extract_title(html)    title_match = html.match(/&lt;title&gt;(.*?)&lt;\\/title&gt;/i)    title_match ? title_match[1] : \"No title found\"  endend# Usage exampleurls = [  'https://www.ruby-lang.org',  'https://github.com',  'https://stackoverflow.com',  'https://www.google.com']scraper = ConcurrentWebScraper.new(urls)puts \"=== Sequential Scraping ===\"sequential_results = scraper.scrape_sequentiallyputs \"\\n=== Concurrent Scraping ===\"concurrent_results = scraper.scrape_concurrentlyputs \"\\nResults:\"concurrent_results.each do |url, data|  puts \"#{url}: #{data[:status]} - #{data[:title]}\"endPractical Example 2: File Processing with Thread PoolFor CPU-intensive tasks or when you need to control the number of concurrent threads:class ThreadPool  def initialize(size = 4)    @size = size    @jobs = Queue.new    @pool = Array.new(@size) do      Thread.new do        catch(:exit) do          loop do            job, args, block = @jobs.pop            case job            when :work              block.call(*args)            when :exit              throw :exit            end          end        end      end    end  end  def schedule(*args, &amp;block)    @jobs &lt;&lt; [:work, args, block]  end  def shutdown    @size.times do      @jobs &lt;&lt; [:exit]    end    @pool.each(&amp;:join)  endendclass FileProcessor  def initialize(thread_count = 4)    @thread_pool = ThreadPool.new(thread_count)    @results = []    @mutex = Mutex.new  end  def process_files(file_paths)    file_paths.each do |file_path|      @thread_pool.schedule(file_path) do |path|        result = process_single_file(path)                @mutex.synchronize do          @results &lt;&lt; result        end      end    end        # Wait a bit for processing to complete    sleep(1) while @results.length &lt; file_paths.length        @thread_pool.shutdown    @results  end  private  def process_single_file(file_path)    return { file: file_path, error: \"File not found\" } unless File.exist?(file_path)        content = File.read(file_path)        {      file: file_path,      size: content.length,      lines: content.lines.count,      words: content.split.count,      processed_at: Time.now,      thread_id: Thread.current.object_id    }  rescue =&gt; e    { file: file_path, error: e.message }  endend# Create sample files for demonstrationsample_files = []5.times do |i|  filename = \"sample_#{i}.txt\"  File.write(filename, \"Sample content for file #{i}\\n\" * (i + 1) * 10)  sample_files &lt;&lt; filenameend# Process files concurrentlyprocessor = FileProcessor.new(3)results = processor.process_files(sample_files)puts \"File Processing Results:\"results.each do |result|  if result[:error]    puts \"Error processing #{result[:file]}: #{result[:error]}\"  else    puts \"#{result[:file]}: #{result[:lines]} lines, #{result[:words]} words (Thread: #{result[:thread_id]})\"  endend# Cleanupsample_files.each { |file| File.delete(file) if File.exist?(file) }Thread Synchronization and SafetyMutex for Thread Safetyclass ThreadSafeCounter  def initialize    @count = 0    @mutex = Mutex.new  end  def increment    @mutex.synchronize do      @count += 1    end  end  def decrement    @mutex.synchronize do      @count -= 1    end  end  def value    @mutex.synchronize do      @count    end  endend# Demonstrate thread safetycounter = ThreadSafeCounter.newthreads = []# Create 10 threads that increment counter 1000 times each10.times do  threads &lt;&lt; Thread.new do    1000.times { counter.increment }  endendthreads.each(&amp;:join)puts \"Final counter value: #{counter.value}\" # Should be 10,000Using Queue for Producer-Consumer Patternclass ProducerConsumer  def initialize    @queue = Queue.new    @results = []    @mutex = Mutex.new  end  def start_processing(items_to_process)    # Producer thread    producer = Thread.new do      items_to_process.each do |item|        @queue &lt;&lt; item        puts \"Produced: #{item}\"        sleep(0.1) # Simulate work      end            # Signal completion      3.times { @queue &lt;&lt; :done }    end    # Consumer threads    consumers = 3.times.map do |i|      Thread.new do        loop do          item = @queue.pop          break if item == :done                    # Simulate processing          processed = process_item(item)                    @mutex.synchronize do            @results &lt;&lt; processed            puts \"Consumer #{i} processed: #{processed}\"          end                    sleep(0.2) # Simulate work        end      end    end    # Wait for completion    producer.join    consumers.each(&amp;:join)        @results  end  private  def process_item(item)    \"processed_#{item}_#{Time.now.to_f}\"  endend# Usagepc = ProducerConsumer.newitems = (1..10).to_aresults = pc.start_processing(items)puts \"\\nFinal results:\"results.each { |result| puts result }Practical Example 3: Concurrent Database Operationsrequire 'sqlite3'class ConcurrentDatabase  def initialize(db_path = 'concurrent_example.db')    @db_path = db_path    setup_database  end  def setup_database    db = SQLite3::Database.new(@db_path)    db.execute &lt;&lt;-SQL      CREATE TABLE IF NOT EXISTS users (        id INTEGER PRIMARY KEY,        name TEXT,        email TEXT,        created_at DATETIME,        thread_id TEXT      )    SQL    db.close  end  def insert_users_sequentially(users)    start_time = Time.now        db = SQLite3::Database.new(@db_path)    users.each do |user|      db.execute(        \"INSERT INTO users (name, email, created_at, thread_id) VALUES (?, ?, ?, ?)\",        [user[:name], user[:email], Time.now, 'main']      )    end    db.close        puts \"Sequential inserts: #{Time.now - start_time} seconds\"  end  def insert_users_concurrently(users)    start_time = Time.now    threads = []        users.each_slice(users.length / 4) do |user_batch|      threads &lt;&lt; Thread.new do        db = SQLite3::Database.new(@db_path)        user_batch.each do |user|          db.execute(            \"INSERT INTO users (name, email, created_at, thread_id) VALUES (?, ?, ?, ?)\",            [user[:name], user[:email], Time.now, Thread.current.object_id.to_s]          )        end        db.close      end    end        threads.each(&amp;:join)    puts \"Concurrent inserts: #{Time.now - start_time} seconds\"  end  def get_stats    db = SQLite3::Database.new(@db_path)    total = db.execute(\"SELECT COUNT(*) FROM users\")[0][0]    by_thread = db.execute(\"SELECT thread_id, COUNT(*) FROM users GROUP BY thread_id\")    db.close        puts \"Total users: #{total}\"    puts \"By thread:\"    by_thread.each do |thread_id, count|      puts \"  Thread #{thread_id}: #{count} users\"    end  end  def cleanup    File.delete(@db_path) if File.exist?(@db_path)  endend# Generate sample datausers = 1000.times.map do |i|  {    name: \"User #{i}\",    email: \"user#{i}@example.com\"  }end# Test concurrent database operationsdb = ConcurrentDatabase.newputs \"=== Sequential Database Operations ===\"db.insert_users_sequentially(users[0...250])puts \"\\n=== Concurrent Database Operations ===\"db.insert_users_concurrently(users[250...1000])puts \"\\n=== Database Statistics ===\"db.get_stats# Cleanupdb.cleanupBest Practices and Common Pitfalls1. Always Join Your Threads# Bad: Thread may not completeThread.new { expensive_operation }# Good: Ensure thread completionthread = Thread.new { expensive_operation }thread.join2. Handle Exceptions Properlythread = Thread.new do  begin    risky_operation  rescue =&gt; e    puts \"Thread error: #{e.message}\"  endendthread.join# Check for exceptionsif thread.status.nil?  puts \"Thread completed\"elsif thread.status == false  puts \"Thread terminated with exception\"end3. Avoid Race Conditions# Bad: Race condition@shared_data = []threads = 5.times.map do  Thread.new do    @shared_data &lt;&lt; \"data\"  # Unsafe  endend# Good: Thread-safe access@shared_data = []@mutex = Mutex.newthreads = 5.times.map do  Thread.new do    @mutex.synchronize do      @shared_data &lt;&lt; \"data\"  # Safe    end  endendWhen to Use Ruby ThreadingIdeal Use Cases:  I/O-bound operations: File reading, network requests, database queries  Producer-consumer scenarios: Background job processing  Parallel data processing: When tasks can be divided into independent chunks  User interface responsiveness: Keeping UI responsive during long operationsWhen NOT to Use:  CPU-intensive tasks: Ruby’s GIL limits true parallelism  Simple sequential operations: Threading overhead may not be worth it  Shared mutable state: Without proper synchronization, leads to bugsPerformance ConsiderationsRuby’s Global Interpreter Lock (GIL) means that only one thread can execute Ruby code at a time. However, threads are still valuable because:  I/O operations release the GIL: Threads can run truly concurrently during I/O  Better resource utilization: While one thread waits, others can work  Improved responsiveness: Applications feel more responsive to usersKey Takeaways  Ruby threading excels at I/O-bound tasks despite the GIL limitation  Always use proper synchronization (Mutex, Queue) for shared data  Thread pools help manage resource usage and prevent thread explosion  Handle exceptions within threads to prevent silent failures  Consider alternatives like processes or async libraries for CPU-intensive work"
  },
  
  {
    "title": "Ruby Exception Handling",
    "url": "/posts/ruby-exception-handling/",
    "categories": "Ruby, exception_handling",
    "tags": "ruby, exception_handling",
    "date": "2025-06-05 11:00:00 +0545",
    





    
    "snippet": "Ruby Exception Handling: A Complete Guide to Robust Error ManagementException handling is a critical aspect of writing robust Ruby applications. It allows you to gracefully handle errors, provide m...",
    "content": "Ruby Exception Handling: A Complete Guide to Robust Error ManagementException handling is a critical aspect of writing robust Ruby applications. It allows you to gracefully handle errors, provide meaningful feedback to users, and prevent your application from crashing unexpectedly. Ruby provides a comprehensive exception handling system that gives developers fine-grained control over error management.Understanding Ruby ExceptionsIn Ruby, exceptions are objects that represent errors or exceptional conditions. When an error occurs, Ruby “raises” an exception, which can be “caught” and handled appropriately. All exceptions in Ruby inherit from the Exception class.Ruby Exception HierarchyException +-- NoMemoryError +-- ScriptError |    +-- LoadError |    +-- NotImplementedError |    +-- SyntaxError +-- SecurityError +-- SignalException |    +-- Interrupt +-- StandardError -- default for rescue |    +-- ArgumentError |    +-- IOError |    |    +-- EOFError |    +-- IndexError |    |    +-- KeyError |    |    +-- StopIteration |    +-- LocalJumpError |    +-- NameError |    |    +-- NoMethodError |    +-- RangeError |    |    +-- FloatDomainError |    +-- RegexpError |    +-- RuntimeError -- default for raise |    +-- SystemCallError |    |    +-- Errno::* |    +-- ThreadError |    +-- TypeError |    +-- ZeroDivisionError +-- SystemExit +-- SystemStackError +-- fatal -- impossible to rescueBasic Exception Handling with begin/rescueThe most common way to handle exceptions is using the begin/rescue block:begin  # Code that might raise an exception  risky_operationrescue  # Code to handle the exception  puts \"An error occurred!\"end# Example with specific exception typebegin  result = 10 / 0rescue ZeroDivisionError  puts \"Cannot divide by zero!\"  result = nilendRescuing Multiple Exception Typesbegin  # Risky code here  perform_operationrescue ZeroDivisionError  puts \"Division by zero error\"rescue ArgumentError  puts \"Invalid argument provided\"rescue StandardError =&gt; e  puts \"Other error occurred: #{e.message}\"end# Alternative syntax for multiple exceptionsbegin  perform_operationrescue ZeroDivisionError, ArgumentError =&gt; e  puts \"Math or argument error: #{e.message}\"rescue =&gt; e  puts \"Unexpected error: #{e.message}\"endRaising ExceptionsYou can raise exceptions manually using the raise keyword:# Raise a generic RuntimeErrorraise \"Something went wrong!\"# Raise a specific exception typeraise ArgumentError, \"Invalid input provided\"# Raise with custom exception classraise ZeroDivisionError.new(\"Cannot divide by zero\")# Re-raise the current exceptionbegin  1 / 0rescue =&gt; e  puts \"Logging error: #{e.message}\"  raise  # Re-raises the same exceptionendCreating Custom Exceptions# Define custom exception classesclass ValidationError &lt; StandardError  attr_reader :field, :value    def initialize(field, value, message = nil)    @field = field    @value = value    super(message || \"Validation failed for #{field}: #{value}\")  endendclass DatabaseConnectionError &lt; StandardError  def initialize(host, port)    super(\"Failed to connect to database at #{host}:#{port}\")  endend# Usagedef validate_email(email)  raise ValidationError.new(:email, email, \"Invalid email format\") unless email.include?(\"@\")endbegin  validate_email(\"invalid-email\")rescue ValidationError =&gt; e  puts \"Field: #{e.field}, Value: #{e.value}\"  puts \"Error: #{e.message}\"endComplete Exception Handling SyntaxRuby provides several clauses for comprehensive exception handling:begin  # Code that might raise an exception  risky_operationrescue SpecificError =&gt; e  # Handle specific exceptions  handle_specific_error(e)rescue =&gt; e  # Handle any StandardError  handle_general_error(e)else  # Executed only if no exception was raised  puts \"Operation completed successfully\"ensure  # Always executed, regardless of exceptions  cleanup_resourcesendThe ensure ClauseThe ensure clause is always executed, making it perfect for cleanup operations:def read_file(filename)  file = nil  begin    file = File.open(filename, 'r')    content = file.read    return content  rescue IOError =&gt; e    puts \"Error reading file: #{e.message}\"    return nil  ensure    # This always runs, even if an exception occurs    file&amp;.close    puts \"File handle closed\"  endendPractical Example 1: File Processing with Exception Handlingclass FileProcessor  class FileProcessingError &lt; StandardError; end  class InvalidFileTypeError &lt; FileProcessingError; end  class FileSizeError &lt; FileProcessingError; end    ALLOWED_EXTENSIONS = %w[.txt .csv .json].freeze  MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB    def initialize(log_errors: true)    @log_errors = log_errors    @processed_files = []    @failed_files = []  end    def process_files(file_paths)    file_paths.each do |path|      begin        process_single_file(path)        @processed_files &lt;&lt; path        puts \"✓ Successfully processed: #{path}\"      rescue FileProcessingError =&gt; e        @failed_files &lt;&lt; { path: path, error: e.message, type: e.class.name }        log_error(\"Processing failed for #{path}: #{e.message}\") if @log_errors      rescue =&gt; e        @failed_files &lt;&lt; { path: path, error: e.message, type: \"UnexpectedError\" }        log_error(\"Unexpected error processing #{path}: #{e.message}\") if @log_errors      end    end        generate_report  end    private    def process_single_file(file_path)    # Check if file exists    raise FileProcessingError, \"File not found: #{file_path}\" unless File.exist?(file_path)        # Validate file extension    extension = File.extname(file_path).downcase    unless ALLOWED_EXTENSIONS.include?(extension)      raise InvalidFileTypeError, \"Unsupported file type: #{extension}. Allowed: #{ALLOWED_EXTENSIONS.join(', ')}\"    end        # Check file size    file_size = File.size(file_path)    if file_size &gt; MAX_FILE_SIZE      raise FileSizeError, \"File too large: #{file_size} bytes (max: #{MAX_FILE_SIZE})\"    end        # Process the file    File.open(file_path, 'r') do |file|      case extension      when '.txt'        process_text_file(file)      when '.csv'        process_csv_file(file)      when '.json'        process_json_file(file)      end    end  end    def process_text_file(file)    content = file.read    # Simulate processing    raise FileProcessingError, \"Text file is empty\" if content.strip.empty?        # Count lines and words    lines = content.lines.count    words = content.split.count    puts \"  Text file stats: #{lines} lines, #{words} words\"  end    def process_csv_file(file)    lines = file.readlines    raise FileProcessingError, \"CSV file has no data rows\" if lines.length &lt; 2        puts \"  CSV file stats: #{lines.length - 1} data rows\"  end    def process_json_file(file)    require 'json'    content = file.read        begin      data = JSON.parse(content)      puts \"  JSON file stats: #{data.keys.count if data.is_a?(Hash)} top-level keys\"    rescue JSON::ParserError =&gt; e      raise FileProcessingError, \"Invalid JSON format: #{e.message}\"    end  end    def log_error(message)    File.open('file_processing_errors.log', 'a') do |log_file|      log_file.puts \"[#{Time.now}] #{message}\"    end  rescue =&gt; e    puts \"Failed to write to log file: #{e.message}\"  end    def generate_report    puts \"\\n\" + \"=\"*50    puts \"FILE PROCESSING REPORT\"    puts \"=\"*50    puts \"Processed successfully: #{@processed_files.count}\"    puts \"Failed to process: #{@failed_files.count}\"        unless @failed_files.empty?      puts \"\\nFailed files:\"      @failed_files.each do |failure|        puts \"  ✗ #{failure[:path]}\"        puts \"    Error: #{failure[:error]} (#{failure[:type]})\"      end    end        {      successful: @processed_files,      failed: @failed_files,      total: @processed_files.count + @failed_files.count    }  endend# Create sample files for demonstrationsample_files = []# Valid filesFile.write('sample.txt', \"This is a sample text file.\\nWith multiple lines.\")File.write('data.csv', \"name,age\\nJohn,25\\nJane,30\")File.write('config.json', '{\"app\": \"demo\", \"version\": \"1.0\"}')# Invalid filesFile.write('large_file.txt', \"x\" * (11 * 1024 * 1024))  # Too largeFile.write('invalid.json', '{\"invalid\": json}')  # Invalid JSONFile.write('empty.txt', '')  # Empty filesample_files = [  'sample.txt', 'data.csv', 'config.json',  'large_file.txt', 'invalid.json', 'empty.txt',  'nonexistent.txt', 'document.pdf'  # Non-existent and unsupported type]# Process files with exception handlingprocessor = FileProcessor.new(log_errors: true)report = processor.process_files(sample_files)# Cleanup sample files['sample.txt', 'data.csv', 'config.json', 'large_file.txt', 'invalid.json', 'empty.txt'].each do |file|  File.delete(file) if File.exist?(file)endPractical Example 2: Network Request Handler with Retry Logicrequire 'net/http'require 'uri'require 'json'class NetworkRequestHandler  class NetworkError &lt; StandardError; end  class TimeoutError &lt; NetworkError; end  class ServerError &lt; NetworkError; end  class ClientError &lt; NetworkError; end    def initialize(max_retries: 3, timeout: 10)    @max_retries = max_retries    @timeout = timeout  end    def fetch_with_retry(url, method: :get, payload: nil)    attempt = 1        begin      puts \"Attempt #{attempt}: Fetching #{url}\"      response = make_request(url, method, payload)            case response.code.to_i      when 200..299        puts \"✓ Success (#{response.code})\"        return parse_response(response)      when 400..499        raise ClientError, \"Client error (#{response.code}): #{response.message}\"      when 500..599        raise ServerError, \"Server error (#{response.code}): #{response.message}\"      else        raise NetworkError, \"Unexpected response (#{response.code}): #{response.message}\"      end          rescue Timeout::Error      raise TimeoutError, \"Request timed out after #{@timeout} seconds\"    rescue ServerError, TimeoutError =&gt; e      # Retry on server errors and timeouts      if attempt &lt;= @max_retries        wait_time = 2 ** (attempt - 1)  # Exponential backoff        puts \"  ✗ #{e.message}\"        puts \"  Retrying in #{wait_time} seconds... (#{attempt}/#{@max_retries})\"        sleep(wait_time)        attempt += 1        retry      else        puts \"  ✗ Max retries exceeded\"        raise e      end    rescue ClientError, NetworkError =&gt; e      # Don't retry on client errors      puts \"  ✗ #{e.message}\"      raise e    rescue =&gt; e      # Handle unexpected errors      puts \"  ✗ Unexpected error: #{e.message}\"      raise NetworkError, \"Unexpected error: #{e.message}\"    end  end    def fetch_multiple(urls)    results = {}        urls.each do |url|      begin        results[url] = fetch_with_retry(url)      rescue NetworkError =&gt; e        results[url] = { error: e.message, type: e.class.name }      rescue =&gt; e        results[url] = { error: e.message, type: \"UnexpectedError\" }      end    end        generate_summary(results)  end    private    def make_request(url, method, payload)    uri = URI(url)        Net::HTTP.start(uri.host, uri.port,                    use_ssl: uri.scheme == 'https',                   open_timeout: @timeout,                   read_timeout: @timeout) do |http|            case method      when :get        http.get(uri.path.empty? ? '/' : uri.path)      when :post        request = Net::HTTP::Post.new(uri.path)        request.body = payload.to_json if payload        request['Content-Type'] = 'application/json'        http.request(request)      else        raise ArgumentError, \"Unsupported HTTP method: #{method}\"      end    end  end    def parse_response(response)    content_type = response['content-type'] || ''        if content_type.include?('application/json')      begin        JSON.parse(response.body)      rescue JSON::ParserError =&gt; e        raise NetworkError, \"Invalid JSON response: #{e.message}\"      end    else      {        content_type: content_type,        body_length: response.body.length,        headers: response.to_hash      }    end  end    def generate_summary(results)    successful = results.count { |_, result| !result.key?(:error) }    failed = results.count { |_, result| result.key?(:error) }        puts \"\\n\" + \"=\"*50    puts \"NETWORK REQUEST SUMMARY\"    puts \"=\"*50    puts \"Total requests: #{results.count}\"    puts \"Successful: #{successful}\"    puts \"Failed: #{failed}\"        unless failed.zero?      puts \"\\nFailed requests:\"      results.each do |url, result|        if result.key?(:error)          puts \"  ✗ #{url}\"          puts \"    #{result[:error]} (#{result[:type]})\"        end      end    end        results  endend# Example usage with various URLs (some will fail)urls = [  'https://jsonplaceholder.typicode.com/posts/1',  # Valid JSON API  'https://www.google.com',                        # Valid HTML  'https://httpstat.us/500',                       # Server error (for retry demo)  'https://httpstat.us/404',                       # Client error (no retry)  'https://nonexistent-domain-12345.com',          # DNS error]handler = NetworkRequestHandler.new(max_retries: 2, timeout: 5)puts \"Fetching multiple URLs with exception handling and retry logic:\"results = handler.fetch_multiple(urls)Method-Level Exception HandlingRuby allows you to add rescue clauses directly to methods:def risky_method  # method body  perform_operationrescue ArgumentError =&gt; e  puts \"Invalid argument: #{e.message}\"  return nilrescue =&gt; e  puts \"Unexpected error: #{e.message}\"  raise  # Re-raise if you can't handle itend# This is equivalent to:def risky_method  begin    perform_operation  rescue ArgumentError =&gt; e    puts \"Invalid argument: #{e.message}\"    return nil  rescue =&gt; e    puts \"Unexpected error: #{e.message}\"    raise  endendException Information and BacktraceRuby exceptions carry useful information for debugging:begin  raise \"Something went wrong!\"rescue =&gt; e  puts \"Exception class: #{e.class}\"  puts \"Exception message: #{e.message}\"  puts \"Backtrace:\"  puts e.backtrace.first(5)  # Show first 5 lines of backtrace    # Full backtrace  puts \"\\nFull backtrace:\"  e.backtrace.each_with_index do |line, index|    puts \"  #{index}: #{line}\"  endendCatch and Throw (Non-Local Exits)Ruby provides catch and throw for non-local exits, which are different from exceptions:def find_user(users, target_name)  catch(:found) do    users.each do |user|      user[:friends].each do |friend|        throw(:found, friend) if friend[:name] == target_name      end    end    nil  # Not found  endend# Usageusers = [  { name: \"Alice\", friends: [{ name: \"Bob\" }, { name: \"Charlie\" }] },  { name: \"David\", friends: [{ name: \"Eve\" }, { name: \"Frank\" }] }]result = find_user(users, \"Charlie\")puts result ? \"Found: #{result[:name]}\" : \"Not found\"Common Exception Types and When to Use ThemBuilt-in Exceptions:# ArgumentError - Invalid argumentsdef divide(a, b)  raise ArgumentError, \"Arguments must be numbers\" unless a.is_a?(Numeric) &amp;&amp; b.is_a?(Numeric)  raise ZeroDivisionError, \"Cannot divide by zero\" if b.zero?  a / bend# TypeError - Wrong typedef process_array(arr)  raise TypeError, \"Expected Array, got #{arr.class}\" unless arr.is_a?(Array)  arr.map(&amp;:to_s)end# RuntimeError - General runtime errorsdef validate_state  raise \"Invalid application state\" unless valid_state?end# IOError - Input/output errorsdef read_config  raise IOError, \"Config file is corrupted\" unless valid_config_format?endBest Practices for Exception Handling1. Be Specific with Exception Types# Bad - too genericbegin  operationrescue  puts \"Something went wrong\"end# Good - specific handlingbegin  operationrescue ArgumentError =&gt; e  puts \"Invalid input: #{e.message}\"rescue IOError =&gt; e  puts \"File operation failed: #{e.message}\"rescue =&gt; e  puts \"Unexpected error: #{e.message}\"  raise  # Re-raise if you can't handle it properlyend2. Don’t Ignore Exceptions# Bad - silently ignoring errorsbegin  risky_operationrescue  # Silent failureend# Good - at least log the errorbegin  risky_operationrescue =&gt; e  logger.error \"Operation failed: #{e.message}\"  # Handle appropriately or re-raiseend3. Use Custom Exceptions for Domain Logicclass BankAccount  class InsufficientFundsError &lt; StandardError    attr_reader :requested_amount, :available_balance        def initialize(requested, available)      @requested_amount = requested      @available_balance = available      super(\"Insufficient funds: requested #{requested}, available #{available}\")    end  end    def withdraw(amount)    raise InsufficientFundsError.new(amount, @balance) if amount &gt; @balance    @balance -= amount  endend4. Always Clean Up Resourcesdef process_file(filename)  file = File.open(filename)  begin    # Process file    process_data(file.read)  ensure    file.close if file  endend# Or better, use blocks that auto-closedef process_file(filename)  File.open(filename) do |file|    process_data(file.read)  end  # File automatically closedendKey Takeaways  Use specific exception types rather than generic rescue clauses  Create custom exceptions for domain-specific errors  Always clean up resources using ensure or block syntax  Don’t ignore exceptions - at minimum, log them  Use retry logic for transient failures (network, database)  Provide meaningful error messages for debugging  Re-raise exceptions you can’t handle properly  Use catch/throw for control flow, not error handlingException handling is crucial for building robust Ruby applications. By understanding the exception hierarchy, using appropriate rescue strategies, and following best practices, you can create applications that gracefully handle errors and provide excellent user experiences even when things go wrong."
  },
  
  {
    "title": "Regex in Ruby",
    "url": "/posts/regex-in-ruby/",
    "categories": "Ruby, regex",
    "tags": "ruby, regex",
    "date": "2025-06-05 11:00:00 +0545",
    





    
    "snippet": "Ruby Regex: Pattern Matching Made SimpleRegular expressions (regex) in Ruby are powerful tools for pattern matching and text manipulation. Let’s dive into the essentials with practical examples you...",
    "content": "Ruby Regex: Pattern Matching Made SimpleRegular expressions (regex) in Ruby are powerful tools for pattern matching and text manipulation. Let’s dive into the essentials with practical examples you can use immediately.Basic SyntaxRuby provides two ways to create regex patterns:# Literal notationpattern = /hello/# Constructor methodpattern = Regexp.new(\"hello\")Common Matching Methodsmatch - Returns MatchData objectemail = \"user@example.com\"result = email.match(/@(.+)\\.(.+)/)puts result[1]  # \"example\"puts result[2]  # \"com\"=~ - Returns position of matchtext = \"The price is $25\"position = text =~ /\\$\\d+/puts position  # 13scan - Returns all matchestext = \"Call me at 123-456-7890 or 987-654-3210\"phones = text.scan(/\\d{3}-\\d{3}-\\d{4}/)puts phones  # [\"123-456-7890\", \"987-654-3210\"]Essential Patterns            Pattern      Meaning      Example                  \\d      Digit      /\\d{3}/ matches “123”              \\w      Word character      /\\w+/ matches “hello”              \\s      Whitespace      /\\s+/ matches spaces              .      Any character      /h.llo/ matches “hello”              ^      Start of string      /^Hello/              $      End of string      /world$/      Practical Examples1. Email Validationdef valid_email?(email)  email.match?(/\\A[\\w+\\-.]+@[a-z\\d\\-]+(\\.[a-z\\d\\-]+)*\\.[a-z]+\\z/i)endputs valid_email?(\"test@example.com\")  # trueputs valid_email?(\"invalid-email\")     # false2. Phone Number Extractiontext = \"Contact: (555) 123-4567 or 555.987.6543\"phones = text.scan(/\\(?\\d{3}\\)?[\\s.-]?\\d{3}[\\s.-]?\\d{4}/)puts phones  # [\"(555) 123-4567\", \"555.987.6543\"]3. URL Parserurl = \"https://www.example.com/path?param=value\"match = url.match(/^(https?):\\/\\/([^\\/]+)(\\/[^?]*)?\\??(.*)$/)puts \"Protocol: #{match[1]}\"  # httpsputs \"Domain: #{match[2]}\"    # www.example.computs \"Path: #{match[3]}\"      # /pathputs \"Query: #{match[4]}\"     # param=value4. Password Strength Checkerdef strong_password?(password)  return false if password.length &lt; 8  return false unless password.match?(/[A-Z]/)     # uppercase  return false unless password.match?(/[a-z]/)     # lowercase  return false unless password.match?(/\\d/)        # digit  return false unless password.match?(/[!@#$%^&amp;*]/) # special char  trueendputs strong_password?(\"MyPass123!\")  # trueputs strong_password?(\"weak\")        # false5. Text Cleaning and Replacement# Remove extra whitespacetext = \"Too    many     spaces\"clean = text.gsub(/\\s+/, \" \")puts clean  # \"Too many spaces\"# Extract hashtagstweet = \"Loving #ruby and #programming today!\"hashtags = tweet.scan(/#\\w+/)puts hashtags  # [\"#ruby\", \"#programming\"]# Mask credit card numberscard = \"My card number is 1234-5678-9012-3456\"masked = card.gsub(/\\d{4}-\\d{4}-\\d{4}-(\\d{4})/, \"****-****-****-\\\\1\")puts masked  # \"My card number is ****-****-****-3456\"ModifiersAdd flags after the closing / to modify behavior:# Case insensitive/hello/i.match(\"HELLO\")  # matches# Multiline mode/^start/m.match(\"line1\\nstart here\")  # matches# Extended mode (ignore whitespace)pattern = /  \\d{3}    # area code  -        # separator  \\d{4}    # number/xQuick Tips  Use match? for boolean checks - it’s faster than match when you only need true/false  Escape special characters with backslash: \\., \\$, \\(  Use raw strings for complex patterns: %r{pattern} instead of /pattern/  Test your regex - use online tools or IRB to verify patternsCommon Gotchas# Wrong: . matches any character\"hello world\".match(/hello.world/)  # matches \"hello world\"# Right: escape the dot\"hello.world\".match(/hello\\.world/)  # matches \"hello.world\"# Wrong: greedy matching\"&lt;tag&gt;content&lt;/tag&gt;\".match(/&lt;.+&gt;/)  # matches entire string# Right: non-greedy matching\"&lt;tag&gt;content&lt;/tag&gt;\".match(/&lt;.+?&gt;/)  # matches \"&lt;tag&gt;\"Ruby’s regex engine is both powerful and intuitive. Start with these examples and gradually build more complex patterns as needed. Remember: readable code is better than clever regex - use them wisely!"
  },
  
  {
    "title": "Ruby Enumerable group_by",
    "url": "/posts/ruby-group-by/",
    "categories": "Ruby, group_by",
    "tags": "ruby, group_by",
    "date": "2025-06-04 22:25:58 +0545",
    





    
    "snippet": "Ruby GroupingRuby provides several powerful methods for grouping and organizing data, making it easy to transform collections into structured formats. These grouping methods are part of Ruby’s Enum...",
    "content": "Ruby GroupingRuby provides several powerful methods for grouping and organizing data, making it easy to transform collections into structured formats. These grouping methods are part of Ruby’s Enumerable module, which means they’re available on arrays, hashes, ranges, and any other object that includes Enumerable. Whether you’re working with arrays of objects, processing user data, or analyzing datasets, Ruby’s grouping methods can simplify complex data manipulation tasks.The group_by MethodThe most commonly used grouping method in Ruby is group_by, which comes from the Enumerable module. It creates a hash where keys are the result of the block evaluation and values are arrays of elements that share the same key. An important characteristic is that the order of elements within each group is preserved from the original collection.Syntaxenumerable.group_by { |element| criterion }# Group words by their lengthwords = ['apple', 'banana', 'cherry', 'date', 'elderberry']grouped = words.group_by(&amp;:length)# =&gt; {5=&gt;[\"apple\", \"cherry\"], 6=&gt;[\"banana\"], 4=&gt;[\"date\"], 10=&gt;[\"elderberry\"]}# Group numbers by parity (even/odd)numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]parity_groups = numbers.group_by { |number| number % 2 }# =&gt; {1=&gt;[1, 3, 5, 7, 9], 0=&gt;[2, 4, 6, 8, 10]}# Group strings by their starting letterstrings = [\"apple\", \"avogado\", \"donkey\", \"dirt\", \"scaler\"]letter_groups = strings.group_by { |string| string[0] }# =&gt; {\"a\"=&gt;[\"apple\", \"avogado\"], \"d\"=&gt;[\"donkey\", \"dirt\"], \"s\"=&gt;[\"scaler\"]}# Group people by age categorypeople = [  { name: 'Alloy', age: 25 },  { name: 'Bobby', age: 35 },  { name: 'Charlie', age: 28 },  { name: 'Donny', age: 42 }]age_groups = people.group_by do |person|  case person[:age]  when 18..30 then 'young'  when 31..40 then 'middle'  else 'senior'  endend# =&gt; {\"young\"=&gt;[{:name=&gt;\"Alloy\", :age=&gt;25}, {:name=&gt;\"Charlie\", :age=&gt;28}], #     \"middle\"=&gt;[{:name=&gt;\"Bobby\", :age=&gt;35}], #     \"senior\"=&gt;[{:name=&gt;\"Donny\", :age=&gt;42}]}# Working with custom objectsclass Person  attr_accessor :name, :age    def initialize(name, age)    @name = name    @age = age  endendpeople_objects = [  Person.new(\"Alice\", 25),  Person.new(\"Bob\", 30),  Person.new(\"Charlie\", 25)]age_based_groups = people_objects.group_by(&amp;:age)# Groups Person objects by their age attributeThe chunk MethodFor more complex grouping scenarios, chunk groups consecutive elements that return the same value from the block. This is particularly useful when working with sorted data.# Group consecutive numbersnumbers = [1, 1, 2, 2, 2, 3, 1, 1]chunks = numbers.chunk(&amp;:itself).to_a# =&gt; [[1, [1, 1]], [2, [2, 2, 2]], [3, [3]], [1, [1, 1]]]# Group transactions by daytransactions = [  { date: '2024-01-01', amount: 100 },  { date: '2024-01-01', amount: 50 },  { date: '2024-01-02', amount: 75 },  { date: '2024-01-02', amount: 200 }]daily_transactions = transactions.chunk { |t| t[:date] }.to_h# =&gt; {\"2024-01-01\"=&gt;[{:date=&gt;\"2024-01-01\", :amount=&gt;100}, {:date=&gt;\"2024-01-01\", :amount=&gt;50}], #     \"2024-01-02\"=&gt;[{:date=&gt;\"2024-01-02\", :amount=&gt;75}, {:date=&gt;\"2024-01-02\", :amount=&gt;200}]}The partition MethodWhen you need to split a collection into exactly two groups based on a condition, partition is your friend.numbers = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]evens, odds = numbers.partition(&amp;:even?)# evens =&gt; [2, 4, 6, 8, 10]# odds =&gt; [1, 3, 5, 7, 9]# Separate active and inactive usersusers = [  { name: 'John', active: true },  { name: 'Jane', active: false },  { name: 'Bob', active: true }]active_users, inactive_users = users.partition { |user| user[:active] }Advanced Grouping with slice_whenThe slice_when method creates groups by splitting the enumerable whenever the block returns true for consecutive elements.# Group ascending sequencesnumbers = [1, 2, 3, 1, 2, 4, 5, 2, 3]ascending_groups = numbers.slice_when { |a, b| a &gt;= b }.to_a# =&gt; [[1, 2, 3], [1, 2, 4, 5], [2, 3]]Working with Different EnumerablesSince these methods are part of the Enumerable module, they work with various data structures:# Arrays (most common)[1, 2, 3, 4].group_by(&amp;:even?)# Ranges(1..10).group_by { |n| n % 3 }# Hashes (groups by key-value pairs){ a: 1, b: 2, c: 1 }.group_by { |key, value| value }# Custom objects that include Enumerableclass NumberCollection  include Enumerable    def initialize(numbers)    @numbers = numbers  end    def each    @numbers.each { |n| yield n }  endendcollection = NumberCollection.new([1, 2, 3, 4, 5])collection.group_by(&amp;:odd?)# =&gt; {true=&gt;[1, 3, 5], false=&gt;[2, 4]}Error Handling and Best PracticesThe group_by method itself doesn’t raise exceptions, but the criterion block can. It’s important to ensure your grouping logic is robust:# Safe grouping with error handlingdata = [1, 2, \"three\", 4, nil, 6]safe_groups = data.group_by do |item|  begin    item.even? rescue false  rescue    :invalid  endend# Groups items safely, handling non-numeric valuesAlways ensure your criterion block is predictable and doesn’t have unexpected side effects.Practical ApplicationsRuby’s grouping methods shine in real-world scenarios:  Data Analysis: Group sales data by region, product category, or time period  User Management: Organize users by role, subscription status, or activity level  File Processing: Group files by extension, size, or modification date  Report Generation: Create summaries and aggregations from raw dataPerformance ConsiderationsWhile these methods are convenient, be mindful of performance with large datasets. Consider using database-level grouping for massive collections, and remember that group_by creates a new hash, so memory usage can grow significantly with large datasets. The order of elements within each group is preserved, which adds to the method’s reliability but also its memory overhead.Key TakeawaysRuby’s Enumerable module provides elegant solutions for data organization challenges. The group_by method is particularly powerful because it:  Creates hash-based groupings with preserved element order  Works with any enumerable collection (arrays, hashes, ranges, custom objects)  Provides a clean, functional programming approach to data categorization  Handles complex grouping criteria through flexible block syntax"
  },
  
  {
    "title": "Understanding Authentication Types: A Developer's Guide to Securing Applications",
    "url": "/posts/understanding-authentication-types/",
    "categories": "security, authentication, webdev",
    "tags": "auth, security, oauth, jwt, session, api",
    "date": "2025-05-29 05:13:00 +0545",
    





    
    "snippet": "Authentication is the cornerstone of application security, determining how users prove their identity to access protected resources. With the evolution of web applications and APIs, various authent...",
    "content": "Authentication is the cornerstone of application security, determining how users prove their identity to access protected resources. With the evolution of web applications and APIs, various authentication methods have emerged, each with distinct advantages and use cases. This comprehensive guide explores the most common authentication types, their implementations, and when to use each approach.What is Authentication?Authentication is the process of verifying that a user is who they claim to be. It differs from authorization, which determines what an authenticated user is allowed to do. Think of authentication as checking someone’s ID at a nightclub entrance, while authorization is determining whether they can access the VIP section.1. Basic AuthenticationBasic Authentication is the simplest HTTP authentication scheme, where credentials are sent with each request.How It Works  Username and password are combined with a colon separator  The string is Base64 encoded  Sent in the Authorization header as Basic &lt;encoded-credentials&gt;ExampleAuthorization: Basic dXNlcm5hbWU8cGFzc3dvcmQ=Pros and ConsAdvantages:  Simple to implement  Widely supported  No server-side session storage requiredDisadvantages:  Credentials sent with every request  Base64 is not encryption (easily decoded)  Requires HTTPS for security  No logout mechanismWhen to UseBasic Authentication works well for simple APIs, internal tools, or development environments where simplicity is prioritized over advanced security features.2. Session-Based AuthenticationSession-based authentication creates a server-side session after successful login, with the client storing a session identifier.How It Works  User submits login credentials  Server validates credentials  Server creates a session and stores user data  Server sends session ID to client (usually via cookie)  Client includes session ID in subsequent requests  Server validates session ID and retrieves user dataImplementation Example  // Server-side session creation  app.post('/login', (req, res) =&gt; {    const { username, password } = req.body;        if (validateCredentials(username, password)) {      const sessionId = generateSessionId();      sessions[sessionId] = { username, loginTime: new Date() };      res.cookie('sessionId', sessionId, { httpOnly: true, secure: true });      res.json({ success: true });    }  });  // Session validation middleware  function authenticateSession(req, res, next) {    const sessionId = req.cookies.sessionId;    if (sessions[sessionId]) {      req.user = sessions[sessionId];      next();    } else {      res.status(401).json({ error: 'Invalid session' });    }  }Pros and ConsAdvantages:  Server has full control over sessions  Can revoke sessions immediately  Familiar to developers  Works well with traditional web applicationsDisadvantages:  Server must store session data  Scaling challenges across multiple servers  CSRF vulnerability concerns  Not ideal for APIs consumed by mobile appsWhen to UseSession-based authentication is excellent for traditional web applications where users interact primarily through browsers and you need fine-grained session control.3. Token-Based Authentication (JWT)JSON Web Tokens (JWT) provide a stateless authentication mechanism where the token itself contains user information.How It Works  User submits login credentials  Server validates credentials  Server creates a JWT containing user claims  Client stores JWT (localStorage, sessionStorage, or cookie)  Client includes JWT in Authorization header for requests  Server validates JWT signature and extracts user informationJWT StructureA JWT consists of three parts separated by dots:  Header: Token type and signing algorithm  Payload: Claims (user data)  Signature: Ensures token integrityImplementation Example  const jwt = require('jsonwebtoken');  // Generate JWT  app.post('/login', (req, res) =&gt; {    const { username, password } = req.body;        if (validateCredentials(username, password)) {      const token = jwt.sign(        { username, role: 'user' },        process.env.JWT_SECRET,        { expiresIn: '1h' }      );      res.json({ token });    }  });  // JWT validation middleware  function authenticateToken(req, res, next) {    const authHeader = req.headers['authorization'];    const token = authHeader &amp;&amp; authHeader.split(' ')[1];        if (!token) {      return res.status(401).json({ error: 'Token required' });    }        jwt.verify(token, process.env.JWT_SECRET, (err, user) =&gt; {      if (err) {        return res.status(403).json({ error: 'Invalid token' });      }      req.user = user;      next();    });  }Pros and ConsAdvantages:  Stateless (no server-side storage)  Scales easily across multiple servers  Self-contained user information  Works well with SPAs and mobile apps  Can include custom claimsDisadvantages:  Cannot revoke tokens before expiration  Larger than session IDs  Token exposed if XSS vulnerability exists  Clock synchronization issuesWhen to UseJWT is ideal for APIs, single-page applications, microservices, and mobile applications where stateless authentication is preferred and you need to scale across multiple servers.4. OAuth 2.0OAuth 2.0 is an authorization framework that enables applications to obtain limited access to user accounts on third-party services.OAuth 2.0 FlowsAuthorization Code FlowThe most secure flow for web applications:  Client redirects user to authorization server  User authenticates and grants permission  Authorization server redirects back with authorization code  Client exchanges code for access token  Client uses access token to access protected resourcesImplicit Flow (Deprecated)Previously used for SPAs, now discouraged due to security concerns.Client Credentials FlowFor server-to-server authentication:  const params = new URLSearchParams();  params.append('grant_type', 'client_credentials');  params.append('client_id', CLIENT_ID);  params.append('client_secret', CLIENT_SECRET);  const response = await fetch('/oauth/token', {    method: 'POST',    body: params  });When to Use OAuth 2.0  Integrating with third-party services (Google, Facebook, GitHub)  Building APIs that need granular permissions  Implementing “Login with…” functionality  Microservices requiring service-to-service authentication5. Multi-Factor Authentication (MFA)MFA adds additional security layers beyond username/password combinations.Common MFA Methods  SMS/Email codes: Temporary codes sent to registered devices  TOTP (Time-based One-Time Password): Apps like Google Authenticator  Hardware tokens: Physical devices like YubiKey  Biometrics: Fingerprints, facial recognition  Push notifications: Approve login from mobile appImplementation Considerations  // TOTP verification example  const speakeasy = require('speakeasy');  function verifyTOTP(token, secret) {    return speakeasy.totp.verify({      secret: secret,      encoding: 'base32',      token: token,      window: 2 // Allow for time drift    });  }6. Single Sign-On (SSO)SSO allows users to authenticate once and access multiple applications without re-entering credentials.Common SSO Protocols  SAML 2.0: XML-based, enterprise-focused  OpenID Connect: Built on OAuth 2.0, JSON-based  CAS: Central Authentication ServiceBenefits  Improved user experience  Centralized access control  Reduced password fatigue  Enhanced security through centralized policiesSecurity Best PracticesRegardless of the authentication method chosen, follow these security guidelines:General Security Measures  Always use HTTPS in production  Implement proper password policies  Use secure session configuration  Implement rate limiting for login attempts  Log authentication events for monitoringToken Security  Use short expiration times for access tokens  Implement token refresh mechanisms  Store tokens securely (avoid localStorage for sensitive apps)  Use proper CORS configurationSession Security  app.use(session({    secret: process.env.SESSION_SECRET,    resave: false,    saveUninitialized: false,    cookie: {      secure: true,      // HTTPS only      httpOnly: true,    // Prevent XSS      maxAge: 3600000,   // 1 hour      sameSite: 'strict' // CSRF protection    }  }));Choosing the Right Authentication MethodThe choice of authentication method depends on your application’s requirements:Use Basic Authentication When:  Building simple APIs or internal tools  Implementing machine-to-machine communication  Rapid prototyping or development environmentsUse Session-Based Authentication When:  Building traditional web applications  You need immediate session revocation  Users primarily access via browsers  You have a single server or sticky sessionsUse JWT When:  Building APIs for mobile or SPA consumption  You need stateless authentication  Implementing microservices architecture  You require custom claims in tokensUse OAuth 2.0 When:  Integrating with third-party services  Building public APIs with granular permissions  Implementing social login features  You need delegated authorizationConclusionAuthentication is not a one-size-fits-all solution. Understanding the strengths and limitations of each approach enables you to make informed decisions based on your application’s specific needs. Modern applications often combine multiple authentication methods – using OAuth for third-party integration, JWT for API access, and MFA for enhanced security.As security threats evolve, staying current with authentication best practices and emerging standards like WebAuthn and passwordless authentication will help ensure your applications remain secure and user-friendly.The key is to balance security, user experience, and implementation complexity while always prioritizing the protection of user data and system integrity."
  },
  
  {
    "title": "The Complete Guide to AWS, GCP, and Azure for Web Developers",
    "url": "/posts/cloud-services-guide-for-web-devs/",
    "categories": "cloud webdev devops aws gcp azure",
    "tags": "aws, gcp, azure, cloud, devops, web development, deployment",
    "date": "2025-05-29 02:30:00 +0545",
    





    
    "snippet": "A practical guide to the core services of AWS, GCP, and Azure with step-by-step instructions for developers building and deploying modern web applications.☁️ The Complete Guide to AWS, GCP, and Azu...",
    "content": "A practical guide to the core services of AWS, GCP, and Azure with step-by-step instructions for developers building and deploying modern web applications.☁️ The Complete Guide to AWS, GCP, and Azure for Web Developers🚀 IntroductionWhether you’re building your first full-stack app, deploying production services, or diving into the world of DevOps, understanding cloud platforms is a crucial part of modern web development.This guide explores the three major cloud platforms — Amazon Web Services (AWS), Google Cloud Platform (GCP), and Microsoft Azure — and walks through essential services and how to work with them hands-on.From hosting your app to managing storage, databases, and serverless functions, this guide gives you a practical roadmap to mastering cloud concepts that apply across projects, companies, and use cases.🧭 What You’ll Learn            Topic      AWS      GCP      Azure                  Virtual Machines      EC2      Compute Engine      Virtual Machines              Static File Storage      S3      Cloud Storage      Blob Storage              Relational Databases      RDS      Cloud SQL      Azure SQL Database              Serverless Functions      Lambda      Cloud Functions      Azure Functions              App Hosting      Elastic Beanstalk      App Engine      App Services              DNS and Domain Routing      Route 53      Cloud DNS      Azure DNS              Identity &amp; Access Control      IAM      IAM      Azure Active Directory      🌐 Virtual Machines (Compute)💡 ConceptVirtual Machines are the backbone of the cloud. They let you run any OS with full control, ideal for hosting backend services or custom environments.✅ Launching a VM🔸 AWS EC2# Launch EC2 instance via Console &gt; Choose Ubuntussh -i \"your-key.pem\" ubuntu@&lt;your-ec2-ip&gt;sudo apt updatesudo apt install nodejs npm🔸 GCP Compute Engine# Launch VM from Console &gt; Use browser-based SSHsudo apt updatesudo apt install nodejs npm🔸 Azure Virtual Machines# Create a VM via Azure Portalssh azureuser@&lt;your-vm-ip&gt;sudo apt updatesudo apt install nodejs npm📁 Object Storage💡 ConceptObject storage is used to store large files like images, PDFs, backups, or static website content.✅ Uploading Files🔸 AWS S3aws s3 mb s3://my-bucket-nameaws s3 cp image.png s3://my-bucket-name🔸 GCP Cloud Storagegsutil mb gs://my-bucket-namegsutil cp image.png gs://my-bucket-name🔸 Azure Blob Storageaz storage blob upload \\  --account-name mystorage \\  --container-name mycontainer \\  --file image.png \\  --name image.png🧮 Relational Databases💡 ConceptRelational databases store structured data using SQL. Cloud platforms offer managed databases with automatic backups and scalability.✅ PostgreSQL Setup🔸 AWS RDS  Create RDS (PostgreSQL) from Console  Configure VPC/Security Group to allow port 5432psql -h your-rds-endpoint -U dbuser -d dbname🔸 GCP Cloud SQLgcloud sql instances create my-postgres \\  --database-version=POSTGRES_13 --tier=db-f1-micro🔸 Azure SQL Databasesqlcmd -S server-name.database.windows.net -U username -P password⚡ Serverless Functions💡 ConceptServerless lets you run code without managing servers. Define a function, set triggers (like HTTP or storage events), and the cloud handles the rest.✅ Deploying a Function🔸 AWS Lambda (Console)  Create Lambda function  Choose Node.js or Python or RoR  Trigger via API Gateway or S3🔸 GCP Cloud Functionsgcloud functions deploy helloWorld \\  --runtime nodejs18 \\  --trigger-http \\  --allow-unauthenticated🔸 Azure Functions (CLI)func init MyFuncProj --worker-runtime nodecd MyFuncProjfunc new --name HelloHttp --template \"HTTP trigger\"func start🌍 Application Hosting💡 ConceptDeploy and scale apps easily using managed hosting platforms. Ideal for web apps, APIs, or microservices.✅ Hosting a Node.js App🔸 AWS Elastic Beanstalkeb init -p node.js my-appeb create my-enveb deploy🔸 GCP App Enginegcloud app creategcloud app deploy🔸 Azure App Servicesaz webapp up --name mywebapp123 \\  --runtime \"NODE|18-lts\" \\  --location \"eastus\"🔐 Identity &amp; Access Management (IAM)💡 ConceptIAM defines who can access what in your cloud. It includes users, roles, policies, and permissions.            Cloud      Features                  AWS IAM      Fine-grained roles/policies              GCP IAM      Role-based access via principals              Azure AD      Role assignments + AD groups      Best Practices:  Use least privilege access  Prefer roles over root/admin accounts  Rotate keys regularly🔧 Developer Tools to Install            Tool      Command (macOS)                  AWS CLI      `brew install awscli`              GCP SDK      `brew install –cask google-cloud-sdk`              Azure CLI      `brew install azure-cli`              Terraform      `brew install terraform`              Docker      `brew install –cask docker`      🧠 Tips to Practice Cloud Skills  ✅ Deploy a simple Node.js/Python/RoR app on a VM  ✅ Move to serverless when comfortable  ✅ Store logs and media in object storage  ✅ Attach a cloud-managed SQL database  ✅ Use IAM to test permission boundariesInitial VM DeploymentFor the VM deployment, you have several straightforward options:  Traditional servers (DigitalOcean, Linode, AWS EC2)  Platform-as-a-Service (Heroku, Railway, Render)  Container platforms (Docker on any cloud provider)The PaaS options are often easiest for getting started since they handle most infrastructure concerns automatically.Migration to ServerlessWhen you’re ready to go serverless, you have a few architectural approaches:Function-based approach:  Break your Rails app into smaller functions using AWS Lambda, Google Cloud Functions, or similar  This requires significant refactoring since Rails is designed as a monolithic framework  You’d typically extract specific endpoints or services into individual functionsContainer-based serverless:  Use services like AWS Fargate, Google Cloud Run, or Azure Container Instances  Package your Rails app in a container that scales to zero when not in use  Minimal code changes required - mainly configuration adjustments  Still gets you the serverless benefits of automatic scaling and pay-per-useHybrid approach:  Keep core Rails app on containers/VMs  Extract specific heavy or intermittent workloads (image processing, report generation, etc.) into serverless functions  Use message queues or webhooks to connect themHere’s a practical example of extracting heavy workloads from a Rails app into serverless functions:Current Rails Implementation# In your Rails app - products_controller.rbclass ProductsController &lt; ApplicationController  def create    @product = Product.new(product_params)    if @product.save      # This blocks the request for 10-30 seconds      ImageProcessorService.new(@product).process_images      redirect_to @product    end  endend# Heavy service that blocks requestsclass ImageProcessorService  def process_images    # Resize original image    # Generate 5 different thumbnail sizes    # Optimize for web    # This takes 15-30 seconds per product  endendAfter Serverless Migration# products_controller.rb - Now fast and responsiveclass ProductsController &lt; ApplicationController  def create    @product = Product.new(product_params)    if @product.save      # Queue the heavy work instead of doing it synchronously      ImageProcessingJob.perform_later(@product.id)      redirect_to @product, notice: \"Product created! Images are being processed.\"    end  endend# Simple job that triggers serverless functionclass ImageProcessingJob &lt; ApplicationJob  def perform(product_id)    # Trigger AWS Lambda function    lambda_client = Aws::Lambda::Client.new        response = lambda_client.invoke({      function_name: 'image-processor', # ← This name references the deployed function      payload: { product_id: product_id }.to_json    })    puts \"Lambda response: #{response.payload.read}\"  endendServerless Function (AWS Lambda)# lambda_function.rb - Deployed as separate AWS Lambdarequire 'aws-sdk-s3'require 'mini_magick'def lambda_handler(event:, context:)  product_id = event['product_id']    # Fetch product data from database  product_data = fetch_product_from_api(product_id)    # Download original image from S3  original_image = download_image(product_data['image_url'])    # Fetch from your Rails API  ## rails_api_url = \"https://your-rails-app.com/api/products/#{product_id}\"    # Process images (this heavy work now runs separately)  thumbnails = generate_thumbnails(original_image)  optimized_image = optimize_image(original_image)    # Upload processed images back to S3  upload_processed_images(product_id, thumbnails, optimized_image)    # Update product record via API  update_product_images(product_id, processed_image_urls)    { statusCode: 200, body: 'Images processed successfully' }enddef generate_thumbnails(image)  sizes = [100, 200, 400, 800, 1200]  sizes.map do |size|    MiniMagick::Image.open(image).resize(\"#{size}x#{size}&gt;\")  endendThe lambda_function.rb file needs to be deployed as an AWS Lambda function with the name ‘image-processor’.Step 1: Deploy Lambda Function# You need to package and deploy lambda_function.rb to AWS Lambda# This creates a function named 'image-processor' in AWS# Using AWS CLI or deployment tools like:aws lambda create-function \\  --function-name image-processor \\  --runtime ruby2.7 \\  --role arn:aws:iam::your-account:role/lambda-execution-role \\  --handler lambda_function.lambda_handler \\  --zip-file fileb://function.zipDeployment Script (package.sh)#!/bin/bash# Install gemsbundle install --deployment# Create deployment packagezip -r function.zip lambda_function.rb vendor/# Deploy to AWS Lambdaaws lambda update-function-code \\  --function-name image-processor \\  --zip-file fileb://function.zipStep 2: The Connection# In Rails - ImageProcessingJobclass ImageProcessingJob &lt; ApplicationJob  def perform(product_id)    lambda_client = Aws::Lambda::Client.new(region: 'us-east-1')        # This calls the AWS Lambda function named 'image-processor'    # The function name must match what you deployed to AWS    response = lambda_client.invoke({      function_name: 'image-processor',  # ← This name references the deployed function      invocation_type: 'Event',         # Async execution      payload: {        product_id: product_id,        callback_url: \"#{ENV['RAILS_APP_URL']}/webhooks/image_complete\"       }.to_json    })    Rails.logger.info \"Lambda invoked successfully: #{response.status_code}\"        puts \"Lambda response: #{response.payload.read}\"  endendStep 3: AWS Lambda ExecutionWhen Rails calls lambda_client.invoke(), AWS Lambda:  Finds the function named ‘image-processor’  Loads the deployed lambda_function.rb code  Calls the lambda_handler method with the payload  Executes all the image processing code  Returns the response back to RailsComplete Example with DeploymentFile structureserverless-functions/├── image-processor/│   ├── lambda_function.rb          # The handler code│   ├── Gemfile                     # Dependencies│   └── package.sh                  # Deployment script└── rails-app/    └── app/jobs/image_processing_job.rb  # Rails jobThe Flow in Action  User uploads product → Rails controller  Rails saves product → Database  Rails queues job → ImageProcessingJob.perform_later(product.id)  Job executes → Calls lambda_client.invoke(function_name: ‘image-processor’)  AWS receives call → Finds function named ‘image-processor’  AWS executes → Runs lambda_handler method in deployed lambda_function.rb  Lambda processes → Downloads, resizes, uploads images  Lambda completes → Returns success response to RailsThe key is that ‘image-processor’ is the name you give the function when deploying to AWS Lambda, and that same name is used in Rails to reference and invoke it.📚 SummaryCloud platforms are essential to every stage of web development — from prototype to production. Understanding core services across AWS, GCP, and Azure empowers developers to:  Build and deploy scalable apps  Work across different cloud providers  Integrate DevOps best practices into workflows"
  },
  
  {
    "title": "Design Patterns in Rails: A Practical Guide",
    "url": "/posts/rails-design-pattern-overview/",
    "categories": "Ruby on Rails, design-patterns, architecture",
    "tags": "ruby on rails, design-patterns, architecture",
    "date": "2025-05-28 18:38:00 +0545",
    





    
    "snippet": "Ruby on Rails (Rails) is known for its convention-over-configuration philosophy and rapid development capabilities. But as applications grow in complexity, simply following Rails conventions isn’t ...",
    "content": "Ruby on Rails (Rails) is known for its convention-over-configuration philosophy and rapid development capabilities. But as applications grow in complexity, simply following Rails conventions isn’t always enough. That’s where design patterns come into play.In this post, we’ll explore how common design patterns are used in Rails applications to keep code maintainable, readable, and scalable.Why Design Patterns Matter in RailsDesign patterns are proven solutions to recurring software design problems. They provide a shared vocabulary for developers and help manage complexity.While Rails promotes certain patterns out of the box (like MVC), seasoned developers often go further, adopting patterns like Service Objects, Decorators, and Form Objects to maintain clean architecture.1. Model-View-Controller (MVC)Pattern Type: ArchitecturalPurpose: Separates data, user interface, and control logic.Rails is built on MVC:  Model: Handles business logic and database interactions.  View: Renders the HTML (or other formats).  Controller: Coordinates between model and view.Tip:Keep your controllers skinny and models lean by pushing complex logic into service objects or concerns.2. Service ObjectsPattern Type: BehavioralPurpose: Encapsulate business logic that doesn’t naturally fit into models or controllers.class ProcessPayment  def initialize(order)    @order = order  end  def call    charge_customer    send_receipt  end  private  def charge_customer    # Payment logic  end  def send_receipt    # Email logic  endendUse it in your controller:ProcessPayment.new(@order).call3. Decorator PatternPattern Type: StructuralPurpose: Add responsibilities to objects without modifying their structure.In Rails, you might use the Draper gem to create decorators for models.class OrderDecorator &lt; Draper::Decorator  def formatted_total    h.number_to_currency(object.total)  endend4. Presenter/ViewModelPattern Type: StructuralPurpose: Encapsulate view-specific logic, often used in place of helpers or decorators.class DashboardPresenter  def initialize(user)    @user = user  end  def recent_orders    @user.orders.recent.limit(5)  endend5. Form ObjectsPattern Type: StructuralPurpose: Manage complex forms that interact with multiple models or validations.class SignupForm  include ActiveModel::Model  attr_accessor :user, :account_name, :email, :password  validates :email, :password, presence: true  def save    return false unless valid?    create_user_and_account  end  private  def create_user_and_account    # handle multi-model logic  endend6. Policy Objects (Pundit/Cancancan)Pattern Type: BehavioralPurpose: Manage authorization logic.class PostPolicy &lt; ApplicationPolicy  def update?    user.admin? || record.author == user  endend7. Query ObjectsPattern Type: BehavioralPurpose: Encapsulate complex ActiveRecord queries.class RecentOrdersQuery  def initialize(user)    @user = user  end  def call    @user.orders.where(\"created_at &gt; ?\", 1.week.ago)  endendWhen to Use Which Pattern?            Pattern      Use When                  Service Object      Logic doesn’t belong in model or controller              Decorator      You need to format or enhance model output for the view              Form Object      Your form touches multiple models or complex validations              Query Object      Queries get too long to be readable or reusable              Policy Object      Managing user permissions and access control      Final ThoughtsDesign patterns aren’t a silver bullet—but they are powerful tools. In Rails, the key is to use them only when they provide clear benefits. Start simple, and refactor when complexity grows."
  },
  
  {
    "title": "Deploy & Scale AI Models with Amazon SageMaker: A Comprehensive Guide",
    "url": "/posts/amazon_sagemaker_foundation_model_deploy_and_scale/",
    "categories": "Artificial Intelligence (AI), Machine Learning, AmazonSagemaker, AWS",
    "tags": "ai, ml, amazon_sagemaker, aws",
    "date": "2025-04-11 04:33:18 +0545",
    





    
    "snippet": "IntroductionIn today’s AI-driven landscape, deploying and scaling machine learning models efficiently is as crucial as developing them. Amazon SageMaker has emerged as a powerful platform that simp...",
    "content": "IntroductionIn today’s AI-driven landscape, deploying and scaling machine learning models efficiently is as crucial as developing them. Amazon SageMaker has emerged as a powerful platform that simplifies this process, offering a comprehensive suite of tools designed to streamline the entire machine learning lifecycle. This blog post will guide you through deploying foundation models, specifically focusing on how to efficiently deploy and scale models like Gemma 2-2B using Amazon SageMaker.The Anatomy of SageMaker Model DeploymentAmazon SageMaker provides multiple paths for model deployment, each catering to different requirements:  Real-time Inference - For applications requiring immediate responses  Batch Transform - For processing large datasets offline  Serverless Inference - For intermittent workloads with cost optimization  Asynchronous Inference - For long-running inference requestsLet’s explore how to deploy a foundation model using real-time inference, as shown in the screenshots.Deploying Gemma 2-2B Model Using SageMakerFrom the images, we can see a step-by-step process of deploying Google’s Gemma 2-2B model on SageMaker. Here’s what the workflow looks like:Step 1: Prepare Your EnvironmentSageMaker Studio provides a comprehensive development environment with JupyterLab integration. As seen in the screenshots, you can create notebooks and run simple code:print(\"hello world\")SageMaker Studio also offers multiple kernel options including Python 3, Glue PySpark, Glue Spark, SparkMagic PySpark, and SparkMagic Spark, allowing you to choose the right environment for your workload.Step 2: Configure Your Model DeploymentWhen deploying a model, you need to configure several settings:  Accept the license agreement - For foundation models like Gemma 2-2B, you need to accept the End User License Agreement (EULA)  Endpoint settings:          Endpoint name: jumpstart-dft-hf-llm-gemma-2-2b-20250409-100206      Instance type: ml.g5.xlarge (Default)      Initial instance count: 1      Inference type: Real-time (for sustained traffic and consistently low latency)      Step 3: Monitor DeploymentOnce deployed, you can monitor the endpoint status through CloudWatch logs. From the screenshots, we can see the logs showing:  Model downloading and initialization  Using flashdecoding with prefix caching  CUDA graph configuration  Successful model weights download  Sharded configuration waiting for completion  Web server initializationThe deployment status will show “Creating” initially, then change to “InService” once the endpoint is ready to accept inference requests.Scaling Your Model DeploymentSageMaker offers several ways to scale your model deployments:Auto ScalingSageMaker supports automatic scaling of endpoints based on workload, allowing you to:  Configure scaling policies based on metrics like invocation count, latency, or CPU utilization  Set minimum and maximum instance counts  Define scale-in and scale-out behaviorsModel VariantsAs shown in the endpoint details screen, you can deploy multiple variants of the same model:  Different instance types for different performance requirements  Multiple versions of the same model for A/B testing  Custom resource allocation through instance weightingIn the example, we can see the “AllTraffic” variant using ml.g5.xlarge instance type.Integration with JupyterLabSageMaker Studio provides seamless integration with JupyterLab, allowing you to:  Develop and test your machine learning code  Prepare data for training and inference  Deploy models directly from your notebook  Test inference against deployed endpointsThe screenshots show a JupyterLab environment with a simple “hello world” test, but you can use it for much more complex ML workflows.API-Based Model InvocationOnce your model is deployed, you can invoke it using the SageMaker Runtime API:import boto3import json# Create a SageMaker runtime clientruntime = boto3.client('sagemaker-runtime')# Define the payloadpayload = {    \"inputs\": \"Write a short poem about machine learning\",    \"parameters\": {        \"max_new_tokens\": 128,        \"temperature\": 0.7,        \"top_p\": 0.9    }}# Invoke the endpointresponse = runtime.invoke_endpoint(    EndpointName='jumpstart-dft-hf-llm-gemma-2-2b-20250409-100206',    ContentType='application/json',    Body=json.dumps(payload))# Parse the responseresult = json.loads(response['Body'].read().decode())print(result)This API-based approach allows you to integrate SageMaker-hosted models into your applications, microservices, or other cloud resources.API call for model predictionSetup AuthorizationPass JSON raw data as POSt route in model endpoint invocations to get predictionFoundation Models Available on SageMakerAmazon SageMaker supports a wide variety of foundation models, including:  Large Language Models:          Anthropic Claude (various versions)      Meta Llama 2 and Llama 3      Mistral      Google Gemma (as seen in our example)      Amazon Titan        Multimodal Models:          Stable Diffusion      DALL-E      Amazon Titan Image Generator      Anthropic Claude Multimodal        Embedding Models:          BERT variants      Sentence transformers      Amazon Titan Embeddings      Advantages of Using Amazon SageMaker for Model DeploymentBased on the screenshots and SageMaker capabilities, here are some key advantages:  Simplified Deployment - Point-and-click deployment of complex models  Infrastructure Management - Automated handling of underlying infrastructure  Cost Optimization - Various instance types and auto-scaling to control costs  Monitoring and Observability - Integrated CloudWatch monitoring  Security and Compliance - IAM integration, VPC support, and encryption options  Flexibility - Support for custom containers and bring-your-own-model scenarios  Integrated ML Lifecycle - Seamless transition from experimentation to production  Pre-trained Foundation Models - Quick access to state-of-the-art modelsBest Practices for SageMaker DeploymentsTo get the most out of your SageMaker deployments:  Right-size your instances - Choose appropriate instance types for your workload  Implement auto-scaling - Configure scaling policies based on expected traffic patterns  Monitor performance - Use CloudWatch to track invocations, latency, and errors  Optimize costs - Consider serverless inference for intermittent workloads  Version control your models - Use model registries to track model versions  Test thoroughly - Validate model performance before production deployment  Implement CI/CD pipelines - Automate the deployment process for consistencyConclusionAmazon SageMaker significantly simplifies the deployment and scaling of machine learning models, including complex foundation models like Google’s Gemma. By providing a comprehensive platform that handles infrastructure management, scaling, and monitoring, SageMaker allows data scientists and ML engineers to focus on creating value rather than managing infrastructure.The deployment process demonstrated in this post shows how quickly you can go from model selection to a production-ready API endpoint. Whether you’re deploying a simple sentiment analysis model or a large language model like Gemma 2-2B, SageMaker provides the tools and capabilities to do so efficiently and at scale.As AI continues to evolve and more foundation models become available, platforms like SageMaker will play an increasingly important role in making these technologies accessible and manageable in production environments.Resources  Amazon SageMaker Documentation  Amazon SageMaker JumpStart  Foundation Models in SageMaker  SageMaker Auto Scaling"
  },
  
  {
    "title": "The Complete Guide to Apache Kafka in Rails: From Development to Production",
    "url": "/posts/kafka-in-rails/",
    "categories": "ApacheKafka",
    "tags": "ApacheKafka",
    "date": "2025-04-03 03:20:18 +0545",
    





    
    "snippet": "What is Apache Kafka?Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and real-time data processing. It is used for:  Messaging System:Acts as a ...",
    "content": "What is Apache Kafka?Apache Kafka is a distributed event streaming platform designed for high-throughput, fault-tolerant, and real-time data processing. It is used for:  Messaging System:Acts as a message broker between producers (data sources) and consumers (data processors).  Event Streaming:Allows applications to publish, subscribe to, store, and process event streams in real time.  Decoupling Services:Helps in building microservices architectures by enabling loosely coupled communication between services.  Data Pipeline:Used for collecting and processing large volumes of data before sending it to databases, analytics tools, or machine learning modelsKafka is widely used by large-scale applications for real-time analytics, monitoring, and log processing.IntroductionApache Kafka has become the backbone of modern event-driven Rails applications. This guide provides a complete implementation walkthrough with real-world examples, troubleshooting tips, and production-ready patterns. We’ll cover:  Docker-based Kafka setup  Rails integration with best practices  Real-world event publishing  Robust consumer implementation  Production deployment on Heroku  Detailed troubleshootingSubscribers –&gt; Kafka as message Broker –&gt; ConsumersGemfilegem 'ruby-kafka', '~&gt; 1.5'  # This is the correct gem to usegem 'delivery_boy'  # Add this linegem 'racecar' # for easy consumer setuphttps://kafka.apache.org/quickstart#quickstart_sendhttps://deadmanssnitch.com/opensource/kafka/docs/index.htmlPull Kafka Docker Imagedocker pull apache/kafka:4.0.0Run Kafka Containerdocker run -p 9092:9092 apache/kafka:4.0.0Setup Initializersconfig/initializers/kafka.rbrequire 'kafka'  # This now loads ruby-kafkaKAFKA = Kafka.new(  seed_brokers: ['localhost:9092'],  # Note the parameter name  client_id: 'blog_app',  logger: Rails.logger)Creating Kafka TopicsBash Command to Create Topickafka-topics --create --topic blog_events --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1Create Topic Manually via Dockerdocker-compose exec kafka \\ kafka-topics --create \\ --topic blog_events \\ --bootstrap-server localhost:9092 \\ --partitions 1 \\ --replication-factor 1Check Topicsdocker exec -it gallant_shockley kafka-topics --list --bootstrap-server localhost:9092If kafka-topics is Missingdocker exec -it gallant_shockley /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092If the command does not output any errors, Kafka is running successfullyCheck Kafka Logsdocker logs gallant_shockley --tail 50Create blog_events Topic if Missingdocker exec -it gallant_shockley /opt/kafka/bin/kafka-topics.sh --create \\ --topic blog_events \\ --bootstrap-server localhost:9092 \\ --partitions 1 \\ --replication-factor 1Debugging Kafka IssuesPort Already in Use ErrorIf you encounter:Error: (HTTP code 500) server error - driver failed programming external connectivity on endpoint gallant_shockleyCheck which process is using port 9092:lsof -i :9092  # Shows which process is using 9092kill -9 &lt;PID&gt;  # Replace &lt;PID&gt; with the actual process IDRestart Kafka:docker restart gallant_shockleyVerify Topic Creationdocker exec -it gallant_shockley /opt/kafka/bin/kafka-topics.sh --describe --topic blog_events --bootstrap-server localhost:9092Check Kafka Listenersdocker exec -it gallant_shockley sh -c \"cat /opt/kafka/config/server.properties | grep listeners\"If the output is:advertised.listeners=PLAINTEXT://localhost:9092Change it to:advertised.listeners=PLAINTEXT://&lt;your_host_ip&gt;:9092Restart Kafka:docker restart gallant_shockleyRails Model Integrationclass Blog &lt; ApplicationRecord  after_create :publish_creation_event  private  def publish_creation_event    event = {      event_id: SecureRandom.uuid,      event_type: 'blog_created',      event_time: Time.now.utc.iso8601,      data: {        id: id,        title: title,        description: description,        created_at: created_at      }    }    # Asynchronously publish the event    KafkaDeliveryBoy.deliver_async(      event.to_json,      topic: 'blog_events'    )  endendFinal Steps  Kill any process using port 9092 (lsof -i :9092).  Ensure the topic exists (kafka-topics.sh –list).  Check Kafka listener settings (cat server.properties).  Restart Kafka (docker restart gallant_shockley)."
  },
  
  {
    "title": "Apache Kafka In Rails",
    "url": "/posts/apache-kafka-in-rails/",
    "categories": "",
    "tags": "",
    "date": "2025-04-03 00:00:00 +0545",
    





    
    "snippet": "Subscribers – Kafka as Broker – ConsumersGemfilegem 'ruby-kafka', '~&gt; 1.5'  # This is the correct gem to usegem 'delivery_boy'  # Add this linegem 'racecar' # for easy consumer setuphttps://kafk...",
    "content": "Subscribers – Kafka as Broker – ConsumersGemfilegem 'ruby-kafka', '~&gt; 1.5'  # This is the correct gem to usegem 'delivery_boy'  # Add this linegem 'racecar' # for easy consumer setuphttps://kafka.apache.org/quickstart#quickstart_sendhttps://deadmanssnitch.com/opensource/kafka/docs/index.html** docker pull apache/kafka:4.0.0** docker run -p 9092:9092 apache/kafka:4.0.0setup initializers/kafka.rbbash - let’s create the topickafka-topics --create --topic blog_events --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1create topic manuallydocker-compose exec kafka \\ kafka-topics --create \\ --topic blog_events \\ --bootstrap-server localhost:9092 \\ --partitions 1 \\ --replication-factor 1apache/kafka:4.0.0docker exec -it gallant_shockley kafka-topics --list --bootstrap-server localhost:9092  OCI runtime exec failed: exec failed: container_linux.go:380: starting container process caused: exec: “kafka-topics”: executable file not found in $PATH: unknownmay be kafka-topics is not theredocker exec -it gallant_shockley /opt/kafka/bin/kafka-topics.sh --list --bootstrap-server localhost:9092  did not out put any thingThis means kafka is running successfully.docker logs gallant_shockley --tail 50ssl.endpoint.identification.algorithm = httpsBy default, Kafka does not automatically create topics. You need to manually create blog_events:docker exec -it gallant_shockley /opt/kafka/bin/kafka-topics.sh --create \\ --topic blog_events \\ --bootstrap-server localhost:9092 \\ --partitions 1 \\ --replication-factor 1Error invoking remote method ‘docker-start-container’: Error: (HTTP code 500) server error - driver failed programming external connectivity on endpoint gallant_shockley (6e5102c8320a31dab372355d2c43cf70ac239e92fc431982756d6933d66b7bec): Bind for 0.0.0.0:9092 failed: port is already allocatedrails c -&gt; Blog created and this is outputapache-kafka-demo(dev)&gt; Blog.create(title: ‘blog 1’, description: ‘blog 1 description’)TRANSACTION (0.2ms) BEGIN immediate TRANSACTION /application=’ApacheKafkaDemo’/Blog Create (2.2ms) INSERT INTO “blogs” (“title”, “description”, “createdat”, “updated_at”) VALUES (‘blog 1’, ‘blog 1 description’, ‘2025-04-02 01:42:33.049807’, ‘2025-04-02 01:42:33.049807’) RETURNING “id” /_application=’ApacheKafkaDemo’/I, [2025-04-02T07:27:33.055743 #85432] INFO – : [Producer ] Starting async producer in the background…TRANSACTION (0.3ms) COMMIT TRANSACTION /application=’ApacheKafkaDemo’/=&gt;#&lt;Blog:0x000000011fc20b58id: 1,title: “blog 1”,description: “blog 1 description”,created_at: “2025-04-02 01:42:33.049807000 +0000”,updated_at: “2025-04-02 01:42:33.049807000 +0000”&gt;apache-kafka-demo(dev)&gt; I, [2025-04-02T07:27:43.058547 #85432] INFO – : [Producer ] New topics added to target list: blog_eventsI, [2025-04-02T07:27:43.058967 #85432] INFO – : [Producer ] Fetching cluster metadata from kafka://localhost:9092D, [2025-04-02T07:27:43.059640 #85432] DEBUG – : [Producer ] [topic_metadata] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:43.065307 #85432] DEBUG – : [Producer ] [topic_metadata] Sending topic_metadata API request 1 to localhost:9092D, [2025-04-02T07:27:43.065638 #85432] DEBUG – : [Producer ] [topic_metadata] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:43.127048 #85432] DEBUG – : [Producer ] [topic_metadata] Received response 1 from localhost:9092I, [2025-04-02T07:27:43.127517 #85432] INFO – : [Producer ] Discovered cluster metadata; nodes: localhost:9092 (node_id=1)D, [2025-04-02T07:27:43.127560 #85432] DEBUG – : [Producer ] Closing socket to localhost:9092E, [2025-04-02T07:27:43.128119 #85432] ERROR – : [Producer ] Failed to assign partitions to 1 messages in blog_eventsW, [2025-04-02T07:27:43.128314 #85432] WARN – : [Producer ] Failed to send all messages to ; attempting retry 1 of 2 after 1sI, [2025-04-02T07:27:44.132206 #85432] INFO – : [Producer ] Fetching cluster metadata from kafka://localhost:9092D, [2025-04-02T07:27:44.132823 #85432] DEBUG – : [Producer ] [topic_metadata] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:44.135533 #85432] DEBUG – : [Producer ] [topic_metadata] Sending topic_metadata API request 1 to localhost:9092D, [2025-04-02T07:27:44.135948 #85432] DEBUG – : [Producer ] [topic_metadata] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:44.154177 #85432] DEBUG – : [Producer ] [topic_metadata] Received response 1 from localhost:9092I, [2025-04-02T07:27:44.154276 #85432] INFO – : [Producer ] Discovered cluster metadata; nodes: localhost:9092 (node_id=1)D, [2025-04-02T07:27:44.154299 #85432] DEBUG – : [Producer ] Closing socket to localhost:9092D, [2025-04-02T07:27:44.154541 #85432] DEBUG – : [Producer ] Current leader for blog_events/0 is node localhost:9092 (node_id=1)I, [2025-04-02T07:27:44.154591 #85432] INFO – : [Producer ] Sending 1 messages to localhost:9092 (node_id=1)D, [2025-04-02T07:27:44.154690 #85432] DEBUG – : [Producer ] [produce] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:44.155944 #85432] DEBUG – : [Producer ] [produce] Sending produce API request 1 to localhost:9092D, [2025-04-02T07:27:44.790992 #85432] DEBUG – : [Producer ] [produce] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:44.836204 #85432] DEBUG – : [Producer ] [produce] Received response 1 from localhost:9092D, [2025-04-02T07:27:44.836494 #85432] DEBUG – : [Producer ] Successfully appended 1 messages to blog_events/0 on localhost:9092 (node_id=1)apache-kafka-demo(dev)&gt;  Above output indicates  The blog record was successfully created in your database (INSERT INTO blogs)  Kafka Connection:Your producer connected to Kafka at localhost:9092Discovered the cluster metadata (single node)Confirmed the blog_events topic exists      Message Delivery:After 2 retries (due to initial partition assignment failure), the event was successfully published to blog_events/0 (partition 0)        The Error SequenceFirst Attempt Failed:Failed to assign partitions to 1 messages in blog_eventsThis is normal when Kafka is still initializing or when the producer first discovers the cluster        Automatic Recovery:The producer retried after 1 second (as configured)On the second attempt:          Re-fetched cluster metadata      Identified the leader for blog_events/0      Successfully delivered the message            Why This HappenedNormal Kafka Behavior:Partition assignment can fail temporarily during:Broker startupTopic creationNetwork latency        Your Configuration is Correct:The system self-healed because:Retries were enabled (default in ruby-kafka)The topic existed (created earlier via kafka-topics.sh)    How to Verify the Message➜ apache_kafka_demo git:(main) ✗ docker logs my_kafka | grep “blog_events”[2025-04-02 01:42:43,114] INFO Sent auto-creation request for Set(blog_events) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)[2025-04-02 01:42:43,173] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(blog_events-0) (kafka.server.ReplicaFetcherManager)[2025-04-02 01:42:43,189] INFO [LogLoader partition=blog_events-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.UnifiedLog)[2025-04-02 01:42:43,192] INFO Created log for partition blog_events-0 in /tmp/kraft-combined-logs/blog_events-0 with properties {} (kafka.log.LogManager)[2025-04-02 01:42:43,193] INFO [Partition blog_events-0 broker=1] No checkpointed highwatermark is found for partition blog_events-0 (kafka.cluster.Partition)[2025-04-02 01:42:43,194] INFO [Partition blog_events-0 broker=1] Log loaded for partition blog_events-0 with initial high watermark 0 (kafka.cluster.Partition)^^ This tells:Kafka Logs Confirm Success:The blog_events topic was auto-created when first usedPartition blog_events-0 was initialized with offset 0All metadata was properly registered➜ apache_kafka_demo git:(main) ✗ docker exec -it my_kafka  /opt/kafka/bin/kafka-console-consumer.sh  –topic blog_events  –from-beginning  –bootstrap-server localhost:9092{“event_id”:”4ebd836d-fab7-4958-90e1-f116661c7059”,”event_type”:”blog_created”,”event_time”:”2025-04-02T01:42:33Z”,”data”:{“id”:1,”title”:”blog 1”,”description”:”blog 1 description”,”created_at”:”2025-04-02T01:42:33.049Z”}}This tells ^^Message Verified:Your blog creation event was successfully published to KafkaThe consumer shows the complete JSON message with:{  \"event_id\": \"4ebd836d-fab7-4958-90e1-f116661c7059\",  \"event_type\": \"blog_created\",  \"data\": {    \"id\": 1,    \"title\": \"blog 1\",    \"description\": \"blog 1 description\"  }}config/initializers/kafka.rbDeliveryBoy.configure do |config|  config.delivery_interval = 1  # Batch messages for 1 second  config.delivery_threshold = 5 # Or batch 5 messages  config.max_retries = 3        # Increase retriesendCreate Blog model title:string, description:textSubscribersSee logsdocker logs my_kafka | grep \"blog_events\"output like this[2025-04-02 01:42:43,114] INFO Sent auto-creation request for Set(blog_events) to the active controller. (kafka.server.DefaultAutoTopicCreationManager)[2025-04-02 01:42:43,173] INFO [ReplicaFetcherManager on broker 1] Removed fetcher for partitions Set(blog_events-0) (kafka.server.ReplicaFetcherManager)[2025-04-02 01:42:43,189] INFO [LogLoader partition=blog_events-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.UnifiedLog)[2025-04-02 01:42:43,192] INFO Created log for partition blog_events-0 in /tmp/kraft-combined-logs/blog_events-0 with properties {} (kafka.log.LogManager)[2025-04-02 01:42:43,193] INFO [Partition blog_events-0 broker=1] No checkpointed highwatermark is found for partition blog_events-0 (kafka.cluster.Partition)[2025-04-02 01:42:43,194] INFO [Partition blog_events-0 broker=1] Log loaded for partition blog_events-0 with initial high watermark 0 (kafka.cluster.Partition)docker exec -it my_kafka \\  /opt/kafka/bin/kafka-console-consumer.sh \\  --topic blog_events \\  --from-beginning \\  --bootstrap-server localhost:9092{\"event_id\":\"4ebd836d-fab7-4958-90e1-f116661c7059\",\"event_type\":\"blog_created\",\"event_time\":\"2025-04-02T01:42:33Z\",\"data\":{\"id\":1,\"title\":\"blog 1\",\"description\":\"blog 1 description\",\"created_at\":\"2025-04-02T01:42:33.049Z\"}}docker exec -it my_kafka \\  /opt/kafka/bin/kafka-console-consumer.sh \\  --topic blog_events \\  --from-beginning \\  --bootstrap-server localhost:9092{\"event_id\":\"4ebd836d-fab7-4958-90e1-f116661c7059\",\"event_type\":\"blog_created\",\"event_time\":\"2025-04-02T01:42:33Z\",\"data\":{\"id\":1,\"title\":\"blog 1\",\"description\":\"blog 1 description\",\"created_at\":\"2025-04-02T01:42:33.049Z\"}}To see the logs of the kafka, run the command docker logs container_iddocker logs gallant_shockleyoutput looks like this===&gt; Useruid=1000(appuser) gid=1000(appuser) groups=1000(appuser)===&gt; Setting default values of environment variables if not already set.CLUSTER_ID not set. Setting it to default value: \"5L6g3nShT-eMCtK--X86sw\"===&gt; Configuring ...===&gt; Launching ...===&gt; Using provided cluster id 5L6g3nShT-eMCtK--X86sw ...[0.001s][warning][cds] The shared archive file has a bad magic number: 0[2025-04-02 00:45:09,555] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2025-04-02 00:45:09,743] INFO Registered signal handlers for TERM, INT, HUP (org.apache.kafka.common.utils.LoggingSignalHandler)[2025-04-02 00:45:09,744] INFO [ControllerServer id=1] Starting controller (kafka.server.ControllerServer)[2025-04-02 00:45:09,933] INFO Updated connection-accept-rate max connection creation rate to 2147483647 (kafka.network.ConnectionQuotas)[2025-04-02 00:45:09,957] INFO [SocketServer listenerType=CONTROLLER, nodeId=1] Created data-plane acceptor and processors for endpoint : ListenerName(CONTROLLER) (kafka.network.SocketServer)[2025-04-02 00:45:09,969] INFO CONTROLLER: resolved wildcard host to d3cc737bfffc (org.apache.kafka.metadata.ListenerInfo)[2025-04-02 00:45:09,973] INFO authorizerStart completed for endpoint CONTROLLER. Endpoint is now READY. (org.apache.kafka.server.network.EndpointReadyFutures)[2025-04-02 00:45:09,974] INFO [SharedServer id=1] Starting SharedServer (kafka.server.SharedServer)[2025-04-02 00:45:10,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Loading producer state till offset 0 (org.apache.kafka.storage.internals.log.UnifiedLog)[2025-04-02 00:45:10,009] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Reloading from producer snapshot and rebuilding producer state from offset 0 (org.apache.kafka.storage.internals.log.UnifiedLog)[2025-04-02 00:45:10,012] INFO [LogLoader partition=__cluster_metadata-0, dir=/tmp/kraft-combined-logs] Producer state recovery took 0ms for snapshot load and 0ms for segment recovery from offset 0 (org.apache.kafka.storage.internals.log.UnifiedLog)[2025-04-02 00:45:10,030] INFO Initialized snapshots with IDs SortedSet() from /tmp/kraft-combined-logs/__cluster_metadata-0 (kafka.raft.KafkaMetadataLog$)[2025-04-02 00:45:10,040] INFO [raft-expiration-reaper]: Starting (kafka.raft.TimingWheelExpirationService$ExpiredOperationReaper)[2025-04-02 00:45:10,048] INFO [RaftManager id=1] Reading KRaft snapshot and log as part of the initialization (org.apache.kafka.raft.KafkaRaftClient)[2025-04-02 00:45:10,050] INFO [RaftManager id=1] Starting voters are VoterSet(voters={1=VoterNode(voterKey=ReplicaKey(id=1, directoryId=&lt;undefined&gt;), listeners=Endpoints(endpoints={ListenerName(CONTROLLER)=localhost/127.0.0.1:9093}), supportedKRaftVersion=SupportedVersionRange[min_version:0, max_version:0])}) (org.apache.kafka.raft.KafkaRaftClient)[2025-04-02 00:45:10,050] INFO [RaftManager id=1] Starting request manager with static voters: [localhost:9093 (id: 1 rack: null isFenced: false)] (org.apache.kafka.raft.KafkaRaftClient)blog.rb model# app/models/blog.rbclass Blog &lt; ApplicationRecord  after_create :publish_creation_event  private  def publish_creation_event    event = {      event_id: SecureRandom.uuid,      event_type: 'blog_created',      event_time: Time.now.utc.iso8601,      data: {        id: id,        title: title,        description: description,        created_at: created_at      }    }    # Asynchronously publish the event    KafkaDeliveryBoy.deliver_async(      event.to_json,      topic: 'blog_events'    )  endendblog_consumer.rb# app/consumers/blog_consumer.rbclass BlogConsumer &lt; Racecar::Consumer  subscribes_to \"blog_events\"  def process(message)    event = JSON.parse(message.value)    # Same processing logic as above  endendOnce blog is created, event is pushed to kafka broker.apache-kafka-demo(dev)&gt; Blog.create(title: ‘blog 1’, description: ‘blog 1 description’)  TRANSACTION (0.2ms)  BEGIN immediate TRANSACTION /application=’ApacheKafkaDemo’/  Blog Create (2.2ms)  INSERT INTO “blogs” (“title”, “description”, “created_at”, “updated_at”) VALUES (‘blog 1’, ‘blog 1 description’, ‘2025-04-02 01:42:33.049807’, ‘2025-04-02 01:42:33.049807’) RETURNING “id” /application=’ApacheKafkaDemo’/I, [2025-04-02T07:27:33.055743 #85432]  INFO – : [Producer ] Starting async producer in the background…  TRANSACTION (0.3ms)  COMMIT TRANSACTION /application=’ApacheKafkaDemo’/=&gt;#&lt;Blog:0x000000011fc20b58 id: 1, title: “blog 1”, description: “blog 1 description”, created_at: “2025-04-02 01:42:33.049807000 +0000”, updated_at: “2025-04-02 01:42:33.049807000 +0000”&gt;apache-kafka-demo(dev)&gt; I, [2025-04-02T07:27:43.058547 #85432]  INFO – : [Producer ] New topics added to target list: blog_eventsI, [2025-04-02T07:27:43.058967 #85432]  INFO – : [Producer ] Fetching cluster metadata from kafka://localhost:9092D, [2025-04-02T07:27:43.059640 #85432] DEBUG – : [Producer ] [topic_metadata] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:43.065307 #85432] DEBUG – : [Producer ] [topic_metadata] Sending topic_metadata API request 1 to localhost:9092D, [2025-04-02T07:27:43.065638 #85432] DEBUG – : [Producer ] [topic_metadata] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:43.127048 #85432] DEBUG – : [Producer ] [topic_metadata] Received response 1 from localhost:9092I, [2025-04-02T07:27:43.127517 #85432]  INFO – : [Producer ] Discovered cluster metadata; nodes: localhost:9092 (node_id=1)D, [2025-04-02T07:27:43.127560 #85432] DEBUG – : [Producer ] Closing socket to localhost:9092E, [2025-04-02T07:27:43.128119 #85432] ERROR – : [Producer ] Failed to assign partitions to 1 messages in blog_eventsW, [2025-04-02T07:27:43.128314 #85432]  WARN – : [Producer ] Failed to send all messages to ; attempting retry 1 of 2 after 1sI, [2025-04-02T07:27:44.132206 #85432]  INFO – : [Producer ] Fetching cluster metadata from kafka://localhost:9092D, [2025-04-02T07:27:44.132823 #85432] DEBUG – : [Producer ] [topic_metadata] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:44.135533 #85432] DEBUG – : [Producer ] [topic_metadata] Sending topic_metadata API request 1 to localhost:9092D, [2025-04-02T07:27:44.135948 #85432] DEBUG – : [Producer ] [topic_metadata] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:44.154177 #85432] DEBUG – : [Producer ] [topic_metadata] Received response 1 from localhost:9092I, [2025-04-02T07:27:44.154276 #85432]  INFO – : [Producer ] Discovered cluster metadata; nodes: localhost:9092 (node_id=1)D, [2025-04-02T07:27:44.154299 #85432] DEBUG – : [Producer ] Closing socket to localhost:9092D, [2025-04-02T07:27:44.154541 #85432] DEBUG – : [Producer ] Current leader for blog_events/0 is node localhost:9092 (node_id=1)I, [2025-04-02T07:27:44.154591 #85432]  INFO – : [Producer ] Sending 1 messages to localhost:9092 (node_id=1)D, [2025-04-02T07:27:44.154690 #85432] DEBUG – : [Producer ] [produce] Opening connection to localhost:9092 with client id delivery_boy…D, [2025-04-02T07:27:44.155944 #85432] DEBUG – : [Producer ] [produce] Sending produce API request 1 to localhost:9092D, [2025-04-02T07:27:44.790992 #85432] DEBUG – : [Producer ] [produce] Waiting for response 1 from localhost:9092D, [2025-04-02T07:27:44.836204 #85432] DEBUG – : [Producer ] [produce] Received response 1 from localhost:9092D, [2025-04-02T07:27:44.836494 #85432] DEBUG – : [Producer ] Successfully appended 1 messages to blog_events/0 on localhost:9092 (node_id=1)———————- Consumer ——————–Run BlogConsumer – log event in BlogAuditbundle exec racecar BlogConsumerIn production (including Heroku) you must run bundle exec racecar BlogConsumer continuously as a separate process (not a scheduler). This is because:Kafka consumers are long-running processes (like a web server)They maintain persistent connections to Kafka brokersThey process messages in real-time as they arriveHow to Handle This in Production (Heroku)Option 1: Heroku Worker Dyno (Recommended)1. Add to your Procfile:web: bundle exec rails serverworker: bundle exec racecar BlogConsumerScale the worker:heroku ps:scale worker=12. Sidekiq/Scheduler (Not Recommended ❌)# Bad approach - don't do this!# This will miss messages between scheduler runsclass ScheduledConsumerJob  def perform    system(\"bundle exec racecar BlogConsumer --timeout 10\")  endend3. Kubernetes/ECSFor containerized setups:# docker-compose.ymlservices:  consumer:    command: bundle exec racecar BlogConsumer    depends_on: [kafka]Best Practices for Heroku      Auto-restart: Heroku will restart crashed consumers        Scaling:  heroku ps:scale worker=2  # Add more consumers  Logging:heroku logs --tail --ps worker  Pricing: Worker dynos cost like web dynos (~$25/month for Hobby tier)What If You Stop the Consumer?Messages will accumulate in Kafka (no data loss)When restarted, it processes from the last committed offsetUse this to pause/upgrade consumers safelyAlternatives If You Prefer SchedulersIf you truly want periodic processing:  Use Kafka Consumer Groups with short timeoutsbundle exec racecar BlogConsumer --timeout 300  # Exit after 5min  Then run via Heroku Scheduler:bash -c \"while true; do bundle exec racecar BlogConsumer --timeout 300; done\"But this is not recommended for real-time systems.Example: Full Heroku Setup  Add Redis/Kafka addons:heroku addons:create heroku-kafka:basic-0heroku addons:create heroku-redis:mini  Update config/racecar.yml:production:  brokers: &lt;%= ENV[\"KAFKA_URL\"] %&gt;  group_id: \"blog_events_#{Rails.env}\""
  },
  
  {
    "title": "Vive Coding: The Future of AI-Driven Development with Purpose",
    "url": "/posts/vive-coding-future-of-ai-driven-development/",
    "categories": "Artificial Intelligence (AI), Vive Coding",
    "tags": "AI, vivecoding, future",
    "date": "2025-04-02 10:20:18 +0545",
    





    
    "snippet": "Software development is undergoing a transformation. Traditionally, coding required deep knowledge of syntax, algorithms, and frameworks. But with the rise of AI-powered tools, a new paradigm has e...",
    "content": "Software development is undergoing a transformation. Traditionally, coding required deep knowledge of syntax, algorithms, and frameworks. But with the rise of AI-powered tools, a new paradigm has emerged: Vibe Coding—a style of development where programmers instruct AI to write code based on natural language prompts, focusing on creativity rather than manual coding.At the same time, a complementary philosophy is gaining traction: VIVE Coding—a mindset that emphasizes Visibility, Intent, Value, and Efficiency in programming. Together, these approaches shape the future of software development, blending AI automation with human-driven clarity and purpose.What is Vibe Coding?1. AI-Driven CodingVibe coding leverages the power of AI, particularly large language models (LLMs), to generate code from user-provided descriptions. Instead of writing code line by line, developers describe the desired functionality, and AI translates it into executable code.2. Focus on Ideas, Not SyntaxRather than getting bogged down in technical details, developers can concentrate on the bigger picture—concepts, features, and user experience. The AI handles syntax, logic, and implementation details, allowing programmers to act as architects rather than bricklayers.3. Natural Language PromptsUsers describe what they want the software to do in everyday language, and AI tools generate the necessary code. For example, instead of manually crafting a complex SQL query, a user could simply state:  “Create a social media management dashboard showing analytics, scheduled posts, and engagement metrics across platforms.”AI would then generate the corresponding backend and frontend code.Advocates:Some argue that vibe coding allows even non-developers to create software, shifting the programmer’s role from manual coding to guiding, testing, and refining the AI-generated code.Criticism:Others question the accuracy and reliability of AI-generated code, especially for complex applications, and whether it can truly replace the skills of a software engineer.The Four Pillars of VIVE CodingWhile AI enhances coding efficiency, human developers must ensure that the generated code is maintainable, clear, and purposeful. This is where VIVE Coding comes into play. It is a philosophy that ensures code remains:1. Visibility – Write Code That Speaks for ItselfReadable, well-structured code reduces cognitive load for both the original developer and future maintainers.✅ Do:  Use meaningful variable and function names (calculateTotalPrice instead of calc).  Follow consistent formatting and indentation.  Break down complex logic into smaller, well-named functions.❌ Avoid:  Cryptic abbreviations (tmp, x, data1).  Overly clever one-liners that sacrifice readability.2. Intent – Make Your Code’s Purpose ClearEvery line of code should have a clear reason for existing.✅ Do:  Use comments sparingly, only to explain why (not what).  Structure code to reflect business logic.  Avoid “magic numbers” by using named constants (MAX_RETRIES = 3 instead of if (retries &lt; 3)).❌ Avoid:  Writing code that “just works” without explaining its role.  Mixing multiple responsibilities in a single function.3. Value – Focus on What Truly MattersNot all code needs to be perfect—focus on delivering real value.✅ Do:  Solve the immediate problem first, then refactor.  Avoid over-engineering (“You Ain’t Gonna Need It” – YAGNI).  Write tests for critical logic but don’t obsess over 100% coverage for trivial code.❌ Avoid:  Premature optimization before identifying bottlenecks.  Adding unnecessary abstractions “just in case.”4. Efficiency – Write Performant (But Readable) CodeEfficiency matters, but not at the cost of maintainability.✅ Do:  Optimize only after profiling and identifying real bottlenecks.  Use efficient algorithms (e.g., prefer O(n) over O(n²)).  Leverage built-in language features for better performance.❌ Avoid:  Micro-optimizations that make code harder to read.  Ignoring performance entirely in favor of “clean code.”The Future of CodingThe term “Vibe Coding” was coined by Andrej Karpathy in February 2025. As AI models continue to improve, the role of software developers is shifting from manual coding to guiding, testing, and refining AI-generated code. But AI cannot fully replace human oversight—developers must ensure their code adheres to VIVE principles for clarity, maintainability, and long-term value.How to Apply VIVE Coding in Your Projects  Start Small – Refactor a single function to be more visible and intentional.  Review Code with VIVE in Mind – Ask:          Is this easy to understand?      Does it clearly express its purpose?      Does it provide real value?      Is it efficient enough for its use case?        Iterate – Continuously improve code readability and efficiency without over-engineering.Final ThoughtsVibe Coding and VIVE Coding together represent the future of software development—blending AI-powered automation with human-driven clarity and structure. While AI helps accelerate development, programmers must ensure that code remains visible, intentional, valuable, and efficient.🚀 Challenge: Pick a piece of your recent code and refactor it using VIVE principles. Notice how much easier it becomes to read and modify!Vive Coding! 🚀"
  },
  
  {
    "title": "Tailwind CSS Mastery Guide",
    "url": "/posts/tailwind-cheatsheet/",
    "categories": "Tailwind CSS, UI, cheatsheet",
    "tags": "css, tailwind_css, UI, cheatsheet",
    "date": "2025-04-01 12:20:18 +0545",
    





    
    "snippet": "Here’s a comprehensive list of Tailwind CSS utility categories and their class name prefixes to help you master Tailwind:Tailwind CSS Mastery GuideLayout            Category      Prefixes/Classes  ...",
    "content": "Here’s a comprehensive list of Tailwind CSS utility categories and their class name prefixes to help you master Tailwind:Tailwind CSS Mastery GuideLayout            Category      Prefixes/Classes                  Container      container              Display      block, inline-block, inline, flex, inline-flex, grid, inline-grid              Box Sizing      box-border, box-content              Float      float-right, float-left, float-none              Clear      clear-left, clear-right, clear-both, clear-none              Position      static, fixed, absolute, relative, sticky              Top/Right/Bottom/Left      top-0, right-0, bottom-0, left-0 (with various sizes)              Visibility      visible, invisible, collapse              Z-Index      z-0 to z-50, z-auto      Flexbox            Category      Prefixes/Classes                  Flex Direction      flex-row, flex-row-reverse, flex-col, flex-col-reverse              Flex Wrap      flex-wrap, flex-wrap-reverse, flex-nowrap              Flex      flex-1, flex-auto, flex-initial, flex-none              Flex Grow      grow, grow-0              Flex Shrink      shrink, shrink-0              Order      order-1 to order-12, order-first, order-last, order-none              Justify Content      justify-start, justify-end, justify-center, justify-between, etc.              Align Items      items-start, items-end, items-center, items-baseline, etc.      Grid            Category      Prefixes/Classes                  Grid Template Columns      grid-cols-1 to grid-cols-12, grid-cols-none              Grid Column Start/End      col-start-1, col-end-3, etc.              Grid Template Rows      grid-rows-1 to grid-rows-6, grid-rows-none              Gap      gap-0 to gap-96 (also gap-x-*, gap-y-*)      Spacing            Category      Prefixes/Classes                  Padding      p-0 to p-96 (also pt-*, pr-*, pb-*, pl-*, px-*, py-*)              Margin      m-0 to m-96 (also mt-*, mr-*, mb-*, ml-*, mx-*, my-*)              Space Between      space-x-*, space-y-*      Sizing            Category      Prefixes/Classes                  Width      w-0 to w-96, w-auto, w-full, w-screen, w-min, w-max              Min-Width      min-w-0, min-w-full, min-w-min, min-w-max              Height      h-0 to h-96, h-auto, h-full, h-screen      Typography            Category      Prefixes/Classes                  Font Family      font-sans, font-serif, font-mono              Font Size      text-xs to text-9xl              Font Weight      font-thin to font-black              Text Color      text-{color}-{shade} (e.g., text-red-500)              Text Align      text-left, text-center, text-right, text-justify      Backgrounds            Category      Prefixes/Classes                  Background Color      bg-{color}-{shade} (e.g., bg-blue-500)              Background Opacity      bg-opacity-0 to bg-opacity-100              Background Position      bg-bottom, bg-center, bg-left, etc.              Background Gradient      bg-gradient-to-{direction} (e.g., bg-gradient-to-r)      Borders            Category      Prefixes/Classes                  Border Radius      rounded, rounded-t, rounded-r, rounded-b, rounded-l, etc.              Border Width      border, border-0 to border-8, border-t, border-r, etc.              Border Color      border-{color}-{shade} (e.g., border-gray-300)      Effects            Category      Prefixes/Classes                  Box Shadow      shadow-sm, shadow, shadow-md, shadow-lg, shadow-xl, shadow-2xl              Opacity      opacity-0 to opacity-100      Transitions &amp; Animation            Category      Prefixes/Classes                  Transition Duration      duration-75 to duration-1000              Animation      animate-none, animate-spin, animate-ping, animate-pulse      Interactivity            Category      Prefixes/Classes                  Cursor      cursor-auto, cursor-pointer, cursor-wait, etc.              User Select      select-none, select-text, select-all, select-auto      Pseudo-class Variants  Hover: hover:  Focus: focus:  Active: active:  Responsive: sm:, md:, lg:, xl:, 2xl:  Dark mode: dark:"
  },
  
  {
    "title": "Streamlining Development with CircleCI: A Guide to Continuous Integration and Delivery",
    "url": "/posts/circle-ci/",
    "categories": "CircleCI",
    "tags": "continuous_integration, continuous_deployment",
    "date": "2025-03-27 04:50:00 +0545",
    





    
    "snippet": "In the fast-paced world of software development, delivering high-quality applications rapidly and reliably is paramount. CircleCI, a leading continuous integration and delivery (CI/CD) platform, ha...",
    "content": "In the fast-paced world of software development, delivering high-quality applications rapidly and reliably is paramount. CircleCI, a leading continuous integration and delivery (CI/CD) platform, has emerged as a favorite among developers seeking to optimize their workflows and automate processes. In this blog, we’ll explore what CircleCI is, its key features, and how it can transform your development pipeline.What is CircleCI?CircleCI is a cloud-based or on-premises CI/CD platform that automates the process of building, testing, and deploying software. It supports various programming languages, frameworks, and cloud environments, making it highly versatile for diverse teams and projects. By enabling developers to catch bugs early and streamline deployments, CircleCI helps teams ship high-quality code faster.Key Features of CircleCI  Customizable Workflows: CircleCI allows you to create complex workflows with multiple jobs and steps, tailored to your project’s needs. This flexibility ensures a smooth and efficient build process.  Integration-Friendly: With built-in support for popular tools like Docker, Kubernetes, GitHub, and Bitbucket, CircleCI fits seamlessly into your existing development ecosystem.  Parallelism: Speed up your pipeline by running tests and builds in parallel across multiple machines.  Automated Testing: CircleCI provides tools for automated testing, ensuring that every code change is vetted before it reaches production.  Insights Dashboard: Gain visibility into your workflows with detailed analytics, helping teams identify bottlenecks and optimize processes.Benefits of Using CircleCI  Faster Development: Automating builds and tests reduces manual effort, freeing up time for developers to focus on coding.  Higher Quality Code: Early bug detection and thorough testing lead to more stable releases.  Scalability: Whether you’re a small startup or a large enterprise, CircleCI scales to meet your needs.  Cost-Effective: By saving time and reducing errors, CircleCI ultimately lowers development costs.How to Get Started with CircleCIGetting started with CircleCI is straightforward:  Sign Up: Create an account on the CircleCI website.  Connect Your Repository: Link your GitHub, Bitbucket, or other version control system to CircleCI.  Configure Your Workflow: Write a .circleci/config.yml file to define your build and test steps.  Run Your Pipeline: Push code changes to your repository, and CircleCI will automatically run the configured workflows.In the screenshot, we see a visual representation of CircleCI’s feedback mechanism. When tests fail, CircleCI prominently displays a red flag, alerting developers to issues that need immediate attention. Conversely, successful tests are marked with a reassuring green indicator, signifying that the pipeline has been executed without errors. This clear and intuitive color-coding system allows teams to quickly assess the status of their builds and prioritize their tasks effectively. By making feedback visually distinct, CircleCI ensures that developers can respond to changes with speed and confidence.Final ThoughtsCircleCI is a powerful tool for teams looking to modernize their development workflows and embrace automation. With its robust feature set and ease of use, CircleCI has become an essential part of the CI/CD landscape. Whether you’re deploying web applications, mobile apps, or complex microservices, CircleCI can help you achieve faster, more reliable releases."
  },
  
  {
    "title": "Mastering Ruby Memory Management: A Practical Guide to Profiling and Optimization",
    "url": "/posts/memory-profiler-for-performance-optimization/",
    "categories": "Ruby, Performance, Ruby on Rails",
    "tags": "ruby, performance, profiling, ruby on rails",
    "date": "2025-03-20 12:24:00 +0545",
    





    
    "snippet": "Memory usage is often the silent performance killer in Ruby applications. While we frequently focus on execution speed, memory consumption can cause slowdowns, unexpected crashes, and increased hos...",
    "content": "Memory usage is often the silent performance killer in Ruby applications. While we frequently focus on execution speed, memory consumption can cause slowdowns, unexpected crashes, and increased hosting costs. In this guide, I’ll walk you through practical techniques for tracking and optimizing memory usage in Ruby and Rails applications using the powerful memory_profiler gem.Why Memory MattersMemory issues in Ruby apps typically manifest in several ways:  Slow performance: Memory bloat forces the garbage collector to work overtime  Random crashes: Out-of-memory errors  Steadily increasing memory usage: Signs of memory leaks  Excessive hosting costs: Needing larger instances to handle memory requirementsLet’s dive into how to identify and solve these issues.Setting Up Memory ProfilerFirst, you’ll need to install the memory_profiler gem:# In your Gemfilegem 'memory_profiler'# Or install it globally# gem install memory_profilerThen, install the gems:bundle installBasic Memory ProfilingLet’s start with a simple example. Create a file named memory_test.rb:require 'memory_profiler'# Set up logging to a file (optional but recommended)log_file = File.new('memory_profile.log', 'w')$stdout = log_file$stdout.sync = truereport = MemoryProfiler.report do  # The code you want to profile  array = Array.new(1_000_000) { |i| \"string #{i}\" }end# Print the reportreport.pretty_printRun it:ruby memory_test.rbNow look at memory_profile.log. The most important lines are at the top:Total allocated: 120,000,816 bytes (2,000,002 objects)Total retained:  120,000,816 bytes (2,000,002 objects)This tells you:  How much memory was allocated during execution  How much remained in use after execution (not garbage collected)Profiling Rails ApplicationsFor Rails applications, here’s how to profile a specific action or process:# In a controller or jobdef expensive_action  data = nil    report = MemoryProfiler.report do    # Code to profile    data = User.includes(:posts, :comments)              .where(active: true)              .map { |u| u.attributes.merge(post_count: u.posts.size) }  end    # Save the report to a file  File.open(\"#{Rails.root}/log/memory_profile_#{Time.now.to_i}.log\", 'w') do |file|    report.pretty_print(to_file: file)  end    render json: dataendMemory Optimization TechniquesNow that you can measure memory usage, let’s look at common memory optimization strategies with real examples.1. Use Batching for Large CollectionsProblem:# Memory-intensive approachreport = MemoryProfiler.report do  users = User.all  processed_users = users.map do |user|    # Process each user    process_user_data(user)  endendreport.pretty_printSolution:# Memory-optimized approachreport = MemoryProfiler.report do  processed_users = []  User.find_each(batch_size: 100) do |user|    processed_users &lt;&lt; process_user_data(user)  endendreport.pretty_printUsing find_each with batching reduces memory usage by loading records in smaller chunks rather than all at once.2. Optimize String OperationsProblem:report = MemoryProfiler.report do  result = \"\"  1000.times do |i|    result += \"Adding string #{i}. \"  # Creates a new string each time  endendreport.pretty_printSolution:report = MemoryProfiler.report do  chunks = []  1000.times do |i|    chunks &lt;&lt; \"Adding string #{i}. \"  end  result = chunks.joinendreport.pretty_printThe second approach allocates fewer intermediate string objects, reducing memory churn.3. Avoid Unnecessary Object CreationProblem:report = MemoryProfiler.report do  users = User.all.to_a  users.each do |user|    # Creating temporary hash for each user    user_data = {      id: user.id,      name: user.name,      email: user.email,      # Many more attributes      created_at: user.created_at    }    process_data(user_data)  endendreport.pretty_printSolution:report = MemoryProfiler.report do  User.select(:id, :name, :email, :created_at).find_each do |user|    # Use the ActiveRecord object directly    process_data(user)  endendreport.pretty_printThis approach reduces memory by:  Selecting only needed columns  Avoiding unnecessary hash creation  Processing in batches4. Identify Memory-Heavy GemsThe memory profiler report includes a breakdown of memory allocation by gem:allocated memory by gem----------------------------------- 42462489  activesupport-7.0.4 24595828  activerecord-7.0.4  8953418  json-2.6.2If a gem is using excessive memory, consider:  Updating to a newer version  Finding a more memory-efficient alternative  Implementing a lightweight solution yourself5. Monitor JSON Parsing and GenerationProblem:report = MemoryProfiler.report do  large_data = File.read('large_data.json')  parsed_data = JSON.parse(large_data)  # Work with the data  processed = process_json_data(parsed_data)  JSON.generate(processed)endreport.pretty_printSolution:report = MemoryProfiler.report do  # Stream parsing for large JSON files  result = []  Oj::Parser.new(:strict).parse_file('large_data.json') do |parsed|    # Process each object as it's parsed    result &lt;&lt; transform_json_object(parsed)  endendreport.pretty_printUsing streaming parsers like Oj (optimized JSON) for large files dramatically reduces memory usage.Real-World Case Study: Rails Model LoadingLet’s examine a common memory issue in Rails - loading models with many associations:The Problem# Controller actiondef dashboard  report = MemoryProfiler.report do    @users = User.all.includes(:posts, :comments, :profile)    @data = @users.map do |user|      {        user: user.attributes,        posts: user.posts.map(&amp;:attributes),        comments: user.comments.map(&amp;:attributes),        profile: user.profile&amp;.attributes      }    end  end    File.open(\"#{Rails.root}/log/dashboard_memory.log\", 'w') do |file|    report.pretty_print(to_file: file)  end    render json: @dataendMemory profile results:Total allocated: 254,328,816 bytes (3,200,502 objects)Total retained:  125,624,816 bytes (1,600,252 objects)The Solutiondef dashboard  report = MemoryProfiler.report do    # 1. Select only needed columns    # 2. Process in batches    # 3. Use pluck for simple data extraction    @data = []        User.select(:id, :name, :email, :created_at)        .find_in_batches(batch_size: 100) do |user_batch|            user_ids = user_batch.map(&amp;:id)            # Fetch related data efficiently      posts = Post.where(user_id: user_ids)                 .select(:id, :title, :user_id)                 .group_by(&amp;:user_id)                       comments = Comment.where(user_id: user_ids)                       .select(:id, :content, :user_id)                       .group_by(&amp;:user_id)                             profiles = Profile.where(user_id: user_ids)                       .select(:id, :bio, :user_id)                       .index_by(&amp;:user_id)            # Build the response without creating unnecessary objects      user_batch.each do |user|        user_data = {          id: user.id,          name: user.name,          email: user.email,          posts: posts[user.id]&amp;.map { |p| { id: p.id, title: p.title } } || [],          comments: comments[user.id]&amp;.map { |c| { id: c.id, content: c.content } } || [],          profile: profiles[user.id] ? { bio: profiles[user.id].bio } : nil        }        @data &lt;&lt; user_data      end    end  end    File.open(\"#{Rails.root}/log/dashboard_memory_optimized.log\", 'w') do |file|    report.pretty_print(to_file: file)  end    render json: @dataendMemory profile results after optimization:Total allocated: 42,328,816 bytes (520,502 objects)Total retained:  15,624,816 bytes (200,252 objects)That’s an 83% reduction in memory allocation and 88% reduction in retained memory!Advanced Techniques1. Detect Memory LeaksTo detect memory leaks, run the same code multiple times and watch for increasing memory:5.times do |i|  puts \"Iteration #{i+1}\"  report = MemoryProfiler.report do    # Code that might leak    perform_operation  end    puts \"Allocated: #{report.total_allocated_memsize} bytes\"  puts \"Retained: #{report.total_retained_memsize} bytes\"  puts \"---\"    # Force garbage collection between runs  GC.startendIf retained memory grows with each iteration, you likely have a leak.2. Targeted Detail AnalysisFor complex issues, examine object allocation details:report = MemoryProfiler.report do  # Code to profileend# Get the top 20 locations allocating memoryputs \"Top allocation locations:\"report.pretty_print(to_file: nil, detailed_report: false, scale_bytes: true,                    top: 20)# Get detailed string allocationsstring_locations = report.strings_allocatedstring_locations.sort_by! { |l| -l[:count] }string_locations[0..10].each do |location|  puts \"#{location[:count]} strings (#{location[:memsize]} bytes) allocated at #{location[:location]}\"end3. Memory-Conscious Design PatternsHere are some memory-efficient design patterns for Ruby applications:Value Objects Instead of Hashes# Memory-heavy approachusers.map do |user|  { id: user.id, name: user.name, stats: calculate_stats(user) }end# Memory-efficient approachclass UserPresenter  attr_reader :id, :name    def initialize(user)    @user = user    @id = user.id    @name = user.name  end    def stats    @stats ||= calculate_stats(@user)  end    private    def calculate_stats(user)    # Calculation logic  endendusers.map { |user| UserPresenter.new(user) }Lazy Loadingclass Report  def initialize(user_id)    @user_id = user_id  end    def summary    @summary ||= generate_summary  end    def details    @details ||= generate_details  end    private    def user    @user ||= User.find(@user_id)  end    def generate_summary    # Only calculated when needed    { name: user.name, post_count: user.posts.count }  end    def generate_details    # Only calculated when needed    user.posts.map { |post| { title: post.title, likes: post.likes } }  endendMemory Profiling in ProductionFor monitoring memory in production:  Use application monitoring tools: New Relic, Scout APM, Skylight  Set up custom memory logging:# In an initializermodule MemoryLogger  def self.log(label)    memory_before = `ps -o rss= -p #{Process.pid}`.to_i / 1024    yield if block_given?    memory_after = `ps -o rss= -p #{Process.pid}`.to_i / 1024        Rails.logger.info \"[MEMORY] #{label}: #{memory_before}MB -&gt; #{memory_after}MB (Δ#{memory_after - memory_before}MB)\"        # Force garbage collection and measure again to see retained memory    GC.start    memory_after_gc = `ps -o rss= -p #{Process.pid}`.to_i / 1024    Rails.logger.info \"[MEMORY] #{label} (after GC): #{memory_after_gc}MB (Δ#{memory_after_gc - memory_before}MB)\"  endend# Usage in controllerdef expensive_action  MemoryLogger.log(\"Processing users\") do    @users = User.process_all  endendConclusionMemory management in Ruby requires awareness and proactive optimization. The memory_profiler gem gives you powerful tools to identify memory issues and measure the impact of your optimizations.Key takeaways:  Measure before optimizing: Use memory_profiler to identify actual problem areas  Process in batches: Break large operations into manageable chunks  Select only what you need: Fetch only required columns from the database  Minimize object creation: Reuse objects where possible  Optimize string operations: String concatenation can be memory-intensive  Watch for memory leaks: Monitor memory usage over timeBy applying these techniques, you can build Ruby applications that are not only fast but also memory-efficient, resulting in more stable applications and lower hosting costs.Resources  memory_profiler GitHub repository  Ruby Garbage Collection Deep Dive  Ruby Performance Optimization by Alexander Dymo  Derailed Benchmarks - A Rails memory benchmarking tool"
  },
  
  {
    "title": "Quick Load Testing for Heroku Apps with Apache Bench",
    "url": "/posts/apache-bench-heroku/",
    "categories": "Heroku, Performance",
    "tags": "heroku, performance",
    "date": "2025-03-20 11:26:00 +0545",
    





    
    "snippet": "Apache Bench (ab) is a lightweight command-line tool that allows you to quickly perform load testing on web applications, making it ideal for Heroku-deployed Rails applications. This guide walks th...",
    "content": "Apache Bench (ab) is a lightweight command-line tool that allows you to quickly perform load testing on web applications, making it ideal for Heroku-deployed Rails applications. This guide walks through the process of setting up and running basic load tests on your Heroku app.Why Apache Bench?  Simplicity: No complex setup required  Speed: Tests can be executed in minutes  Built-in: Comes pre-installed on many systems  Lightweight: Minimal resource requirements  Sufficient: For many basic load testing needsPrerequisites  Apache Bench installed on your system          On macOS: Comes pre-installed      On Ubuntu/Debian: sudo apt-get install apache2-utils      On Windows: Install via Apache HTTP Server or use WSL        A Heroku application you want to test  Heroku CLI installed and configuredBasic Apache Bench Command SyntaxThe basic syntax for Apache Bench is:ab [options] [http[s]://]hostname[:port]/pathCommon options include:  -n: Number of requests to perform  -c: Number of concurrent requests  -t: Timelimit in seconds  -A: Supply basic authentication credentials  -C: Add cookie line to requests  -H: Add arbitrary header to requestsSetting Up for Heroku Testing1. Enable Enhanced LoggingFor better visibility during testing:heroku addons:upgrade logging:expanded --remote [staging/production]2. Add Monitoring (Optional but Recommended)New Relic provides valuable insights during load tests:heroku addons:add newrelic:standard --remote [staging/production]3. Scale Up Dynos Before TestingIncrease your app’s capacity temporarily for testing:heroku ps:scale web=4 --remote [staging/production]Running Basic Load TestsSimple Homepage TestTest your app’s homepage with 1,000 requests and 10 concurrent users:ab -n 1000 -c 10 https://your-app.herokuapp.com/Testing with AuthenticationIf your staging app is password protected:ab -n 5000 -c 50 -A username:password https://staging-app.herokuapp.com/Testing a Specific EndpointTest a specific API endpoint or page:ab -n 2000 -c 20 https://your-app.herokuapp.com/api/productsTesting with a SessionFor testing authenticated user flows, grab a cookie from your browser:ab -n 1000 -c 10 -C \"_session_id=1234abcd\" https://your-app.herokuapp.com/dashboardReal-World ExampleHere’s a practical example for a medium-traffic Heroku application:# Scale up dynos for testingheroku ps:scale web=12 --remote staging# Open logs in another terminal windowheroku logs -t --remote staging# Run the test (50k requests, 50 concurrent users)ab -n 50000 -c 50 -A user:password https://staging.your-app.com/# After testing, scale back downheroku ps:scale web=1 --remote stagingInterpreting ResultsApache Bench provides detailed statistics after each test:Server Software:        CowboyServer Hostname:        myapp.herokuapp.comServer Port:            443SSL/TLS Protocol:       TLSv1.2,ECDHE-RSA-AES128-GCM-SHA256,2048,128Document Path:          /Document Length:        11322 bytesConcurrency Level:      50Time taken for tests:   30.285 secondsComplete requests:      5000Failed requests:        0Total transferred:      59845000 bytesHTML transferred:       56610000 bytesRequests per second:    165.10 [#/sec] (mean)Time per request:       302.850 [ms] (mean)Time per request:       6.057 [ms] (mean, across all concurrent requests)Transfer rate:          1930.36 [Kbytes/sec] receivedKey metrics to observe:  Requests per second: Higher is better  Time per request: Lower is better  Failed requests: Should be zero or minimal  Connect/Processing/Waiting times: Helps identify bottlenecksMonitoring During TestsHeroku LogsWhile tests are running, watch your logs:heroku logs -t --remote stagingLook for:  Error rates  Request queuing (indicates you need more dynos)  Slow database queries  H12 errors (request timeout)New RelicCheck your New Relic dashboard during and after tests for:  Response time breakdown  Database load  Error rates  Apdex score  Memory usageRealistic Load Testing Strategy  Start small: Begin with low numbers (e.g., -n 500 -c 5)  Gradually increase: Double numbers until you see degradation  Test multiple endpoints: Different routes may have different bottlenecks  Mix in complex operations: Don’t just test the homepage  Test at different times: Performance can vary based on database size/activityLimitations of Apache BenchApache Bench is useful for quick tests but has limitations:  Limited to around 50 concurrent users  Tests single URLs rather than user journeys  Doesn’t simulate browser behavior (JS execution, asset loading)  Can’t simulate gradual traffic ramp-upFor more comprehensive testing, consider tools like:  Siege  JMeter  k6  Gatling  Tsung  Blitz.io (commercial)ConclusionApache Bench provides a quick, easy way to test your Heroku application’s performance under load. While not as comprehensive as dedicated load testing services, it gives you immediate feedback about how your application handles concurrent traffic and can help identify performance bottlenecks before they impact real users.Remember that the goal is to measure, improve, and measure again. Use the insights gained from load testing to guide your optimization efforts, whether that’s adding caching, optimizing database queries, or scaling your Heroku resources."
  },
  
  {
    "title": "Beyond ||=: Smarter Caching Strategies in Ruby",
    "url": "/posts/the-art-of-lazy-loading-ruby-memoization/",
    "categories": "Ruby, Performance",
    "tags": "ruby, performance",
    "date": "2025-03-20 10:50:00 +0545",
    





    
    "snippet": "Ruby developers love their shortcuts, and the memoization pattern using the ||= operator is one of the most widely used tricks in the Ruby world. But is it always the right tool for the job? Let’s ...",
    "content": "Ruby developers love their shortcuts, and the memoization pattern using the ||= operator is one of the most widely used tricks in the Ruby world. But is it always the right tool for the job? Let’s explore when to use memoization and when to consider alternatives.The Classic Memoization PatternWe’ve all seen (and probably written) code like this:def expensive_calculation  @result ||= perform_complex_workendThis elegant one-liner caches the result of perform_complex_work in the @result instance variable, ensuring the work is only done once. But this common pattern comes with trade-offs that aren’t always considered.When Memoization ShinesMemoization is most valuable in these scenarios:1. Expensive Operations That May Not Be Usedclass ReportGenerator  def executive_summary    @executive_summary ||= begin      puts \"Generating executive summary...\"      sleep(2) # Simulating expensive work      analyze_sales_data.merge(calculate_projections)    end  endend# Usagereport = ReportGenerator.new# No expensive work happens yetputs \"Report object created\"# Work happens on first callreport.executive_summary # Second call uses cached resultreport.executive_summary2. API Calls or Database Queriesclass UserProfile  def initialize(user_id)    @user_id = user_id  end    def recent_activities    @recent_activities ||= api_client.fetch_activities(@user_id)  endend3. Resource-Intensive Computationsclass StatisticalAnalyzer  def standard_deviation    @standard_deviation ||= calculate_standard_deviation  end    private    def calculate_standard_deviation    # Complex math that takes significant CPU time    puts \"Calculating standard deviation...\"    sleep(1)    42.0 # Just an example result  endendThe Hidden Costs of MemoizationBefore you ||= everything, consider these drawbacks:      It Obscures the Object Lifecycle: When values are calculated on-demand, it’s harder to reason about an object’s state.        Thread Safety Issues: The classic ||= pattern isn’t thread-safe by default.        Increased Complexity: Adding caching layers should be justified by measured performance gains.        Potential for Stale Data: Memoized values don’t automatically update when dependencies change.  Smart Alternatives to Consider1. Constructor Initialization (Eager Loading)When a value will always be needed, calculate it upfront:class Dashboard  attr_reader :user_statistics    def initialize(user)    @user = user    @user_statistics = calculate_user_statistics  end    private    def calculate_user_statistics    # Complex work here    { logins: 42, avg_session_time: 15.3 }  endend2. Computed Properties (No Caching)For simple derivations, sometimes no caching is needed:class Invoice  attr_reader :items    def total    # Often fast enough without caching    items.sum(&amp;:price)  endend3. Method-Level Caching with Separation of ConcernsSeparate the caching logic from the calculation:class ProductCatalog  def featured_products    @featured_products ||= compute_featured_products  end    def refresh_featured!    @featured_products = compute_featured_products  end    private    def compute_featured_products    puts \"Computing featured products...\"    Product.where(featured: true).order(popularity: :desc).limit(10)  endend# Usagecatalog = ProductCatalog.newcatalog.featured_products # Computes and cachescatalog.featured_products # Uses cachecatalog.refresh_featured! # Forces recalculationcatalog.featured_products # Uses new cache4. Use Ruby’s Memoizable Module or Similar LibrariesFor more complex caching needs, consider gems like memoist:require 'memoist'class WeatherService  extend Memoist    def forecast(city)    puts \"Fetching forecast for #{city}...\"    # API call here    { temp: 22, conditions: \"Sunny\" }  end  memoize :forecastend# Usageweather = WeatherService.newweather.forecast(\"Tokyo\")  # Makes API callweather.forecast(\"Tokyo\")  # Uses cacheweather.forecast(\"London\") # Makes new API callMaking the Right ChoiceTo decide whether memoization is appropriate, ask yourself:  Is the operation actually expensive? Benchmark before optimizing.  Will the value be used multiple times? If not, memoization adds complexity without benefit.  Does the data need to stay fresh? Memoized values don’t auto-update.  Is thread safety a concern? Consider thread-safe alternatives if needed.A Decision Framework            Scenario      Best Approach                  Always needed, expensive      Constructor initialization              May not be needed, expensive      Memoization              Used multiple times, changes rarely      Memoization with refresh method              Simple calculation      No caching              Needs thread safety      Thread-safe caching library      ConclusionMemoization is a powerful technique in Ruby, but it’s not a universal solution. By understanding the trade-offs and alternatives, you can make more informed decisions about when to cache and how to implement it effectively.Remember that the most elegant code is often the simplest. Before adding complexity through caching, ensure you’re solving a real performance problem rather than an imagined one."
  },
  
  {
    "title": "Conquering the N+1 Query Problem in Rails: A Performance Deep Dive",
    "url": "/posts/rails-n-plus-one-query-problem/",
    "categories": "Ruby on Rails, Performance",
    "tags": "ruby on rails, performance",
    "date": "2025-03-20 10:50:00 +0545",
    





    
    "snippet": "If you’ve been developing Rails applications for any length of time, you’ve likely encountered the infamous N+1 query problem—perhaps without even realizing it. This performance bottleneck can sile...",
    "content": "If you’ve been developing Rails applications for any length of time, you’ve likely encountered the infamous N+1 query problem—perhaps without even realizing it. This performance bottleneck can silently slow your application to a crawl as your data grows. In this article, we’ll dive deep into understanding, identifying, and solving the N+1 problem with practical, real-world examples and benchmarks.What Is the N+1 Query Problem?The N+1 query problem is a database performance anti-pattern where your application executes one query to retrieve a collection of records (the “1”), followed by N additional queries (one for each record in the collection) to retrieve related data. This approach can significantly degrade performance, especially as your dataset grows.Let’s illustrate this with a simple example:# This innocent-looking code hides a serious performance issueposts = Post.allposts.each do |post|  puts post.user.name  # Each access to post.user triggers a separate database queryendWhen executed, this code generates SQL that looks something like:SELECT * FROM posts;                           -- The \"1\" querySELECT * FROM users WHERE id = 1 LIMIT 1;      -- First of the \"N\" queriesSELECT * FROM users WHERE id = 2 LIMIT 1;      -- Second of the \"N\" queriesSELECT * FROM users WHERE id = 3 LIMIT 1;      -- And so on...Benchmarking the ImpactLet’s first understand the magnitude of the problem using Ruby’s Benchmark module. We’ll compare the performance of code with and without the N+1 problem:require 'benchmark'# Setup test data (in a real application you'd have this data already)10.times do |i|  user = User.create!(name: \"User #{i}\", email: \"user#{i}@example.com\")  5.times do |j|    Post.create!(title: \"Post #{j} by User #{i}\", content: \"Content...\", user: user)  endendputs \"Benchmarking with N+1 problem:\"time_with_n_plus_one = Benchmark.measure do  posts = Post.all  posts.each do |post|    puts \"#{post.title} by #{post.user.name}\"  endendputs \"Benchmarking with eager loading (solution to N+1):\"time_with_eager_loading = Benchmark.measure do  posts = Post.includes(:user).all  posts.each do |post|    puts \"#{post.title} by #{post.user.name}\"  endendputs \"Time with N+1 problem: #{time_with_n_plus_one.real} seconds\"puts \"Time with eager loading: #{time_with_eager_loading.real} seconds\"puts \"Performance improvement: #{(time_with_n_plus_one.real / time_with_eager_loading.real).round(2)}x faster\"For a modest dataset with just 50 posts across 10 users, you might see results like:Time with N+1 problem: 0.2812 secondsTime with eager loading: 0.0431 secondsPerformance improvement: 6.52x fasterAs your dataset grows, this performance gap widens dramatically.Spotting N+1 Problems in Your Rails AppTelltale Signs in Your LogsThe most straightforward way to identify N+1 issues is by reviewing your development or production logs. Look for patterns of repeated, similar queries occurring in succession:Post Load (0.5ms)  SELECT \"posts\".* FROM \"posts\"User Load (0.3ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = $1 LIMIT $2  [[\"id\", 1], [\"LIMIT\", 1]]User Load (0.2ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = $1 LIMIT $2  [[\"id\", 2], [\"LIMIT\", 1]]User Load (0.2ms)  SELECT \"users\".* FROM \"users\" WHERE \"users\".\"id\" = $1 LIMIT $2  [[\"id\", 3], [\"LIMIT\", 1]]This pattern—one query followed by many similar queries with different parameters—is the classic signature of an N+1 problem.Using the Bullet GemThe Bullet gem is a fantastic tool for automatically detecting N+1 queries. Here’s how to set it up:# Gemfilegem 'bullet', group: [:development, :test]# config/environments/development.rbconfig.after_initialize do  Bullet.enable = true  Bullet.alert = true  # JavaScript alerts in the browser  Bullet.console = true  # Logs to browser console  Bullet.rails_logger = true  # Logs to Rails logger  Bullet.add_footer = true  # Adds details to HTML footerendBullet will now notify you whenever it detects an N+1 query in your application, suggesting exactly where to add eager loading.Common Solutions to the N+1 Problem1. Eager Loading with includesThe most common solution is to use Rails’ includes method, which tells Rails to load the associated records in as few queries as possible:# Instead of:posts = Post.all# Use:posts = Post.includes(:user)# For multiple associations:posts = Post.includes(:user, :comments, :tags)# For nested associations:posts = Post.includes(user: :profile, comments: [:user, :likes])With includes, Rails will typically execute just two queries regardless of how many posts you have:SELECT * FROM posts;SELECT * FROM users WHERE id IN (1, 2, 3, ...);2. Using preload for Specific Loading StrategiesSometimes you need finer control over how associations are loaded. Rails provides preload:# This forces separate queriesposts = Post.preload(:user)This will always use separate queries for each association (never a JOIN), which can be beneficial when retrieving large result sets.3. Using eager_load for JOIN-based LoadingWhen you need to filter based on associated records, eager_load is your friend:# This will use a LEFT OUTER JOINposts = Post.eager_load(:user).where(users: { role: 'admin' })This generates a query with a JOIN, allowing you to filter the primary collection based on conditions on the associated records.4. Using joins for More Complex FilteringFor more complex filtering without loading associated records:# Find all posts written by users with a specific email domainposts = Post.joins(:user).where(\"users.email LIKE ?\", \"%@example.com\")This uses a JOIN but doesn’t load the associated records into memory—useful when you need to filter but don’t need the associated data.5. Batching with find_each and in_batchesFor processing large collections efficiently:# Process records in batches of 1000Post.find_each(batch_size: 1000) do |post|  # This automatically includes batch finding to reduce memory consumption  process_post(post)endAdvanced Techniques1. Optimizing with selectSometimes you don’t need all attributes of your records:# Only select the fields you needposts = Post.select(:id, :title).includes(:user)# You can also select specific fields from associationsposts = Post.includes(:user).references(:user).select('posts.*, users.name as author_name')2. Counter Cache ColumnsFor situations where you frequently count associations, use counter caches:# In your migrationadd_column :users, :posts_count, :integer, default: 0# In your Post modelclass Post &lt; ApplicationRecord  belongs_to :user, counter_cache: trueend# Now instead of:user.posts.count  # Executes a COUNT query# You can use:user.posts_count  # Uses the cached value3. Custom Benchmarking Class for N+1 DetectionCreate a custom class to help identify potential N+1 problems in your codebase:class QueryCounter  attr_reader :count    def initialize    @count = 0  end    def self.track    counter = new    subscription = ActiveSupport::Notifications.subscribe('sql.active_record') do |*args|      event = ActiveSupport::Notifications::Event.new(*args)      counter.count += 1 unless event.payload[:name] == 'SCHEMA' || event.payload[:sql].include?('BEGIN') || event.payload[:sql].include?('COMMIT')    end        yield        ActiveSupport::Notifications.unsubscribe(subscription)    counter  endend# Usagecounter = QueryCounter.track do  # Code that might have N+1 issues  Post.all.each { |post| puts post.user.name }endputs \"Executed #{counter.count} queries.\"Real-World Example: Beyond Simple AssociationsLet’s tackle a more complex example involving multiple levels of associations:class Blog &lt; ApplicationRecord  has_many :postsendclass Post &lt; ApplicationRecord  belongs_to :blog  belongs_to :user  has_many :comments  has_many :tags, through: :taggingsendclass User &lt; ApplicationRecord  has_many :posts  has_one :profileendclass Comment &lt; ApplicationRecord  belongs_to :post  belongs_to :userendclass Tag &lt; ApplicationRecord  has_many :taggings  has_many :posts, through: :taggingsendclass Tagging &lt; ApplicationRecord  belongs_to :post  belongs_to :tagendclass Profile &lt; ApplicationRecord  belongs_to :userendNow, imagine we want to display blogs with their posts, each post’s author details, and the post’s comments and tags:# Inefficient approach with N+1 problems:blogs = Blog.allblogs.each do |blog|  puts \"Blog: #{blog.title}\"    blog.posts.each do |post|    puts \"  Post: #{post.title} by #{post.user.name} (#{post.user.profile.bio})\"        post.comments.each do |comment|      puts \"    Comment by: #{comment.user.name}\"    end        post.tags.each do |tag|      puts \"    Tagged with: #{tag.name}\"    end  endendThis seemingly innocent code could generate hundreds of queries! Let’s fix it:# Efficient approach with proper eager loading:blogs = Blog.includes(  posts: [    {user: :profile},    {comments: :user},    :tags  ])# Same output loop, but now with drastically fewer queriesblogs.each do |blog|  puts \"Blog: #{blog.title}\"    blog.posts.each do |post|    puts \"  Post: #{post.title} by #{post.user.name} (#{post.user.profile.bio})\"        post.comments.each do |comment|      puts \"    Comment by: #{comment.user.name}\"    end        post.tags.each do |tag|      puts \"    Tagged with: #{tag.name}\"    end  endendLet’s benchmark this complex scenario:require 'benchmark'puts \"Benchmarking complex N+1 scenario:\"time_with_n_plus_one = Benchmark.measure do  blogs = Blog.all  # ... (inefficient loop from above)endputs \"Benchmarking with comprehensive eager loading:\"time_with_eager_loading = Benchmark.measure do  blogs = Blog.includes(posts: [{user: :profile}, {comments: :user}, :tags])  # ... (same loop)endputs \"Time with N+1 problem: #{time_with_n_plus_one.real} seconds\"puts \"Time with eager loading: #{time_with_eager_loading.real} seconds\"puts \"Performance improvement: #{(time_with_n_plus_one.real / time_with_eager_loading.real).round(2)}x faster\"With a moderate dataset, you might see a performance improvement of 20x or more!Caveats and ConsiderationsWhile eager loading is powerful, it’s not always the right solution:      Memory usage: Eager loading loads all associated records into memory. For very large datasets, this can consume significant RAM.        Unused data: If you’re not actually using all the eager loaded associations in your code, you’re wasting resources.        JOINs complexity: Complex eager loading with many nested associations can result in inefficient JOINs. In such cases, multiple targeted queries might be faster.        Database-specific optimization: Different databases have different query optimization capabilities. PostgreSQL might handle certain complex JOINs better than MySQL, for example.  ConclusionThe N+1 query problem is one of the most common performance issues in Rails applications, but also one of the most solvable. By understanding the problem, learning to identify it in your own code, and applying the right solutions, you can dramatically improve your application’s performance.Remember to:  Use Rails’ includes, preload, and eager_load methods appropriately  Monitor your application logs for signs of N+1 queries  Use tools like the Bullet gem for automated detection  Benchmark your improvements to ensure they’re having the desired effect  Consider both performance and memory usage when optimizingBy keeping these practices in mind, you’ll be well on your way to faster, more efficient Rails applications that can handle larger datasets with ease.Additional Resources  Rails Guide on Active Record Query Interface  Bullet gem documentation  rack-mini-profiler gem for performance profiling  Rails APM tools such as Scout, New Relic, or AppSignal"
  },
  
  {
    "title": "Mastering Rails Performance Benchmarking: A Developer's Guide",
    "url": "/posts/rails-benchmarking/",
    "categories": "Ruby on Rails, Performance",
    "tags": "ruby on rails, performance",
    "date": "2025-03-20 08:53:10 +0545",
    





    
    "snippet": "In the world of Rails application development, performance isn’t just a nice-to-have—it’s essential. As applications grow in complexity and user base, even small inefficiencies can compound into si...",
    "content": "In the world of Rails application development, performance isn’t just a nice-to-have—it’s essential. As applications grow in complexity and user base, even small inefficiencies can compound into significant performance bottlenecks. This is where benchmarking becomes an invaluable tool in a developer’s arsenal.Understanding Benchmarking in Ruby on RailsBenchmarking is the systematic process of measuring and evaluating your code’s performance metrics. It allows you to identify bottlenecks, compare alternative implementations, and make data-driven optimization decisions rather than relying on intuition.Why Benchmark Your Rails Application?  Identify performance bottlenecks: Find which parts of your application consume the most resources  Data-driven decision making: Choose between implementation approaches based on concrete metrics  Validate optimizations: Verify that your changes actually improve performance  Establish baselines: Create performance standards for your applicationRuby’s Built-in Benchmark ModuleRuby ships with a powerful Benchmark module in its standard library, which provides several methods for measuring code execution time. Let’s explore how to use it effectively in a Rails environment.Setting Up Your Benchmarking EnvironmentFirst, let’s set up a proper benchmarking environment in your Rails application:# In a Rails console or dedicated benchmark scriptrequire 'benchmark'# Optional: Direct output to a log filelog_file = File.open('log/benchmark_results.log', 'a')log_file.sync = true$stdout = log_file# Use this to restore standard output when needed# $stdout = STDOUTBasic Benchmarking TechniquesBenchmark.measure: Timing a Single OperationThe simplest form of benchmarking is measuring how long a single block of code takes to execute:result = Benchmark.measure do  User.where(active: true).includes(:posts, :comments).each do |user|    user.recalculate_statistics!  endendputs resultThis outputs something like:  0.350000   0.050000   0.400000 (  0.412412)The four numbers represent:  User CPU time  System CPU time  Total CPU time (user + system)  Real elapsed time (wall clock time)Benchmark.bm: Comparing Multiple OperationsWhen you want to compare the performance of different approaches, Benchmark.bm is your friend:Benchmark.bm(20) do |x|  # Approach 1: Using ActiveRecord  x.report(\"ActiveRecord:\") do    Post.where(published: true).count  end    # Approach 2: Using raw SQL  x.report(\"Raw SQL:\") do    ActiveRecord::Base.connection.execute(\"SELECT COUNT(*) FROM posts WHERE published = true\").first[\"count\"]  end    # Approach 3: Using Rails counter cache  x.report(\"Counter cache:\") do    Category.sum(:published_posts_count)  endendThe parameter 20 specifies the label width for better formatting of the output.Benchmark.bmbm: Addressing Memory Warm-up IssuesRuby’s garbage collector and other runtime considerations can sometimes skew your benchmark results. Benchmark.bmbm (or “burn-in benchmark”) runs the code twice—once as a rehearsal to warm up the environment, and once for the actual measurement:Benchmark.bmbm(20) do |x|  x.report(\"String concat:\") do    result = \"\"    10000.times { result += \"x\" }  end    x.report(\"Array join:\") do    result = []    10000.times { result &lt;&lt; \"x\" }    result.join  endendAdvanced Benchmarking StrategiesBenchmark.ips: Operations Per SecondWhile not part of the standard library, the benchmark-ips gem provides a more sophisticated approach by measuring iterations per second, which often gives more meaningful comparisons:# Gemfilegem 'benchmark-ips'# In your benchmark coderequire 'benchmark/ips'Benchmark.ips do |x|  x.report(\"Pluck:\") { User.pluck(:email) }  x.report(\"Map:\") { User.all.map(&amp;:email) }  x.compare!endThe compare! method will show how many times faster one approach is compared to others.Creating a Custom Benchmarking ClassFor more structured benchmarking in a Rails application, consider creating a custom benchmarking class:class PerformanceBenchmark  class &lt;&lt; self    def compare_query_methods(dataset_size: 1000)      # Create test data      User.transaction do        dataset_size.times do |i|          User.create!(            name: \"User #{i}\",            email: \"user_#{i}@example.com\",            active: i.even?          )        end                Benchmark.bmbm(25) do |x|          x.report(\"where:\") { User.where(active: true).to_a }          x.report(\"find_by_sql:\") { User.find_by_sql(\"SELECT * FROM users WHERE active = true\") }          x.report(\"in batches:\") { [].tap { |results| User.where(active: true).in_batches(of: 100) { |batch| results.concat(batch.to_a) } } }        end                # Clean up test data        raise ActiveRecord::Rollback      end    end        def profile_action(times: 10, &amp;block)      results = []            times.times do        results &lt;&lt; Benchmark.measure(&amp;block).real      end            {        min: results.min,        max: results.max,        avg: results.sum / results.size,        median: results.sort[results.size / 2]      }    end  endendUsage:PerformanceBenchmark.compare_query_methods(dataset_size: 5000)results = PerformanceBenchmark.profile_action(times: 20) do  UsersController.new.indexendputs \"Average response time: #{results[:avg]}s\"Benchmarking in ProductionFor production environments, consider these approaches:Request-level Benchmarking with ActiveSupport::NotificationsRails provides a powerful instrumentation API through ActiveSupport::Notifications:# In an initializerActiveSupport::Notifications.subscribe(\"process_action.action_controller\") do |*args|  event = ActiveSupport::Notifications::Event.new(*args)  payload = event.payload    if payload[:controller] == \"UsersController\" &amp;&amp; payload[:action] == \"index\"    Rails.logger.info(      \"UsersController#index performance: #{event.duration.round(2)}ms, \" +      \"DB: #{payload[:db_runtime].round(2)}ms, \" +      \"View: #{payload[:view_runtime].round(2)}ms\"    )  endendDatabase Query BenchmarkingTo specifically benchmark database operations:class QueryBenchmark  def self.analyze_query(sql)    connection = ActiveRecord::Base.connection        result = Benchmark.measure do      connection.execute(\"EXPLAIN ANALYZE #{sql}\")    end        puts \"Query execution time: #{result.real.round(4)}s\"  endendQueryBenchmark.analyze_query(\"SELECT * FROM users WHERE created_at &gt; '2023-01-01'\")Practical Real-world ExamplesExample 1: Optimizing User Authenticationclass AuthBenchmark  def self.compare_authentication_methods(iterations = 1000)    user = User.create!(email: \"test@example.com\", password: \"password123\")        Benchmark.bm(25) do |x|      x.report(\"Database lookup:\") do        iterations.times do          User.find_by(email: \"test@example.com\")&amp;.authenticate(\"password123\")        end      end            x.report(\"Cache + Database:\") do        iterations.times do          cached_user = Rails.cache.fetch(\"user/test@example.com\", expires_in: 5.minutes) do            User.find_by(email: \"test@example.com\")          end          cached_user&amp;.authenticate(\"password123\")        end      end            x.report(\"JWT token validation:\") do        token = JWT.encode({ user_id: user.id, exp: Time.now.to_i + 3600 }, Rails.application.credentials.secret_key_base)                iterations.times do          begin            decoded = JWT.decode(token, Rails.application.credentials.secret_key_base)[0]            User.find(decoded[\"user_id\"]) if decoded[\"exp\"] &gt; Time.now.to_i          rescue JWT::DecodeError            nil          end        end      end    end        user.destroy  endendAuthBenchmark.compare_authentication_methodsExample 2: Data Serialization Performanceclass SerializationBenchmark  def self.compare_serialization_methods    user = User.create!(      name: \"John Doe\",      email: \"john@example.com\",      posts: Array.new(10) { |i| Post.create!(title: \"Post #{i}\", body: \"Content #{i}\") }    )        Benchmark.bm(20) do |x|      x.report(\"ActiveModel::Serializer:\") do        100.times { ActiveModelSerializers::SerializableResource.new(user, include: [:posts]).to_json }      end            x.report(\"Jbuilder:\") do        100.times do          Jbuilder.encode do |json|            json.id user.id            json.name user.name            json.email user.email            json.posts user.posts do |post|              json.id post.id              json.title post.title            end          end        end      end            x.report(\"Custom to_json:\") do        100.times do          {            id: user.id,            name: user.name,            email: user.email,            posts: user.posts.map { |p| { id: p.id, title: p.title } }          }.to_json        end      end    end        user.destroy  endendSerializationBenchmark.compare_serialization_methodsBest Practices for Accurate Benchmarking  Run multiple iterations: Single measurements can be misleading due to variance  Warm up the environment: Run the code at least once before measuring  Eliminate external factors: Disable logging, background jobs, and other services  Use realistic data volumes: Test with dataset sizes similar to production  Benchmark in isolation: Test one component at a time for clear results  Consider statistical significance: Use average of multiple runs to account for variance  Test on production-like hardware: Development machines may perform differentlyInterpreting Benchmark ResultsWhen analyzing benchmark results:  Look for orders of magnitude: Small differences (5-10%) might not be significant  Consider the real-world impact: Optimize code that runs frequently or with large datasets  Balance performance with readability: Sometimes slightly slower code is worth it for maintainability  Profile before optimizing: Don’t guess at bottlenecks—measure first  Consider memory usage alongside speed: Faster might not be better if it consumes far more memoryConclusionBenchmarking is an essential skill for Rails developers who want to build high-performance applications. By systematically measuring and comparing different approaches, you can make informed decisions that balance speed, memory usage, and code maintainability.Remember that premature optimization is the root of all evil—benchmark first, then optimize where it matters most, and always validate your optimizations with data.Resources  Ruby Benchmark Documentation  Rails Active Support Instrumentation Guide  benchmark-ips gem  memory_profiler gem  rack-mini-profiler gem"
  },
  
  {
    "title": "Choosing the Right Stack: MERN vs Next.js vs Rails + Next.js",
    "url": "/posts/mern_vs_nextjs/",
    "categories": "MERN, NextJS",
    "tags": "mern, nextjs",
    "date": "2025-03-20 02:40:00 +0545",
    





    
    "snippet": "As a developer embarking on a new web project, one of the most crucial decisions you’ll make is choosing the right technology stack. This decision will influence your development speed, application...",
    "content": "As a developer embarking on a new web project, one of the most crucial decisions you’ll make is choosing the right technology stack. This decision will influence your development speed, application performance, maintainability, and even your team’s happiness. If you’re considering a JavaScript-based frontend, you have several excellent options for structuring your application.In this post, I’ll explore three popular approaches for building modern web applications:  The MERN Stack (MongoDB, Express.js, React, Node.js)  Next.js as a full-stack solution  Ruby on Rails backend with Next.js frontendI’ll compare these approaches across several dimensions including development speed, performance, scalability, and developer experience to help you make an informed decision for your project.Option 1: The MERN StackThe MERN stack is a popular JavaScript-based tech stack that uses MongoDB as the database, Express.js as the server framework, React for the frontend, and Node.js as the runtime environment.How It WorksIn a typical MERN stack architecture:  MongoDB stores your data as JSON-like documents with flexible schemas  Express.js provides a framework for building your API endpoints  React handles the user interface and client-side logic  Node.js executes your server-side JavaScript codeFor example, your Express server might define routes like:// routes/api/users.jsrouter.post('/', async (req, res) =&gt; {  try {    const { name, email, password } = req.body;        // Check if user exists    let user = await User.findOne({ email });    if (user) {      return res.status(400).json({ errors: [{ msg: 'User already exists' }] });    }        // Create new user    user = new User({      name,      email,      password    });        // Hash password    const salt = await bcrypt.genSalt(10);    user.password = await bcrypt.hash(password, salt);    await user.save();        // Generate JWT    const payload = { user: { id: user.id } };    jwt.sign(payload, config.get('jwtSecret'), { expiresIn: 360000 }, (err, token) =&gt; {      if (err) throw err;      res.json({ token });    });  } catch (err) {    console.error(err.message);    res.status(500).send('Server Error');  }});Advantages  JavaScript Everywhere: Using JavaScript for both frontend and backend means you don’t need to context-switch between languages.  Flexible Data Structure: MongoDB’s schema-less nature makes it easy to evolve your data structure over time.  Large Ecosystem: Each component of the MERN stack has extensive libraries and tools.  RESTful API Architecture: Clear separation between frontend and backend forces good API design practices.  Real-time Applications: Node.js excels at handling real-time, data-intensive applications.Challenges  Manual Setup for SEO: You’ll need to implement additional solutions for SEO, as React’s client-side rendering isn’t optimal for search engines by default.  More Configuration: You’ll need to set up routing, state management, and server-side rendering yourself.  DevOps Complexity: You’ll need to deploy and manage both a Node.js server and a React application.  Learning Curve for NoSQL: If you’re coming from a relational database background, MongoDB might require some adjustment.Option 2: Next.js as a Full-Stack SolutionNext.js is a React framework that provides structure, features, and optimizations for your React application. Recent versions of Next.js have evolved to provide full-stack capabilities.How It WorksNext.js simplifies the development process by providing:  Server-Side Rendering (SSR): Renders pages on the server for better SEO and initial load times  Static Site Generation (SSG): Pre-renders pages at build time for optimal performance  API Routes: Create API endpoints directly within your Next.js application  File-based Routing: Define routes based on your file structureFor example, you might structure your API endpoints like this:// pages/api/users.jsimport bcrypt from 'bcryptjs';import jwt from 'jsonwebtoken';import User from '../../models/User';export default async function handler(req, res) {  if (req.method === 'POST') {    const { name, email, password } = req.body;    try {      // Check if user exists      let user = await User.findOne({ email });      if (user) {        return res.status(400).json({ errors: [{ msg: 'User already exists' }] });      }      // Create new user      user = new User({        name,        email,        password      });      // Hash password      const salt = await bcrypt.genSalt(10);      user.password = await bcrypt.hash(password, salt);      await user.save();      // Generate JWT      const payload = { user: { id: user.id } };      const token = jwt.sign(payload, process.env.JWT_SECRET, { expiresIn: '100h' });      res.status(200).json({ token });    } catch (err) {      console.error(err.message);      res.status(500).json({ errors: [{ msg: 'Server error' }] });    }  } else {    res.status(405).end(); // Method Not Allowed  }}Advantages  Built-in SSR and SSG: Excellent for SEO and performance  Simplified Development: File-based routing and API routes reduce boilerplate  Unified Deployment: Deploy both frontend and backend as a single application  Image Optimization: Built-in image optimization for better performance  Incremental Static Regeneration (ISR): Update static content without rebuilding the entire siteChallenges  Learning Curve: Next.js has its own patterns and conventions to learn  Less Flexibility: The unified approach can be constraining for complex backend logic  Database Integration: You still need to set up and configure your database connection  Complex State Management: For large applications, you may need additional state management solutionsOption 3: Ruby on Rails Backend with Next.js FrontendThis approach combines the mature backend capabilities of Ruby on Rails with the modern frontend features of Next.js.How It WorksIn this architecture:  Ruby on Rails serves as an API-only backend, handling database operations, complex business logic, and authentication  Next.js handles the frontend, consuming the Rails APIYour Rails controller might look like:# app/controllers/api/users_controller.rbclass Api::UsersController &lt; ApplicationController  def create    user = User.new(user_params)    if user.save      token = JWT.encode({ user_id: user.id }, ENV['JWT_SECRET'], 'HS256')      render json: { token: token }, status: :created    else      render json: { errors: user.errors.full_messages }, status: :unprocessable_entity    end  end    private    def user_params    params.require(:user).permit(:name, :email, :password, :password_confirmation)  endendAnd your Next.js page might fetch data like:// pages/dashboard.jsexport async function getServerSideProps(context) {  const token = context.req.cookies.token;    try {    const res = await fetch(`${process.env.RAILS_API_URL}/api/dashboard`, {      headers: {        'Authorization': `Bearer ${token}`      }    });        if (res.ok) {      const data = await res.json();      return { props: { data } };    } else {      return {        redirect: {          destination: '/login',          permanent: false,        },      };    }  } catch (error) {    return {      redirect: {        destination: '/login',        permanent: false,      },    };  }}Advantages  Best of Both Worlds: Leverage Rails’ mature backend capabilities with Next.js’ frontend optimizations  Rails Ecosystem: Access to Rails’ robust gems for things like authentication, authorization, and admin panels  Strong Conventions: Rails’ “convention over configuration” philosophy can speed up backend development  Database Migrations: Rails’ migration system makes database changes safe and manageable  Active Record: Rails’ ORM simplifies database interactionsChallenges  Multiple Languages: Working with both Ruby and JavaScript requires context-switching  Deployment Complexity: You’ll need to deploy and manage two separate applications  Authentication Coordination: Ensuring secure authentication between the two systems requires careful planning  API Design: You’ll need to thoughtfully design the API contract between your frontend and backend  Team Expertise: Requires team members familiar with both ecosystemsWhich Stack Should You Choose?The best choice depends on your specific needs and constraints:Choose MERN if:  You want to work exclusively with JavaScript  Your team has strong Node.js and MongoDB experience  You value flexibility in your database schema  You’re building data-intensive applications with real-time features  You need fine-grained control over your backend architectureChoose Next.js if:  SEO is a critical concern for your application  You want faster development with less configuration  You prefer a unified deployment model  You’re comfortable with its conventions and constraints  Your backend logic is relatively straightforwardChoose Rails + Next.js if:  You have existing Rails expertise on your team  Your application requires complex backend business logic  You value Rails’ mature ecosystem for things like admin interfaces  Data integrity and relational data are important for your application  You want to leverage Rails’ battle-tested security featuresConclusionThere’s no universally “right” choice among these three options - each has its strengths and weaknesses. Consider your team’s skills, your project’s specific requirements, and your long-term maintenance plans when making your decision.The JavaScript ecosystem continues to evolve rapidly, and each of these approaches represents a valid way to build modern web applications. By understanding the trade-offs involved, you can make a more informed decision that aligns with your project goals and team capabilities.Have you built applications using one of these stacks? What were your experiences? I’d love to hear your thoughts in the comments!This blog post was created to help developers understand the trade-offs between different tech stacks for building web applications. The code examples are simplified for illustration purposes."
  },
  
  {
    "title": "Mastering PostgreSQL Performance: Proactive Practices to Prevent Bottlenecks",
    "url": "/posts/postgresql-optimization-prevent-performance-bottleneck/",
    "categories": "Ruby on Rails, Performance, PostgreSQL",
    "tags": "ruby on rails, performance, postgresql",
    "date": "2025-03-20 01:11:10 +0545",
    





    
    "snippet": "Database performance is often the silent killer of application responsiveness. As your PostgreSQL database grows with your business, seemingly innocent operations can lead to significant performanc...",
    "content": "Database performance is often the silent killer of application responsiveness. As your PostgreSQL database grows with your business, seemingly innocent operations can lead to significant performance degradation. The proactive practices can help prevent performance bottlenecks before they impact end users.Understanding PostgreSQL Bloat: The Hidden Performance KillerOne of the most insidious performance issues in PostgreSQL is bloat. But what exactly is bloat, and why should you care about it?What Is PostgreSQL Bloat?Bloat occurs when PostgreSQL’s MVCC (Multi-Version Concurrency Control) system leaves behind dead tuples (rows) that aren’t immediately removed from tables and indexes. Consider this seemingly innocent Rails code:User.where(active: false).update_all(status: 'inactive')Behind the scenes, PostgreSQL doesn’t actually update the existing rows. Instead, it:  Creates new versions of these rows with the updated status value  Leaves the old versions as “dead tuples” to maintain MVCC guarantees  Relies on VACUUM processes to eventually clean up these dead tuplesWithout proper vacuuming, these dead tuples accumulate, causing:  Wasted disk space  Slower queries (PostgreSQL must scan through dead tuples)  Degraded index performance  Increased I/O operationsDetecting Bloat in Your DatabaseBefore you can fix bloat, you need to detect it. Here are some queries that can help identify table and index bloat:-- For table bloat estimationSELECT schemaname, relname, n_dead_tup, n_live_tup,       round(n_dead_tup * 100.0 / (n_live_tup + n_dead_tup), 1) AS dead_percentageFROM pg_stat_user_tablesWHERE n_live_tup &gt; 0ORDER BY dead_percentage DESC;-- For index bloat estimation (simplified)SELECT indexrelname, relname, idx_scan,        pg_size_pretty(pg_relation_size(indexrelid)) AS index_sizeFROM pg_stat_user_indexesORDER BY pg_relation_size(indexrelid) DESCLIMIT 20;Strategies for Managing and Preventing Bloat1. Optimizing VACUUM OperationsVACUUM is PostgreSQL’s built-in mechanism for reclaiming space from dead tuples. There are two main approaches:Regular VACUUMVACUUM ANALYZE your_table;This command:  Reclaims space from dead tuples for future use  Updates statistics for the query planner  Is non-blocking (doesn’t lock the table)  Doesn’t return disk space to the operating systemVACUUM FULL (Use with Caution!)VACUUM FULL your_table;According to PostgreSQL experts, you should avoid using VACUUM FULL in production as it:  Requires an exclusive lock on the table  Completely rewrites the table (causes downtime)  Can cause extended service interruptionsInstead, for production environments, it’s recommended to use the pg_repack extension, which can reclaim space without the lengthy downtime of VACUUM FULL.2. Tuning AutoVacuum for Optimal PerformanceAutoVacuum is PostgreSQL’s background process that automatically runs VACUUM on tables that meet certain thresholds. For tables with heavy write loads or bulk operations, default settings may not be aggressive enough.Recommended settings for tables with high write activity:ALTER TABLE high_write_table SET (  autovacuum_vacuum_scale_factor = 0.01,  autovacuum_vacuum_threshold = 50,  autovacuum_analyze_scale_factor = 0.005,  autovacuum_analyze_threshold = 50);You can also adjust global settings in postgresql.conf:autovacuum_naptime = 10s                  # Run more frequentlyautovacuum_vacuum_scale_factor = 0.01     # Trigger at 1% of table sizeautovacuum_max_workers = 6                # More workers for larger systemsTo monitor current autovacuum activity:SELECT datname, usename, query, state, backend_typeFROM pg_stat_activityWHERE query LIKE '%autovacuum%';3. Managing Index BloatIndexes suffer from bloat too, sometimes even more severely than tables. When indexes become bloated:  Query performance degrades as PostgreSQL must scan through more index pages  Memory usage increases as more of the bloated index needs to be cached  Write operations slow down as index updates become more expensiveSafe Reindexing in ProductionFor small tables, a simple reindex works:REINDEX INDEX your_index;However, this locks the table for writes. For production systems, the recommended approach is:-- Create a new index without blocking operationsCREATE INDEX CONCURRENTLY new_index_name ON your_table(column);-- Update dependencies (constraints, etc.) to use the new indexALTER TABLE your_table DROP CONSTRAINT constraint_name;ALTER TABLE your_table ADD CONSTRAINT constraint_name   PRIMARY KEY USING INDEX new_index_name;-- Drop the old indexDROP INDEX CONCURRENTLY old_index_name;Scheduling monthly concurrent reindexing for critical tables can be a good preventative measure.4. Data Type Considerations for PerformanceWhile UUIDs are popular for distributed systems:  Random UUIDs cause scattered writes across B-tree indexes  This scattering leads to increased index fragmentation and bloatInstead, consider:  Using ULID (Universally Unique Lexicographically Sortable Identifier)  Sequential UUIDs  PostgreSQL 18 will have improved handling of UUID indexingBeyond Bloat: Other PostgreSQL Performance OptimizationsMemory ConfigurationPostgreSQL performance heavily depends on effective memory usage:  OS Page Cache: The operating system’s cache for recently accessed disk pages  PostgreSQL Buffer Cache: PostgreSQL’s own cache for table and index dataYou can query the buffer cache effectiveness:SELECT   sum(heap_blks_read) as heap_read,  sum(heap_blks_hit) as heap_hit,  sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read)) as ratioFROM pg_statio_user_tables;A ratio above 0.99 (99%) indicates good cache utilization.Checkpoint TuningCheckpoints write dirty pages from memory to disk. Too-frequent checkpoints can cause I/O spikes.Recommended settings:checkpoint_timeout = 300s        # 5 minutes between checkpointsmax_wal_size = 4GB               # Allow more WAL before checkpointcheckpoint_completion_target = 0.9 # Spread checkpoint over more timeConnection ManagementPostgreSQL creates a separate process for each connection, which can become a bottleneck:  Keep connections under 1,000 if possible  Use a connection pooler like PgBouncer for high-connection applications  Consider the architecture: AppServer → Pooler → PostgreSQLQuery Optimization ToolsTo identify problematic queries:  pg_stat_statements: Collects statistics on all SQL executed  pg_stat_activity: Shows currently running queries  auto_explain: Logs execution plans for slow queries  PG logs: Rich source of information about query performanceLooking Ahead: PostgreSQL Version UpgradesSome of the PostgreSQL improvements includes:  PostgreSQL 16: Introduces pg_stat_io for better I/O monitoring  PostgreSQL 17: Improved SLRU handling and lock partitioning  PostgreSQL 18: Better handling of UUID indexes and continued performance improvementsPractical Takeaways for Rails Developers  Batch Your Operations: Instead of updating records in a loop, use update_all or background jobs  Monitor Bloat Regularly: Set up monitoring for tables with high write activity  Optimize for Reads: Most applications are read-heavy; optimize indexes accordingly  Consider Data Volume Growth: What works for thousands of rows might fail for millionsConclusionDatabase performance isn’t about reactive firefighting—it’s about proactive maintenance and smart design choices. Understanding concepts like bloat, proper indexing, and autovacuum tuning can prevent performance issues before they impact your users.Regular monitoring, combined with the strategic application of the techniques above, will help ensure your PostgreSQL database scales with your application needs. Remember that performance optimization is a continuous process, not a one-time fix."
  },
  
  {
    "title": "The Paradox of Expression: What AI Can Learn from Human Misreading",
    "url": "/posts/the-paradox-of-expression/",
    "categories": "Artificial Intelligence (AI), Emotions",
    "tags": "AI, emotions, mindfulness, deepthoughts",
    "date": "2025-03-19 06:01:10 +0545",
    





    
    "snippet": "There’s a curious phenomenon in human interactions. When people observe others in moments of deep focus, they often misinterpret serious expressions as unhappiness or discontent. The reality couldn...",
    "content": "There’s a curious phenomenon in human interactions. When people observe others in moments of deep focus, they often misinterpret serious expressions as unhappiness or discontent. The reality couldn’t be further from this perception - internally, these focused individuals are typically content and engaged, simply absorbed in thought.This disconnect between external appearance and internal state raises a fascinating question about artificial intelligence. If humans, with all their emotional intelligence and evolutionary adaptations for social reading, frequently misinterpret each other’s emotional states, what implications does this have for AI systems attempting to analyze human sentiment?We expect machines to accurately decode emotions when even we, as humans, misread the facial cues and body language of our fellow beings. This fundamental challenge highlights the complexity of emotional intelligence and the sophisticated nuance required to bridge the gap between appearance and reality.Perhaps the most profound insights about AI sentiment analysis come not from its successes, but from understanding these very human moments of misinterpretation that remind us how complex and internally rich our emotional lives truly are."
  },
  
  {
    "title": "The AI Revolution: How Business Software is Evolving Beyond SaaS",
    "url": "/posts/how-business-software-is-evolving-beyond-saas/",
    "categories": "Artificial Intelligence (AI), SaaS",
    "tags": "AI, SaaS",
    "date": "2025-02-18 08:10:12 +0545",
    





    
    "snippet": "The Shift from SaaS to AI-Driven Business SoftwareThe landscape of business software is undergoing a seismic transformation. In a recent interview on the BG2 Pod, Satya Nadella, CEO of Microsoft, s...",
    "content": "The Shift from SaaS to AI-Driven Business SoftwareThe landscape of business software is undergoing a seismic transformation. In a recent interview on the BG2 Pod, Satya Nadella, CEO of Microsoft, shared his insights on the future of business applications, suggesting that traditional Software as a Service (SaaS) is reaching its end. Instead, AI-powered platforms are emerging as the new foundation for business operations, automating workflows and seamlessly integrating disparate tools.The Rise and Peak of SaaSFor decades, SaaS applications have been the backbone of enterprise software, offering businesses cloud-based solutions for everything from customer relationship management to project collaboration. Companies like Salesforce pioneered the “no software” model, enabling users to access tools without the need for local installations. The subscription-based pricing structure of SaaS made it a scalable and cost-effective solution, fueling its rapid adoption.During the COVID-19 pandemic, SaaS tools became indispensable. Cloud-based applications facilitated remote work, and platforms like Microsoft Teams ensured uninterrupted communication and collaboration. As businesses adapted, the number of SaaS applications in use grew exponentially, with enterprises now managing an average of 130 different SaaS tools.The Growing Challenges of SaaSDespite its advantages, the SaaS model is not without flaws. Organizations struggle with fragmented data, inefficient workflows, and disjointed user experiences due to the sheer volume of applications in use. Managing multiple tools leads to silos, making seamless integration and automation increasingly difficult.This is where artificial intelligence is set to redefine the game.AI-Powered Business Software: The Next EvolutionNadella envisions an AI-centric future where software applications act less as standalone products and more as intelligent orchestrators of business operations. AI will handle complex tasks across multiple systems, eliminating redundant manual processes.Imagine a scenario where AI autonomously compiles sales data, generates reports, and creates presentations—tasks that currently require users to navigate multiple applications. AI-driven agents will seamlessly coordinate these workflows, shifting businesses from tool-centric models to outcome-based automation.Microsoft’s Role in the AI TransitionMicrosoft’s Magentic-One is a testament to this transformation. Built on the AutoGen framework, this multi-agent system enables AI to independently perform tasks such as data analysis, web interactions, and process automation. Unlike conventional software that executes predefined functions, AI-powered agents dynamically adapt to complex workflows, making business processes more efficient and intelligent.Key Impacts of AI on Business Software1. Decoupling Frontend and BackendAI will serve as an intelligent middleware, integrating with various back-end databases without being restricted to a single SaaS solution. This allows businesses to be more flexible in how they manage and update their data.2. Automated Workflow OrchestrationRather than relying on multiple SaaS tools, AI agents will streamline processes across different applications, reducing inefficiencies and improving productivity.3. Rethinking Business ApplicationsTraditional business software will no longer function in isolation. Instead, AI-driven platforms will become the nerve center of business operations, enabling decision-making and execution without excessive manual intervention.4. Intelligent Assistants in Daily OperationsMicrosoft 365 Copilot exemplifies this shift, acting as an AI-powered assistant that integrates across applications to automate complex workflows. From drafting legal documents to optimizing financial reports, AI tools will allow businesses to focus on strategic goals rather than mundane tasks.The Future of AI-Integrated Business PlatformsThe transition from SaaS to AI-driven platforms marks a fundamental shift in how businesses leverage technology. AI’s ability to integrate, automate, and optimize workflows will drive efficiency and innovation at an unprecedented scale. Organizations that embrace AI will not only streamline their operations but also gain a competitive edge in an increasingly digital world.While SaaS has served as a transformative force, AI is now shaping the future of business software. As Nadella’s vision suggests, AI is not just a feature—it is the new foundation of enterprise technology, set to revolutionize how businesses operate and grow in the years ahead."
  },
  
  {
    "title": "Query Optimization with PostgreSQL Execution Plan Visualizer",
    "url": "/posts/pg-query-plan-visualize/",
    "categories": "PostgreSQL, Performance",
    "tags": "postgresql, performance",
    "date": "2024-03-10 10:40:10 +0545",
    





    
    "snippet": "In the world of database management systems, PostgreSQL stands tall as one of the most powerful and versatile options available. However, even seasoned developers can find themselves facing challen...",
    "content": "In the world of database management systems, PostgreSQL stands tall as one of the most powerful and versatile options available. However, even seasoned developers can find themselves facing challenges when it comes to optimizing database queries for performance. Enter the PostgreSQL Execution Plan Visualizer—a tool that can turn the seemingly complex task of query optimization into a streamlined and intuitive process.Understanding the PostgreSQL Execution PlanBefore we delve into the visualizer itself, let’s take a moment to understand what the PostgreSQL execution plan is all about. Essentially, the execution plan outlines the steps that PostgreSQL will take to execute a given query. This includes details such as which indexes will be used, the order in which tables will be scanned, and any additional operations that may be necessary, such as sorting or joining data sets.The Challenge of Query OptimizationOptimizing database queries for performance can be a daunting task, particularly for queries that involve multiple tables, complex joins, or large data sets. Without a clear understanding of how PostgreSQL will execute a given query, developers may find themselves resorting to trial and error—a time-consuming and often frustrating process.Introducing the PostgreSQL Execution Plan VisualizerThis is where the PostgreSQL Execution Plan Visualizer comes into play. Developed with the needs of developers in mind, this powerful tool provides a visual representation of the execution plan for any given query. By simply inputting a query into the visualizer, developers can instantly see how PostgreSQL plans to execute it, allowing them to identify potential bottlenecks or inefficiencies at a glance.The significance of using EXPLAIN ANALYZE in SQL query is to interpret the resulting execution plans. This will provide valuable insights into query performance optimization.-- Example 1: Simple SELECT queryEXPLAIN ANALYZESELECT * FROM users WHERE age &gt; 25;-- Example 2: JOIN queryEXPLAIN ANALYZESELECT u.name, p.titleFROM users uJOIN posts p ON u.id = p.user_idWHERE u.age &gt; 25;-- Example 3: Aggregation queryEXPLAIN ANALYZESELECT department, AVG(salary) AS avg_salaryFROM employeesGROUP BY department;Key Features and BenefitsVisual Representation: The visualizer presents the execution plan in an intuitive and easy-to-understand format, making it accessible to developers of all skill levels.Identify Performance Issues: By visualizing the execution plan, developers can quickly identify potential performance issues such as sequential scans, inefficient joins, or missing indexes.Optimize with Confidence: Armed with insights from the visualizer, developers can make informed decisions about how to optimize their queries for maximum performance, saving time and frustration in the process.Getting Started with the PostgreSQL Execution Plan VisualizerReady to supercharge your query optimization efforts? Getting started with the PostgreSQL Execution Plan Visualizer is easy. Simply visit https://explain.depesz.com/ or https://explain.dalibo.com and input your query. Within seconds, you’ll have a clear visual representation of how PostgreSQL plans to execute it, allowing you to optimize with confidence.To optimize performance there is a term called N + 1 queries which needs to avoid. Thus, we might need to load associated records in advance and limit the number of SQL queries call made to the database using :includes also referred to as eager loading. Depending on the requirement of your query, :includes will use either the ActiveRecord method :preload or :eager_load.But there might be certain situation, where we blindly follow :includes, and, add unused association just a shake of solving N + 1, and later, it creates another performance issue something like PG::InternalError: ERROR: invalid memory alloc request size. That means we are preloading huge number of unwanted records in memory and the size of memory exhausted, issue is visualize in following screen. In this case, we need to optimize by removing unwanted association added inside includes that exhaust memory.ConclusionIn the fast-paced world of database management, optimizing query performance is essential. With the PostgreSQL Execution Plan Visualizer, developers can take the guesswork out of query optimization and streamline the process for maximum efficiency. Try it out for yourself and see the difference it can make in your development workflow."
  },
  
  {
    "title": "Tailwind CSS Transitions And Animations",
    "url": "/posts/tailwind-css-transitions-and-animations/",
    "categories": "Tailwind CSS, Transitions and Animations",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-10 10:30:18 +0545",
    





    
    "snippet": "Transitions and animations can add a polished and interactive feel to your web application. Tailwind CSS provides utility classes to easily apply transitions and animations to elements.TransitionsT...",
    "content": "Transitions and animations can add a polished and interactive feel to your web application. Tailwind CSS provides utility classes to easily apply transitions and animations to elements.TransitionsTransitions allow you to smoothly animate changes to CSS properties, such as changing colors, sizes, or positions. Tailwind CSS provides utility classes to specify transition properties and durations.Here’s how you can use transition classes in Tailwind CSS:&lt;button class=\"bg-blue-500 hover:bg-blue-700 transition-colors duration-300 text-white font-bold py-2 px-4 rounded\"&gt;    Hover over me&lt;/button&gt;In this example:  transition-colors class specifies that the transition will be applied to color changes.  duration-300 class specifies the duration of the transition in milliseconds (300ms in this case).When you hover over the button, you’ll notice that the color change is animated smoothly over the specified duration.AnimationsTailwind CSS also provides utility classes to apply pre-defined animations to elements. These animations are based on the popular animate.css library.Here’s how you can use animation classes in Tailwind CSS:&lt;div class=\"animate-bounce\"&gt;Bouncing element&lt;/div&gt;In this example, the animate-bounce class applies a bouncing animation to the element.Customizing Transitions and AnimationsTailwind CSS allows you to customize transitions and animations by defining custom CSS variables in your configuration file (tailwind.config.js) and then using those variables in your utility classes.For example, you can define custom transition durations:// tailwind.config.jsmodule.exports = {  theme: {    extend: {      transitionDuration: {        '2000': '2000ms',      }    }  }}Then, you can use the custom transition duration in your HTML:&lt;button class=\"bg-blue-500 hover:bg-blue-700 transition-colors duration-2000 text-white font-bold py-2 px-4 rounded\"&gt;    Hover over me&lt;/button&gt;This will apply a transition with a duration of 2000 milliseconds (2 seconds) when the button is hovered over."
  },
  
  {
    "title": "AI And Future",
    "url": "/posts/ai-and-future/",
    "categories": "Artificial Intelligence (AI)",
    "tags": "AI",
    "date": "2024-01-09 12:00:10 +0545",
    





    
    "snippet": "Artificial Intelligence (AI), a term first introduced by John McCarthy, is a discipline that focuses on the creation of intelligent machines that work and react like humans, and The term “Machine I...",
    "content": "Artificial Intelligence (AI), a term first introduced by John McCarthy, is a discipline that focuses on the creation of intelligent machines that work and react like humans, and The term “Machine Intelligence” was first coined by Alan Turing, who conducted substantial research in this field. The concept of “Artificial Brain” was born out of the desire to replicate human intelligence in machines. AI as a field of study was officially recognized in 1956 and has since experienced periods of hype and disillusionment. The AI renaissance of the late 2010s was driven by breakthroughs in deep learning and the transformer model, leading to a surge in interest and funding, primarily from companies and research institutions in the United States.AI technology has found its way into a wide range of sectors, governments, and scientific disciplines. Notable applications include sophisticated search engines such as Bing, recommendation algorithms used by YouTube, Amazon, Spotify, Alibaba, and Netflix, voice interactions like Google Assistant, Cortana, Siri, and Alexa, autonomous vehicles like Waymo, Tesla, creative tools like ChatGPT, AI Arit, DALL-E and AI music, and superior gameplay and analysis in strategic games like chess and Go.The concept of an AI takeover, where artificial intelligence surpasses human intelligence and gains control over the planet, is a recurring theme in science fiction. Prominent individuals like Stephen Hawking and Elon Musk have advocated for research to ensure that superintelligent machines remain under human control.Esteemed physicist Stephen Hawking, Microsoft co-founder Bill Gates, and SpaceX founder Elon Musk have all voiced serious concerns about the potential for AI to evolve to a point where it becomes uncontrollable by humans. Hawking even speculated that such a scenario could “signify the end of the human race”. He once stated, “Achieving AI would be the most significant event in human history. Regrettably, it might also be the last unless we learn how to avoid the risks.” Hawking predicted that AI could offer “immeasurable benefits and risks” in the coming decades, including “technology outsmarting financial markets, out-inventing human researchers, out-manipulating human leaders, and developing weapons we cannot even comprehend.” In 2015, Nick Bostrom, along with Stephen Hawking, Max Tegmark, Elon Musk, Lord Martin Rees, Jaan Tallinn, and numerous AI researchers, signed the Future of Life Institute’s open letter addressing the potential risks and benefits associated with artificial intelligence.A consensus among experts suggests that research on making AI systems robust and beneficial is both important and timely. They propose that there are tangible research directions that can be pursued today.According to OpenAI, AI is not on the verge of taking over the world. Tech experts argue that it is improbable that a single AI system could become so powerful as to dominate the world, and AI is not expected to replace humans in the near future.However, some speculate that AI could actually be the savior of the world. For example, Google Health predicts that by 2050, we could see personalized treatment plans, AI-assisted surgeries, and even predictive healthcare models.Others caution that the rapid advancement of AI could significantly impact labor markets globally. However, there are certain jobs that AI cannot replace. Some examples include:  Mental Health Professionals  Social Workers and Community Outreach Roles  Artists anad Musicians  Strategic Planners and Analysts  Research Scientists and Engineers  Judges  Leadership and Management RolesAI is not a threat to humanity, but a powerful tool that affects our lives in various ways. However, we should be careful about the alignment and ethics of AI systems, as they could surpass human intelligence and capabilities in many domains. AI also changes the nature of work and creates new challenges and opportunities for human workers. The future of AI is not a dystopia, but a dynamic and exciting world that requires adaptation and innovation."
  },
  
  {
    "title": "Tailwind CSS Flex and Grid",
    "url": "/posts/tailwind-css-flex-and-grid/",
    "categories": "Tailwind CSS, Flex and Grid",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-08 11:40:18 +0545",
    





    
    "snippet": "Let’s explore utility classes for working with Flex and Grid in Tailwind CSS. Flexbox and Grid can be utilized in Tailwind CSS to create responsive and flexible layouts with minimal effort.FlexThis...",
    "content": "Let’s explore utility classes for working with Flex and Grid in Tailwind CSS. Flexbox and Grid can be utilized in Tailwind CSS to create responsive and flexible layouts with minimal effort.FlexThis section demonstrates the use of Flexbox in Tailwind CSS. The flex class is used on the parent container to create a flex container. The justify-between class is applied to evenly distribute the child elements along the main axis (horizontally) with space between them. Each child element has flex-1 class to make them grow and fill the available space equally.&lt;div class=\"container mx-auto py-6\"&gt;    &lt;!-- Flexbox Example --&gt;    &lt;p class=\"text-lg text-left text-gray-600 font-bold\"&gt;Flexbox Example&lt;/p&gt;    &lt;div class=\"flex justify-between\"&gt;        &lt;div class=\"flex-1 bg-gray-200 p-4\"&gt;Item 1&lt;/div&gt;        &lt;div class=\"flex-1 bg-gray-300 p-4\"&gt;Item 2&lt;/div&gt;        &lt;div class=\"flex-1 bg-gray-400 p-4\"&gt;Item 3&lt;/div&gt;    &lt;/div&gt;&lt;/div&gt;GridThis section showcases the usage of Grid in Tailwind CSS. The grid class is used on the parent container to create a grid container. Different grid-cols-{number} classes are applied to define the number of columns at different screen sizes. The gap-4 class adds a gap of 1rem between grid items.&lt;div class=\"container mx-auto py-6\"&gt;    &lt;!-- Grid Example --&gt;    &lt;p class=\"text-lg text-left text-gray-600 font-bold\"&gt;Grid Example&lt;/p&gt;    &lt;div class=\"grid grid-cols-1 sm:grid-cols-2 md:grid-cols-3 lg:grid-cols-4 xl:grid-cols-5 gap-4\"&gt;        &lt;div class=\"bg-gray-200 p-4\"&gt;Item 1&lt;/div&gt;        &lt;div class=\"bg-gray-300 p-4\"&gt;Item 2&lt;/div&gt;        &lt;div class=\"bg-gray-400 p-4\"&gt;Item 3&lt;/div&gt;        &lt;div class=\"bg-gray-500 p-4\"&gt;Item 4&lt;/div&gt;        &lt;div class=\"bg-gray-600 p-4\"&gt;Item 5&lt;/div&gt;    &lt;/div&gt;&lt;/div&gt;"
  },
  
  {
    "title": "Tailwind CSS Form Components",
    "url": "/posts/tailwind-css-form-components/",
    "categories": "Tailwind CSS, Form Components",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-07 11:40:18 +0545",
    





    
    "snippet": "Let’s explore utility classes for working with forms and inputs in Tailwind CSS.Here are some examples of how you can style forms and inputs using Tailwind CSS utility classes.Form LayoutTailwind C...",
    "content": "Let’s explore utility classes for working with forms and inputs in Tailwind CSS.Here are some examples of how you can style forms and inputs using Tailwind CSS utility classes.Form LayoutTailwind CSS provides utility classes for creating form layouts easily. You can use flexbox utilities to align form elements horizontally or vertically.&lt;form class=\"flex flex-col\"&gt;    &lt;!-- Vertical form layout --&gt;    &lt;label for=\"username\"&gt;Username:&lt;/label&gt;    &lt;input type=\"text\" id=\"username\" name=\"username\" class=\"border p-2 mb-4\"&gt;    &lt;label for=\"password\"&gt;Password:&lt;/label&gt;    &lt;input type=\"password\" id=\"password\" name=\"password\" class=\"border p-2 mb-4\"&gt;    &lt;button type=\"submit\" class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded\"&gt;Submit&lt;/button&gt;&lt;/form&gt;Input StylesYou can style inputs using utility classes like border, p, rounded, etc. You can also use focus and hover states to add interactivity.&lt;input type=\"text\" class=\"border border-gray-300 p-2 rounded focus:outline-none focus:ring focus:border-blue-500\"&gt;Checkboxes and Radio ButtonsTailwind CSS provides styles for checkboxes and radio buttons as well.&lt;label class=\"inline-flex items-center\"&gt;    &lt;input type=\"checkbox\" class=\"form-checkbox text-blue-500\"&gt;    &lt;span class=\"ml-2\"&gt;Remember me&lt;/span&gt;&lt;/label&gt;&lt;label class=\"inline-flex items-center\"&gt;    &lt;input type=\"radio\" class=\"form-radio text-blue-500\" name=\"radio\"&gt;    &lt;span class=\"ml-2\"&gt;Option 1&lt;/span&gt;&lt;/label&gt;&lt;label class=\"inline-flex items-center\"&gt;    &lt;input type=\"radio\" class=\"form-radio text-blue-500\" name=\"radio\"&gt;    &lt;span class=\"ml-2\"&gt;Option 2&lt;/span&gt;&lt;/label&gt;Select MenusYou can style select menus using Tailwind CSS classes.&lt;select class=\"border p-2 rounded\"&gt;    &lt;option&gt;Option 1&lt;/option&gt;    &lt;option&gt;Option 2&lt;/option&gt;    &lt;option&gt;Option 3&lt;/option&gt;&lt;/select&gt;Validation StatesYou can use utility classes to style form elements based on their validation states.&lt;input type=\"text\" class=\"border p-2 rounded focus:outline-none focus:ring focus:border-blue-500\"&gt;&lt;span class=\"text-red-500\"&gt;Invalid input&lt;/span&gt;Tailwind classes used in Image components&lt;div class=\"container mx-auto py-6\"&gt;    &lt;!-- Responsive Image --&gt;    &lt;img src=\"https://source.unsplash.com/random/800x600\" alt=\"Random Image\" class=\"w-full\"&gt;    &lt;!-- Image with Specific Size --&gt;    &lt;img src=\"https://source.unsplash.com/random/400x300\" alt=\"Random Image\" class=\"w-64 h-48\"&gt;    &lt;!-- Rounded Corners --&gt;    &lt;img src=\"https://source.unsplash.com/random/800x600\" alt=\"Random Image\" class=\"rounded-lg\"&gt;    &lt;!-- Image with Shadow --&gt;    &lt;img src=\"https://source.unsplash.com/random/800x600\" alt=\"Random Image\" class=\"shadow-md\"&gt;    &lt;!-- Image with Aspect Ratio --&gt;    &lt;div class=\"aspect-w-16 aspect-h-9\"&gt;        &lt;img src=\"https://source.unsplash.com/random/800x600\" alt=\"Random Image\" class=\"object-cover\"&gt;    &lt;/div&gt;&lt;/div&gt;"
  },
  
  {
    "title": "Tailwind CSS Button Styling",
    "url": "/posts/tailwind-css-button-styling/",
    "categories": "Tailwind CSS, Button Styling",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-06 11:40:18 +0545",
    





    
    "snippet": "Let’s dive into buttons and interactive elements in Tailwind CSS.Button StylesTailwind CSS provides utility classes to easily style buttons. You can use classes like bg-blue-500, text-white, font-b...",
    "content": "Let’s dive into buttons and interactive elements in Tailwind CSS.Button StylesTailwind CSS provides utility classes to easily style buttons. You can use classes like bg-blue-500, text-white, font-bold, py-2, px-4, rounded, etc., to create visually appealing buttons.&lt;button class=\"bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded\"&gt;    Click me&lt;/button&gt;Button SizesYou can adjust the size of buttons using utility classes like text-xs, text-sm, text-lg, etc.&lt;button class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded text-xs\"&gt;    Small Button&lt;/button&gt;&lt;button class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded text-lg\"&gt;    Large Button&lt;/button&gt;Button StatesTailwind CSS allows you to define styles for different states of buttons, such as hover, focus, active, and disabled.&lt;button class=\"bg-blue-500 hover:bg-blue-700 text-white font-bold py-2 px-4 rounded focus:outline-none focus:ring-2 focus:ring-blue-500\"&gt;    Hoverable Button&lt;/button&gt;&lt;button class=\"bg-blue-500 active:bg-blue-700 text-white font-bold py-2 px-4 rounded\"&gt;    Clicked Button&lt;/button&gt;&lt;button class=\"bg-gray-300 text-gray-500 font-bold py-2 px-4 rounded cursor-not-allowed\" disabled&gt;    Disabled Button&lt;/button&gt;Button GroupsYou can group multiple buttons together using flexbox utilities to create button groups.&lt;div class=\"flex space-x-4\"&gt;    &lt;button class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded\"&gt;Button 1&lt;/button&gt;    &lt;button class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded\"&gt;Button 2&lt;/button&gt;    &lt;button class=\"bg-blue-500 text-white font-bold py-2 px-4 rounded\"&gt;Button 3&lt;/button&gt;&lt;/div&gt;These are some examples of how you can create and style buttons and interactive elements using Tailwind CSS utility classes."
  },
  
  {
    "title": "Tailwind CSS Styling Background and Borders",
    "url": "/posts/tailwind-css-background-and-borders/",
    "categories": "Tailwind CSS, Styling Background And Borders",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-05 11:40:18 +0545",
    





    
    "snippet": "Let’s explore some basic utility classes provided by Tailwind CSS for styling text elements.Background ColorYou can set the background color of an element using utility classes like bg-red-500, bg-...",
    "content": "Let’s explore some basic utility classes provided by Tailwind CSS for styling text elements.Background ColorYou can set the background color of an element using utility classes like bg-red-500, bg-blue-700, etc., where the number represents the shade of the color.&lt;div class=\"bg-red-500 p-4\"&gt;    &lt;p&gt;Red Background&lt;/p&gt;&lt;/div&gt;&lt;div class=\"bg-blue-700 p-4\"&gt;    &lt;p&gt;Blue Background&lt;/p&gt;&lt;/div&gt;Background OpacityYou can also adjust the opacity of the background color using utility classes like bg-opacity-25, bg-opacity-50, etc.&lt;div class=\"bg-red-500 bg-opacity-25 p-4\"&gt;    &lt;p&gt;Red Background with 25% Opacity&lt;/p&gt;&lt;/div&gt;&lt;div class=\"bg-blue-700 bg-opacity-50 p-4\"&gt;    &lt;p&gt;Blue Background with 50% Opacity&lt;/p&gt;&lt;/div&gt;BorderTailwind CSS provides utility classes for adding borders to elements. You can use classes like border, border-solid, border-dashed, border-dotted, etc.&lt;div class=\"border border-black p-4\"&gt;    &lt;p&gt;Border&lt;/p&gt;&lt;/div&gt;&lt;div class=\"border border-red-500 border-solid p-4\"&gt;    &lt;p&gt;Red Border&lt;/p&gt;&lt;/div&gt;Rounded CornersYou can apply rounded corners to elements using classes like rounded-sm, rounded-md, rounded-lg, etc.&lt;div class=\"bg-gray-300 rounded-lg p-4\"&gt;    &lt;p&gt;Rounded Corners&lt;/p&gt;&lt;/div&gt;Box ShadowYou can add box shadows to elements using utility classes like shadow-sm, shadow-md, shadow-lg, etc.&lt;div class=\"bg-gray-300 rounded-lg shadow-md p-4\"&gt;    &lt;p&gt;Box Shadow&lt;/p&gt;&lt;/div&gt;These are some of the basic utility classes for styling backgrounds, borders, and shadows in Tailwind CSS. Experiment with these classes to achieve the desired visual effects for your elements."
  },
  
  {
    "title": "Tailwind CSS Text Styles",
    "url": "/posts/tailwind-css-text-styles/",
    "categories": "Tailwind CSS, Text Styles",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-04 11:40:18 +0545",
    





    
    "snippet": "Let’s explore some basic utility classes provided by Tailwind CSS for styling text elements.Text Color:Tailwind CSS provides utility classes for changing the color of text. You can use classes like...",
    "content": "Let’s explore some basic utility classes provided by Tailwind CSS for styling text elements.Text Color:Tailwind CSS provides utility classes for changing the color of text. You can use classes like text-black, text-white, text-red-500, etc., where the number represents the shade of the color.&lt;p class=\"text-black\"&gt;Black Text&lt;/p&gt;&lt;p class=\"text-red-500\"&gt;Red Text&lt;/p&gt;&lt;p class=\"text-blue-700\"&gt;Blue Text&lt;/p&gt;Text SizeYou can adjust the size of text using classes like text-xs, text-sm, text-lg, etc., which stand for extra small, small, large, etc.&lt;p class=\"text-xs\"&gt;Extra Small Text&lt;/p&gt;&lt;p class=\"text-lg\"&gt;Large Text&lt;/p&gt;&lt;p class=\"text-3xl\"&gt;Extra Large Text&lt;/p&gt;Font WeightTo change the font weight, use classes like font-thin, font-normal, font-bold, etc.&lt;p class=\"font-thin\"&gt;Thin Font&lt;/p&gt;&lt;p class=\"font-bold\"&gt;Bold Font&lt;/p&gt;Text AlignmentYou can align text using classes like text-left, text-center, text-right, etc.&lt;p class=\"text-left\"&gt;Left Aligned Text&lt;/p&gt;&lt;p class=\"text-center\"&gt;Center Aligned Text&lt;/p&gt;&lt;p class=\"text-right\"&gt;Right Aligned Text&lt;/p&gt;Text DecorationTo add text decoration like underline, line-through, etc., use classes like underline, line-through.&lt;p class=\"underline\"&gt;Underlined Text&lt;/p&gt;&lt;p class=\"line-through\"&gt;Line-through Text&lt;/p&gt;These are just a few examples of the text utility classes provided by Tailwind CSS. You can combine these classes to achieve the desired styling for your text elements."
  },
  
  {
    "title": "Tailwind CSS Navigation",
    "url": "/posts/tailwind-css-navigation/",
    "categories": "Tailwind CSS, Navigation",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-03 11:40:18 +0545",
    





    
    "snippet": "Let’s explore utility classes for working with navigation menus and dropdowns in Tailwind CSS.Horizontal Navigation Menu:You can create a horizontal navigation menu using flexbox utilities to align...",
    "content": "Let’s explore utility classes for working with navigation menus and dropdowns in Tailwind CSS.Horizontal Navigation Menu:You can create a horizontal navigation menu using flexbox utilities to align menu items horizontally.&lt;nav class=\"flex\"&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Home&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;About&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Services&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Contact&lt;/a&gt;&lt;/nav&gt;Vertical Navigation Menu:Similarly, you can create a vertical navigation menu using flexbox utilities to align menu items vertically.&lt;nav class=\"flex flex-col\"&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Home&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;About&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Services&lt;/a&gt;    &lt;a href=\"#\" class=\"p-2\"&gt;Contact&lt;/a&gt;&lt;/nav&gt;Dropdown Menu:You can create dropdown menus using nested lists and CSS. Tailwind CSS provides utility classes for styling dropdowns and managing their visibility.&lt;nav&gt;    &lt;ul class=\"flex\"&gt;        &lt;li&gt;            &lt;a href=\"#\" class=\"p-2\"&gt;Home&lt;/a&gt;        &lt;/li&gt;        &lt;li&gt;            &lt;a href=\"#\" class=\"p-2\"&gt;About&lt;/a&gt;        &lt;/li&gt;        &lt;li&gt;            &lt;a href=\"#\" class=\"p-2\"&gt;Services&lt;/a&gt;            &lt;ul class=\"absolute hidden bg-gray-100 p-2\"&gt;                &lt;li&gt;&lt;a href=\"#\" class=\"block\"&gt;Service 1&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;&lt;a href=\"#\" class=\"block\"&gt;Service 2&lt;/a&gt;&lt;/li&gt;                &lt;li&gt;&lt;a href=\"#\" class=\"block\"&gt;Service 3&lt;/a&gt;&lt;/li&gt;            &lt;/ul&gt;        &lt;/li&gt;        &lt;li&gt;            &lt;a href=\"#\" class=\"p-2\"&gt;Contact&lt;/a&gt;        &lt;/li&gt;    &lt;/ul&gt;&lt;/nav&gt;Responsive Navigation:You can use responsive classes to create navigation menus that adapt to different screen sizes.&lt;nav class=\"flex flex-col lg:flex-row\"&gt;    &lt;!-- Menu items here --&gt;&lt;/nav&gt;These are some examples of how you can create navigation menus and dropdowns using Tailwind CSS utility classes. Experiment with these classes to create navigation structures that match your design requirements.Here is full example code:&lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;    &lt;head&gt;        &lt;meta charset=\"UTF-8\"&gt;        &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;        &lt;title&gt;Let's explore Tailwind CSS&lt;/title&gt;        &lt;link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\"&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;!-- Your content goes here --&gt;        &lt;div class=\"container mx-auto py-6\"&gt;            &lt;h1 class=\"text-3xl font-bold text-center text-gray-800\"&gt;Welcome! Let's walkthrough with Tailwind CSS&lt;/h1&gt;            &lt;p class=\"text-lg text-center text-gray-600\"&gt;Let's explore Tailwind!&lt;/p&gt;        &lt;/div&gt;        &lt;div class=\"container mx-auto py-6\"&gt;            &lt;p class=\"text-lg text-left text-gray-600 font-bold\"&gt;Horizontal Nav&lt;/p&gt;            &lt;nav&gt;                &lt;ul class=\"flex\"&gt;                    &lt;li&gt;                        &lt;a href=\"#\" class=\"p-2\"&gt;Home&lt;/a&gt;                    &lt;/li&gt;                    &lt;li&gt;                        &lt;a href=\"#\" class=\"p-2\"&gt;About&lt;/a&gt;                    &lt;/li&gt;                    &lt;li class=\"relative\"&gt;                        &lt;a href=\"#\" class=\"p-2\"&gt;Services&lt;/a&gt;                        &lt;ul class=\"dropdown-menu hidden bg-gray-100 absolute top-full left-0\"&gt;                            &lt;li&gt;&lt;a href=\"#\" class=\"block p-2\"&gt;Service 1&lt;/a&gt;&lt;/li&gt;                            &lt;li&gt;&lt;a href=\"#\" class=\"block p-2\"&gt;Service 2&lt;/a&gt;&lt;/li&gt;                            &lt;li&gt;&lt;a href=\"#\" class=\"block p-2\"&gt;Service 3&lt;/a&gt;&lt;/li&gt;                        &lt;/ul&gt;                    &lt;/li&gt;                    &lt;li&gt;                        &lt;a href=\"#\" class=\"p-2\"&gt;Contact&lt;/a&gt;                    &lt;/li&gt;                &lt;/ul&gt;            &lt;/nav&gt;            &lt;p class=\"text-lg text-left text-gray-600 font-bold\"&gt;Vertical Nav&lt;/p&gt;            &lt;nav class=\"flex flex-col\"&gt;                &lt;a href=\"#\" class=\"p-2\"&gt;Home&lt;/a&gt;                &lt;a href=\"#\" class=\"p-2\"&gt;About&lt;/a&gt;                &lt;a href=\"#\" class=\"p-2\"&gt;Services&lt;/a&gt;                &lt;a href=\"#\" class=\"p-2\"&gt;Contact&lt;/a&gt;            &lt;/nav&gt;        &lt;/div&gt;        &lt;!-- JavaScript --&gt;        &lt;script type=\"text/javascript\"&gt;            document.addEventListener(\"DOMContentLoaded\", function() {                // Get all dropdown toggle buttons                var dropdownToggleButtons = document.querySelectorAll('.relative &gt; a');                                // Add event listeners to toggle dropdown visibility                dropdownToggleButtons.forEach(function(button) {                    button.addEventListener('click', function(event) {                        event.preventDefault(); // Prevent default anchor behavior                        var dropdownMenu = this.parentElement.querySelector('.dropdown-menu');                        dropdownMenu.classList.toggle('hidden');                    });                });                // Close dropdowns when clicking outside                document.addEventListener('click', function(event) {                    if (!event.target.closest('.relative')) {                        var dropdownMenus = document.querySelectorAll('.dropdown-menu');                        dropdownMenus.forEach(function(menu) {                            menu.classList.add('hidden');                        });                    }                });            });        &lt;/script&gt;    &lt;/body&gt;&lt;/html&gt;"
  },
  
  {
    "title": "Tailwind CSS Setup",
    "url": "/posts/tailwind-css-setup/",
    "categories": "Tailwind CSS, Setup",
    "tags": "css, tailwind_css, UI",
    "date": "2024-01-02 11:40:18 +0545",
    





    
    "snippet": "To begin using Tailwind CSS in your project, you first need to set up a project environment. Here’s how you can set up Tailwind CSS in a simple HTML project:      Create an HTML file: Let’s create ...",
    "content": "To begin using Tailwind CSS in your project, you first need to set up a project environment. Here’s how you can set up Tailwind CSS in a simple HTML project:      Create an HTML file: Let’s create an index.html file where we’ll write our HTML code.        Include Tailwind CSS: You can include Tailwind CSS in your HTML file by either linking to a CDN or by installing it via npm and including the compiled CSS file. For simplicity, we’ll use the CDN approach here.  &lt;!DOCTYPE html&gt;&lt;html lang=\"en\"&gt;&lt;head&gt;    &lt;meta charset=\"UTF-8\"&gt;    &lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;    &lt;title&gt;My Tailwind CSS Project&lt;/title&gt;    &lt;link href=\"https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css\" rel=\"stylesheet\"&gt;&lt;/head&gt;&lt;body&gt;&lt;!-- Your content goes here --&gt;&lt;/body&gt;&lt;/html&gt;Start using Tailwind classes: Now you can start using Tailwind CSS classes directly in your HTML elements to style them. For example:&lt;div class=\"container mx-auto py-6\"&gt;    &lt;h1 class=\"text-3xl font-bold text-center text-gray-800\"&gt;Welcome to My Tailwind CSS Project&lt;/h1&gt;    &lt;p class=\"text-lg text-center text-gray-600\"&gt;Let's explore Tailwind together!&lt;/p&gt;&lt;/div&gt;In this example, container, mx-auto, py-6, text-3xl, font-bold, text-center, text-gray-800, text-lg, text-gray-600 are Tailwind CSS utility classes that style the HTML elements.That’s it! You’ve successfully set up Tailwind CSS in your project. Now you can continue exploring and using Tailwind’s utility classes to style your HTML elements further."
  },
  
  {
    "title": "Forecasting the Future: How Automated ML and Tools Like BQML Can Save You Time and Effort",
    "url": "/posts/automated-demand-forecasting-and-auto-ml/",
    "categories": "Artificial Intelligence (AI), Machine Learning, BigQuery",
    "tags": "AI, ML, BigQuery",
    "date": "2023-04-10 09:40:18 +0545",
    





    
    "snippet": "In the age of big data, predicting future trends is no longer a crystal ball endeavor. Machine learning (ML) has emerged as a powerful tool for businesses to unlock insights from their data and gai...",
    "content": "In the age of big data, predicting future trends is no longer a crystal ball endeavor. Machine learning (ML) has emerged as a powerful tool for businesses to unlock insights from their data and gain a competitive edge. But what happens when building and deploying ML models feels like navigating a complex labyrinth? That’s where Automated ML (AutoML) and platforms like BigQuery ML (BQML) come in, ready to simplify the process and make forecasting accessible to everyone.AutoML vs. Manual ML: A Tale of Two ApproachesImagine two scenarios:Google Cloud AI Platform:You’re a data analyst with a solid understanding of ML concepts. You meticulously craft features, experiment with different algorithms, and fine-tune your model like a seasoned sculptor. This approach offers fine-grained control, but it requires significant time and expertise.Google’s AutoML boasts an intuitive interface that allows users to create and deploy custom machine learning models with minimal manual effort. For instance, AutoML Vision enables the creation of image recognition models by simply uploading labeled images, showcasing Google’s commitment to making AI accessible.Microsoft Azure Machine Learning:You’re a business owner with a mountain of data but limited ML knowledge. You simply upload your data, define the desired outcome (e.g., demand forecasting), and let Azure’s AutoML work its magic. It automatically explores various algorithms and configurations, presenting you with the best performing model – no coding required.On the Azure front, Microsoft’s Automated ML simplifies the end-to-end machine learning process, assisting in data preparation, algorithm selection, and hyperparameter tuning. Businesses can leverage Azure’s offering for tasks ranging from forecasting to classification, making machine learning accessible across various domains.Both approaches have their merits. Google’s platform caters to data scientists who want to tinker and optimize, while Azure’s AutoML empowers non-technical users to leverage the power of ML for real-world applications.BQML: Forecasting on AutopilotGoogle Cloud’s BigQuery ML is a shining example of AutoML in action. Built directly into BigQuery, the data warehouse beloved by many businesses, BQML lets you train and deploy machine learning models directly using SQL queries. This means no more jumping between platforms or wrestling with complex coding – just write familiar queries and let BQML handle the heavy lifting.Demand Forecasting without Breaking a SweatNow, let’s talk about the future you’re eager to predict: demand. Building a custom forecasting model from scratch can be daunting. But here’s the good news: you don’t have to!Several tools and services can help you forecast demand without the coding crunch:BigQuery Forecasting:BQML offers pre-built forecasting templates specifically designed for time series data. Simply choose the template that matches your needs, specify the target variable (e.g., sales), and BQML automatically generates a forecast.Google Cloud Vertex AI:This unified AI platform offers various forecasting options, including pre-trained models and AutoML capabilities. You can leverage pre-trained models like Prophet for quick predictions or tap into AutoML Forecasting for more customized solutions.Microsoft Azure Forecasting Services:Similar to Google’s offerings, Azure provides pre-built models and AutoML features for demand forecasting. You can choose from various algorithms and let Azure find the best fit for your data.The Takeaway:The future of forecasting is automated, accessible, and ready to empower businesses of all sizes. By embracing AutoML tools like BigQuery ML and utilizing pre-built models, you can unlock valuable insights from your data and confidently navigate the ever-changing landscape of demand. So, ditch the crystal ball, embrace the power of ML, and let the future unfold before your eyes."
  },
  
  {
    "title": "Unlocking Creativity: Prompt Engineering in Generative AI",
    "url": "/posts/generative-ai-with-prompt-engineering/",
    "categories": "Artificial Intelligence (AI), Prompt Engineering, ChatGPT",
    "tags": "chatGPT, prompt_engineering, AI, LLM",
    "date": "2023-04-05 10:40:18 +0545",
    





    
    "snippet": "In the ever-evolving landscape of Generative AI, one concept has taken center stage, becoming a catalyst for creativity and innovation—Prompt Engineering. This technique, akin to providing a well-c...",
    "content": "In the ever-evolving landscape of Generative AI, one concept has taken center stage, becoming a catalyst for creativity and innovation—Prompt Engineering. This technique, akin to providing a well-crafted instruction to a creative assistant, has proven to be a powerful tool in harnessing the potential of models like ChatGPT (Utilizes OpenAI’s Transformer architecture), Bard (built on Google’s PaLM 2 architecture) and other Large Language Models (LLMs). Let’s embark on a journey to understand the nuances and impact of prompt engineering in the realm of Generative AI.What is Generative AI?Generative AI is an AI that can create  Text  Images  Audio  Videos  3D modelsGiving Generative AIs input is known as AI Prompt Writing or AI Prompt Engineering.What is Prompt Engineering?Prompt engineering is the strategic construction of prompts or input instructions given to generative models, particularly Large Language Models (LLMs), to elicit desired outputs. In the context of Generative AI, such as ChatGPT, the quality and specificity of prompts play a pivotal role in influencing the model’s responses. It’s not just about input; it’s about crafting a precise and context-rich instruction that guides the model to generate relevant and coherent content.The Art and Science of Crafting Effective Prompts  Clarity and Specificity:The more specific and clear the prompt, the better the model, including Large Language Models (LLMs), understands the desired outcome. Explore techniques for refining prompts to achieve optimal results.  Contextual Cues:Leveraging contextual cues in prompts enhances the model’s, including Large Language Models (LLMs), ability to grasp nuances and maintain coherence in responses. Dive into examples that showcase the impact of context in prompt engineering.  Creative Exploration:Beyond specificity, prompt engineering opens doors to creative exploration, especially with the capabilities of Large Language Models (LLMs). Learn how to balance guidance with openness, allowing the model to generate imaginative and unexpected content.Applications Across Industries  Content Generation:Discover how prompt engineering, coupled with Large Language Models (LLMs), is revolutionizing content creation by enabling writers, marketers, and creatives to collaborate with AI models to generate compelling and customized content.  Problem Solving:Explore real-world applications of prompt engineering, driven by Large Language Models (LLMs), in problem-solving scenarios, where the technique aids in generating solutions, ideas, and insights.Challenges and ConsiderationsWhile prompt engineering, especially with Large Language Models (LLMs), offers immense potential, it comes with its set of challenges. From over-specification to balancing creativity, understanding the pitfalls is crucial for effective utilization.The Future of Prompt EngineeringAs Generative AI, driven by Large Language Models (LLMs), continues to advance, the role of prompt engineering is expected to grow. Explore emerging trends and potential developments that could shape the future of this innovative approach.The Role of Large Language Models (LLMs) in Prompt Engineering:Central to the success of prompt engineering is the advent of Large Language Models (LLMs), such as GPT-3 and similar advanced systems. These models, with their vast neural networks and extensive training data, possess an unparalleled ability to understand and generate human-like text. Leveraging the prowess of Large Language Models (LLMs) in prompt engineering amplifies the impact of well-crafted instructions, as these models can comprehend intricate contextual cues and produce responses with a level of coherence and creativity that was once unprecedented. As we explore the intricacies of prompt engineering, recognizing the symbiotic relationship between effective prompts and the capabilities of Large Language Models (LLMs) becomes paramount for unlocking new dimensions of generative AI creativity and utility.Let us look at some examples of prompts specifically on ChatGPT.Conclusion: Embracing the Power of PrecisionIn the dynamic landscape of Generative AI, prompt engineering, particularly with Large Language Models (LLMs), stands out as a key driver of precision and creativity. As we unlock new possibilities in human-AI collaboration, mastering the art of crafting effective prompts becomes essential. Whether you’re a developer, content creator, or industry professional, understanding and harnessing the power of prompt engineering is a journey worth taking."
  },
  
  {
    "title": "What is Artificial Intelligence",
    "url": "/posts/what-is-artificial-intelligence/",
    "categories": "Artificial Intelligence (AI)",
    "tags": "AI",
    "date": "2023-01-04 12:00:10 +0545",
    





    
    "snippet": "AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solv...",
    "content": "AI, or Artificial Intelligence, refers to the development of computer systems that can perform tasks that typically require human intelligence. These tasks include learning, reasoning, problem-solving, perception, natural language understanding, and even the ability to interact with the environment. The goal of AI is to create machines that can mimic cognitive functions associated with human minds.There are two main types of AI:Narrow or Weak AI:This type of AI is designed and trained for a particular task. It can excel at that specific task but lacks the broad cognitive abilities of a human. Examples include virtual personal assistants like Siri or Alexa.General or Strong AI:This hypothetical form of AI would have the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.General AI, also known as Strong AI or Artificial General Intelligence (AGI), refers to a type of artificial intelligence that has the ability to understand, learn, adapt, and apply knowledge across a wide range of tasks at a level equal to or beyond a human being.Strong AI doesn’t just mimic or simulate human intelligence. It’s supposed to understand, reason, plan, learn, communicate in natural language, and integrate all these skills towards common goals, just like a human would. It should be able to transfer knowledge from one domain to another, handling tasks that it was not specifically programmed for.It could understand and apply its knowledge to various situations, learn new skills on its own, and possibly even experience consciousness. AGI could bring about revolutionary changes in numerous fields, from healthcare and science to economics and social sciences. However, ethical considerations and potential risks also need careful examination.As of now, Strong AI remains largely theoretical, with no practical examples in use today. Most AI in use today is considered Weak AI (also known as Narrow AI), which is designed to perform a narrow task (e.g., only facial recognition or only internet searches or only driving a car).AI systems can be categorized into several subfields, including:Machine Learning (ML):A subset of AI that focuses on the development of algorithms that enable computers to learn from and make predictions or decisions based on data.Natural Language Processing (NLP):A field that involves the interaction between computers and human languages, enabling machines to understand, interpret, and generate human-like text.Computer Vision:The ability of computers to interpret visual information, enabling them to “see” and make decisions based on visual input.Robotics:The integration of AI and machines to create intelligent robots capable of performing tasks in the physical world.AI has a wide range of applications, including healthcare diagnostics, autonomous vehicles, recommendation systems, language translation, and many more. As technology continues to advance, AI is expected to play an increasingly significant role in various aspects of our lives. However, ethical considerations, transparency, and responsible development are crucial aspects to address as AI technologies progress."
  },
  
  {
    "title": "Ruby find_all vs. select",
    "url": "/posts/ruby-find-all-vs-select/",
    "categories": "Ruby, select vs. find_all",
    "tags": "ruby, find_all, select",
    "date": "2022-08-15 10:40:18 +0545",
    





    
    "snippet": "About find_all vs. select Ruby methodfind_all or select returns an array which contains all elements of enum for which the given block returns a true value, and, if no block is given, an Enumerator...",
    "content": "About find_all vs. select Ruby methodfind_all or select returns an array which contains all elements of enum for which the given block returns a true value, and, if no block is given, an Enumerator is returned.Here are some examples:arr = 1..8 h = {a: 1, b: 2, c: 3, d: 4, e: 5, f: 6, g: 7, h: 8}arr.select{|x| x.even?} # =&gt; [2, 4, 6, 8]a.find_all{|x| x.even?} # =&gt; [2, 4, 6, 8]On hash select returns hash and find_all returns array.h.select { |k, v| v.even? }   # =&gt; {:b=&gt;2, :d=&gt;4, :f=&gt;6, :h=&gt;8}h.find_all { |k, v| v.even? } # =&gt; [[:b, 2], [:d, 4], [:f, 6], [:h, 8]]"
  },
  
  {
    "title": "Webpacker in Rails6",
    "url": "/posts/rails6-webpacker/",
    "categories": "Ruby on Rails, Webpacker",
    "tags": "ruby on rails, webpacker",
    "date": "2020-06-05 01:45:18 +0545",
    





    
    "snippet": "Webpacker  Webpacker is the JavaScript compiler which compiles the JavaScript code.  Prior to Rails6, JS code were inside app/assets/javascripts  In Rails6, no app/assets/javascripts and have new d...",
    "content": "Webpacker  Webpacker is the JavaScript compiler which compiles the JavaScript code.  Prior to Rails6, JS code were inside app/assets/javascripts  In Rails6, no app/assets/javascripts and have new dir app/javascript to load all the js files which has channels &amp; packs and all Javascript components like Turbolinks, ActiveStorage, Rails-UJS, ActionCable support Webpacker.  Other dir channels generated by Rails ActionCable component  Another dir packs which has app/packs/javascriptsapp/javascript/packs/application.jsrequire(\"@rails/ujs\").start()require(\"turbolinks\").start()require(\"@rails/activestorage\").start()require(\"channels\")  any js files inside packs/ will autocompiled by WebpackAbout Pack  Webpack uses webpacker gem which wraps webpack and used to compile the javascript code which are on the packs directory. This gem creates the application pack as application.js inside app/javascript/packs which is similar to assets pipeline (app/assets/javascripts/application.js) and application pack is the entry point for all the JavaScript code that contains Action Cable, Active Storage, Turbolinks Rails components.  gem webpacker is automatically placed inside the Gemfile of Rails6 application, and yarn is used to install npm packages when creating new Rails 6 application.Gem also generates settings:config/webpacker.yml  As like assets pipeline, JavaScript code using Webpacker and webpack automatically compiles in development mode when running rails server.  Gem also generates the file bin/webpack-dev-server which is used to live reloading the development phase. Inorder to see the live reloading in development mode we need to run the webpack-dev-server with command ./bin/webpack-dev-server separately.  However, in production mode, rake assets:precompile also override the rake webpacker:compile which will compile the assets pipleline and compile the files to be compiled by webpack which updates the package.json.Way to use the JavaScript code in the appwe can use the helper method javascript_pack_tag to include the webpacker packs file which is similar to asset pipeline javascript_link_tag and works on both development and production mode.# app/views/layouts/application.html.erb&lt;%= javascript_pack_tag 'application', 'data-turbolinks-track': 'reload' %&gt;  Prior to Rails 6 application do not install gem webpacker by default, include it in Gemfile, and run the command rake webpacker:install"
  },
  
  {
    "title": "Elastic Search with Chewy",
    "url": "/posts/elastic-search-with-chewy/",
    "categories": "Ruby on Rails, Elastic search Chewy",
    "tags": "elastic_search, chewy",
    "date": "2019-10-25 07:01:18 +0545",
    





    
    "snippet": "Chewy is one of the elastic search Ruby client.Chewy usages:      Multi-model indicesYou can define several types for index one per indexed model.        Every index is observable by all the relate...",
    "content": "Chewy is one of the elastic search Ruby client.Chewy usages:      Multi-model indicesYou can define several types for index one per indexed model.        Every index is observable by all the related models.Most of the indexed models are related to other and it is necessary to denormalize this related data and put at the same object. Chewy is useful for example when we need index for an array of tags together with an article since it specify updated index for every model seperately so corressponding articles will be reindexed on any tag update.        Bulk import everywhereIt supports bulk elastic search api for full reindex and index updates.        Powerful querying DSLChewy has an ActiveRecord style query DSL.        Support for ActiveRecord, Mongoid and Sequel.  Installation Steps:gem 'chewy'bundle installor gem install chewyClient settings:Chewy.settings hash and chewy.yml are two ways in which Chewy client can be configured.Run the command rails g chewy:install to generate the file or create one manually.# config/chewy.yml# separate environment configstest:  host: 'localhost:9250'  prefix: 'test'development:  host: 'localhost:9200'config/initializers/chewy.rbChewy.settings = {host: 'localhost:9250'} # do not use environmentsAws Elastic SearchConfiguration for using AWS’s elastic search using an IAM user policy, sign your requests for the es:* action by injecting the headers passing a proc to transport_options.Chewy.settings = {    host: 'http://my-es-instance-on-aws.us-east-1.es.amazonaws.com:80',    transport_options: {      headers: { content_type: 'application/json' },      proc: -&gt; (f) do          f.request :aws_signers_v4,                    service_name: 'es',                    region: 'us-east-1',                    credentials: Aws::Credentials.new(                      ENV['AWS_ACCESS_KEY'],                      ENV['AWS_SECRET_ACCESS_KEY'])      end    }  }Type accessFollowing API is used to access index-defined typesUsersIndex::UserUsersIndex.type_hash['user']UsersIndex.type('user')UsersIndex.type('foo')UsersIndex.types # [UserIndex::User]UsersIndex.type_names # [\"user\"] Index ManipulationUsersIndex.delete # destroy existed indexUsersIndex.delete!UsersIndex.create # create indexUsersIndex.create!UsersIndex.purgeUsersIndex.purge! # deletes then creates indexUsersIndex::User.import # import with 0 arguments process all the data specified in type definitionUsersIndex::User.import User.where('rating &gt; 100') # or import specified users scopeUsersIndex::User.import User.where('rating &gt; 100').to_a # or import specified users arrayUsersIndex::User.import [1, 2, 42] # pass even ids for import, it will be handled in the most effective wayUsersIndex::User.import user: User.where('rating &gt; 100')  # if update fields are specified - it will update their values only with the `update` bulk action.UsersIndex.reset! # purges index and imports default data for all typesPractical on Ruby on Rails applicationapp/chewy/user_index.rbclass UserIndex &lt; Chewy::Index    settings analysis: {      analyzer: {        email: {          tokenizer: 'keyword',          filter: ['lowercase']        }      }    }      define_type User do      field :name, {type: 'text'}      field :email, analyzer: 'email'      field :phone, {type: 'text'}    end  endapp/controllers/users_controller.rbclass UsersController &lt; ApplicationController    def search      @users = UsersIndex.query(query_string: { fields: [:name, :email, :phone], query: search_params[:query], default_operator: 'and' })        render json: @users.to_json, status: :ok    end      private      def search_params      params.permit(:query, :page, :per)    end  endapp/models/user.rbclass User &lt; ApplicationRecord    update_index('user') { self }    enum status: { unconfirmed: 0, confirmed: 1 }endroutes.rbresources :users do    get :search, on: :collectionendIf you access the url http://localhost:3000/users/search?query=test1Following results are seen on the browser0\tid\t\"18\"name\t\"test1\"status\t\"unconfirmed\"email\t\"test1@example.com\"phone\t\"090111111\"_score\t0.5389965_explanation\tnull1\tid\t\"3\"name\t\"test1\"status\t\"unconfirmed\"email\t\"test1@example.com\"phone\t\"090111112\"_score\t0.5389965_explanation\tnull2\tid\t\"45\"name\t\"test1\"email\t\"test1@example.com\"phone\t\"090111111\"_score\t0.5389965_explanation\tnulland if we inspect the result of @users object on controller.first on console, we will see@_data=  {\"_index\"=&gt;\"user\",   \"_type\"=&gt;\"user\",   \"_id\"=&gt;\"18\",   \"_score\"=&gt;0.5389965,   \"_source\"=&gt;{\"name\"=&gt;\"test1\", \"status\"=&gt;\"unconfirmed\", \"email\"=&gt;\"test1@example.com\", \"phone\"=&gt;\"090111111\"}}, @attributes=  {\"id\"=&gt;\"18\",   \"name\"=&gt;\"test1\",   \"status\"=&gt;\"unconfirmed\",   \"email\"=&gt;\"test1@example.com\",   \"phone\"=&gt;\"090111111\",   \"_score\"=&gt;0.5389965,   \"_explanation\"=&gt;nil}We can refactor the searching as:Create a dir called as app/searches/user_search.rb# user_search.rb# frozen_string_literal: trueclass UserSearch  include ActiveModel::Model  DEFAULT_PER_PAGE = 10  DEFAULT_PAGE = 0  attr_accessor :query, :page, :per  def search    [query_string].compact.reduce(&amp;:merge).page(page_num).per(per_page)  end  def query_string    index.query(query_string: { fields: [:name, :email, :phone], query: query, default_operator: 'and' }) if query.present?  end  private  def index    UsersIndex  end  def page_num    page || DEFAULT_PAGE  end  def per_page    per || DEFAULT_PER_PAGE  endendNow call the UserSearch class and implement it inside the UsersControllerclass UsersController &lt; ApplicationController  def search    user_search = UserSearch.new(search_params)    @users = user_search.search    render json: @users, status: :ok  end  private  def search_params    params.permit(:query, :page, :per)  endendNow modify the search action as:class UsersController &lt; ApplicationController  def search    user_search = UserSearch.new(search_params)    @users = user_search.search  end  private  def search_params    params.permit(:query, :page, :per)  endendsearch.html.erb&lt;% if @users.any? %&gt;    &lt;table border=\"1\"&gt;        &lt;tr&gt;            &lt;th&gt;Id&lt;/th&gt;            &lt;th&gt;Name&lt;/th&gt;            &lt;th&gt;Phone&lt;/th&gt;            &lt;th&gt;Email&lt;/th&gt;            &lt;th&gt;Status&lt;/th&gt;        &lt;/tr&gt;        &lt;% @users.each do |user| %&gt;        &lt;% res = user.attributes %&gt;        &lt;tr&gt;            &lt;td&gt;&lt;%= res[\"id\"] %&gt;&lt;/td&gt;            &lt;td&gt;&lt;%= res[\"name\"] %&gt;&lt;/td&gt;            &lt;td&gt;&lt;%= res[\"phone\"] %&gt;&lt;/td&gt;            &lt;td&gt;&lt;%= res[\"email\"] %&gt;&lt;/td&gt;            &lt;td&gt;&lt;%= res[\"status\"] %&gt;&lt;/td&gt;        &lt;/tr&gt;        &lt;% end %&gt;    &lt;/table&gt;&lt;% else %&gt;    &lt;p&gt;No users found.&lt;/p&gt;&lt;% end %&gt;"
  },
  
  {
    "title": "Elastic Search",
    "url": "/posts/elastic-search/",
    "categories": "Ruby on Rails, Elastic search",
    "tags": "elastic_search",
    "date": "2019-10-18 08:01:18 +0545",
    





    
    "snippet": "Introduction to Elastic SearchElastic Search is a full-text search engine which can be used as NoSQL database and can be used as analytics engine.It is schema-less, easy to scale, near real-time an...",
    "content": "Introduction to Elastic SearchElastic Search is a full-text search engine which can be used as NoSQL database and can be used as analytics engine.It is schema-less, easy to scale, near real-time and provides a restful interface for different operations.Elastic search is used as primary backend of your web application which can be added to an existing system which run through existing data source. Elastic search can be used to monitor and analysis of the existing application without affecting the behaviour of the current application.Various UseCases of Elastic Search are:  Web Application Search Solution  Data Visualization and Analytics  Log Management  Online Database Storage  Monitoring System  Autocomplete and instant searchElastic Search has following components:      ClusterA cluster is a collection of one or more server/nodes which holds your entire data together and provides indexing and search capabilities across all nodes. “elasticsearch” is unique default cluster.        NodeA node is a single server which is a part of a cluster which stores your data, participate’s in cluster’s indexing and search capabilities. A node is unique name and by default Universally Unique Identifier (UUID).        IndexAn index is a data structure which is a collection of documents having similar characteristics which is used to improve query execution time. Index are created for table primary keys, foreign keys, unique numbers, etc so that query executed 250 times faster than query without indexing.        TypeType is used to store various types of data in the same index, in order to keep the total number of indices. The _type field is added to every document which is used for filtering the data when searching with a specific type.        Document  Document is the row of record for the table or collection which is a single piece of information and it can be indexed.        Shard    The data is shared or divided into two or multiple nodes/machines/servers in the cluster when data grows really fast and run out of space which is called as shard.  Sharding is useful as it horizontally scale your content volume and it allows to distribute and parallelize operations across shards which increases the performance.  Installation of ElasticSearch on Ubuntu 18.04  You need to use sudo login      Install the deb package from the official Elasticsearch repository    Install apt-transport-https package that necessary to access a repository over HTTPs.$ sudo apt update$ sudo apt install apt-transport-https  Install OpenJDK 8sudo apt install openjdk-8-jdk  Verify the java installation$ java -version# this gives output as belowopenjdk version \"1.8.0_222\"OpenJDK Runtime Environment (build 1.8.0_222-8u222-b10-1ubuntu1~18.04.1-b10)OpenJDK 64-Bit Server VM (build 25.222-b10, mixed mode)  Import repository’s GPG using the following wget commandwget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -      The output of above command should be OK which means the key is imported successfully and packages from this repository will be considered trusted.        Add the Elasticsearch repository to the system by issuing:  sudo sh -c 'echo \"deb https://artifacts.elastic.co/packages/7.x/apt stable main\" &gt; /etc/apt/sources.list.d/elastic-7.x.list'  Now update apt package list and install Elasticsearch engine by following commands:sudo apt updatesudo apt install elasticsearch  Start the Elasticsearch processessudo systemctl enable elasticsearch.servicesudo systemctl start elasticsearch.service  Verify Elasticsearch is running by commandcurl -X GET \"localhost:9200/\"# it's output is as shown below{  \"name\" : \"crystal-Aspire-E5-575G\",  \"cluster_name\" : \"elasticsearch\",  \"cluster_uuid\" : \"9IFdxeCaRZmSj5c33WxEEg\",  \"version\" : {    \"number\" : \"7.4.0\",    \"build_flavor\" : \"default\",    \"build_type\" : \"deb\",    \"build_hash\" : \"22e1767283e61a198cb4db791ea66e3f11ab9910\",    \"build_date\" : \"2019-09-27T08:36:48.569419Z\",    \"build_snapshot\" : false,    \"lucene_version\" : \"8.2.0\",    \"minimum_wire_compatibility_version\" : \"6.8.0\",    \"minimum_index_compatibility_version\" : \"6.0.0-beta1\"  },  \"tagline\" : \"You Know, for Search\"}```Ruby* Log messages can be seen by using following commandsudo journalctl -u elasticsearch* Some useful directoriesdata storage - /var/lib/elasticsearchconfiguration file - /etc/elasticsearchJava startup options configuration - /etc/default/elasticsearch#### Remote Access SetupElasticsearch by default listen to localhost, so if database also in the same host, it is single node cluster and default configuration works.Anyone can access Elasticsearch by HTTP API as Elasticsearch lacks authentication. So you need to give access to Elasticsearch server to only trusted client which is done by configuring firewall (check firewall tool UFW on ubuntu) and give access to port 9200.First add a rule which allow incoming SSH```Rubysudo ufw allow 22Allow access from trusted clientsudo ufw allow from 192.168.1.65 to any port 9200# replace remote ip address with 192.168.1.65Enable UFWsudo ufw enableCheck firewall statussudo ufw status// o/p looks like thisStatus: activeTo                         Action      From--                         ------      ----22                         ALLOW       Anywhere9200                       ALLOW       192.168.1.6522 (v6)                    ALLOW       Anywhere (v6)  Next edit Elasticsearch configuration allow it to listen to external connectionssudo vim /etc/elasticsearch/elasticsearch.ymlUncomment line having network.host, change the value to 0.0.0.0. To make Elasticsearch listen on specified interface among multiple network interfaces on your machine you can specify interface IP address.Restart the Elasticsearch service and now connection to Elasticsearch server from remote is ready.sudo systemctl restart elasticsearch"
  },
  
  {
    "title": "Elastic Search with Searchkick",
    "url": "/posts/elastic-search-with-searchkick/",
    "categories": "Ruby on Rails, Elastic search Searchkick",
    "tags": "elastic_search, searchkick, ruby on rails",
    "date": "2019-10-18 08:01:18 +0545",
    





    
    "snippet": "What is Searchkick?Searchkick is a smart and intillegent search engine Rubygems that creates quicker search results based on user search activity.Before using Searchkick make sure Elasticsearch is ...",
    "content": "What is Searchkick?Searchkick is a smart and intillegent search engine Rubygems that creates quicker search results based on user search activity.Before using Searchkick make sure Elasticsearch is installed on your system.Steps to use Searchkick  Create a Rails applicationrails new institutions -d postgres  Generate the scaffold for Studentrails g scaffold Student name:string roll:integer grade:string fee:decimal      Run rake db:create rake db:migrate        Configure the routes  root \"students#index\"resources :students  Add following gem into Gemfilegem 'searchkick'Here is the Guide for Elasticsearch 6 or 7.  In each models you need to add keyword searchkick to make searchkick work as shown belowclass Student &lt; ApplicationRecord\tsearchkickend  Now add data to search index by using following code and you need to run this command everytime as model changesStudent.reindexThere are many ways search options based on necessity::word # default:word_start:word_middle:word_end:text_start:text_middle:text_endHere is an example of using :word_start for partial match criteriaclass Student &lt; ApplicationRecord  searchkick word_start: [:name, :role, :grade, :fee]  def search_data    {      name: name,      role: role,      grade: grade,      fee: fee    }  endendSearch EverythingStudent.search \"*\"Partial MatchesStudent.search \"Shiv Raj Badu\" # Shiv AND Raj AND BaduBook.search \"Shiv Raj Badu\", operator: \"or\"Exact MatchesStudent.search params[:search], fields:[{fee: :exact}, :name]Phrase MatchesStudent.search \"another name\", match: :phraseModel associationsStudent.search \"shiv raj\", track: {user_id: current_user.id}Autocomplete and Instant Searchclass Student &lt; ApplicationRecord  searchkick match: :word_start, searchable: [:name, :roll]endLanguage supported based on listsearchkick word_start: [:title, :author, :genre], language: \"turkish\"class StudentsController &lt; ApplicationController  before_action :set_student, only: [:show, :edit, :update]  def searchcriteria    render json: Student.search(params[:query], {      fields: [\"name\", \"roll\", \"grade\", \"fee\"],      limit: 10,      load: false,      misspellings: {below: 5}    }).map(&amp;:title)  endendImplement JavaScript searchbox as below&lt;input type=\"text\" id=\"query\" name=\"query\" /&gt;  $(\"#query\").typeahead({    name: \"student\",    remote: \"/students/search_criteria?query=%QUERY\"  });Suggestions generatorclass Student &lt; ApplicationRecord  searchkick suggest: [:name, :roll, :fee, :grade]endHighlight search result fields like this:class Student &lt; ApplicationRecord  searchkick highlight: [:name]endCreate custom and advanced mapping like this:class Student &lt; ApplicationRecord  searchkick mappings: {    student: {      properties: {        name: {type: \"string\", analyzer: \"keyword\"},        grade: {type: \"string\", analyzer: \"keyword\"}      }    }  }end"
  },
  
  {
    "title": "JQuery",
    "url": "/posts/jquery/",
    "categories": "JQuery, Cheatsheet",
    "tags": "jquery",
    "date": "2019-10-13 08:01:18 +0545",
    





    
    "snippet": "IntroductionjQuery is a JavaScript library created by John Resig in 2006 with an objectives Write less, do more. The main features of jQuery are event handling, Ajax interactions, animations, trave...",
    "content": "IntroductionjQuery is a JavaScript library created by John Resig in 2006 with an objectives Write less, do more. The main features of jQuery are event handling, Ajax interactions, animations, traversing, DOM manipulation, Cross Browser Support,  etc.jQuery Syntax$(selector).action()Here $ sign is used to define jQuery, a selector is used to find HTML DOM elements and an action is jQuery function to be performed on the HTML elements.`$(this).hide()` // To hide current element use`$(\"h1\").hide()` // To hide all h1 elements use`$(\"#myDiv\").hide()` // To hide element with id \"myDiv\" use`$(\".myDiv\").hide()` // To hide element with class \"myDiv\" usejQuery event should call inside $(document).ready() function in order to work on HTML page and script inside this function will exectuted before loaded the page contents and when DOM is loaded.$(document).ready(function() {   // The script written here will execute when DOM is ready});Example Usage&lt;html&gt;    &lt;head&gt;        &lt;title&gt;jQuery Example&lt;/title&gt;        &lt;script type = \"text/javascript\"          src = \"https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\"&gt;        &lt;/script&gt;        &lt;script type=\"text/javascript\"&gt;            $(document).ready(function() {                $(\"#header\").click(function() {                    alert(\"jQuery triggered\");                });            });        &lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;      &lt;div id=\"header\"&gt;        Click Me      &lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;Steps to install jQueryjQuery can be installed in two ways:  Download jQuery library in your system and include in HTML code.          Download latest version of jQuery into your project directory from https://jquery.com/download/        Here is the example code  &lt;html&gt;    &lt;head&gt;        &lt;title&gt;jQuery&lt;/title&gt;        &lt;script type=\"text/javascript\" src=\"/js/jquery-3.4.1.min.js\"&gt;&lt;/script&gt;        &lt;script type=\"text/javascript\"&gt;            $(document).ready(function() {                console.log(\"Hello jQuery\");                document.write(\"Hello jQuery\");            });        &lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p&gt;jQuery&lt;/p&gt;    &lt;/body&gt;&lt;/html&gt;  Include jQuery library in your HTML code from Content Delivery Network (CDN).Here is the example code&lt;html&gt;    &lt;head&gt;        &lt;title&gt;jQuery&lt;/title&gt;        &lt;script type = \"text/javascript\"          src = \"https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\"&gt;      &lt;/script&gt;        &lt;script type=\"text/javascript\"&gt;            $(document).ready(function() {                console.log(\"Hello jQuery\");                document.write(\"Hello jQuery\");            });        &lt;/script&gt;    &lt;/head&gt;    &lt;body&gt;        &lt;p&gt;jQuery&lt;/p&gt;    &lt;/body&gt;&lt;/html&gt;jQuery SelectorsjQuery Selectors are used to select HTML elements based on their name, id, classes, types, attributes, values to perform various tasks.jQuery selectors start with the factory function which starts with dollar sign followed by parentheses $() which is synonym of jQuery(). Sometime conflict occurs with $ sign when using other JavaScript library in that case use jQuery() instead of $() to avoid conflict.Here are few lists of jQuery selectors            Selector                  $(“*”) # Selects all element              $(this) # Selects current element              $(“#myDivId”) # Selects an element with an Id=”myDivId”              $(“#myDivClass”)  # Selects an element with class=”myDivId”              $(“p”) # Selects all paragraph element matched by &lt;p&gt;              $(“p &gt; *”) # Selects all elements that are children of paragraph element              $(“p.myDivClass”) # Select all paragraph elements having class=”myDivClass”              $(“p:first”) # Select first paragraph element              $(“ul li:first”) # Select first &lt;li&gt; element of the first &lt;ul&gt;              $(“ul li:first-child”) # Select first &lt;li&gt; element of every &lt;ul&gt;              $(“#myDiv p”) # Select &lt;p&gt; elements under div element with id=”myDiv”              $(“li &gt; ul”) # Select all &lt;ul&gt; elements which are children of &lt;li&gt; elements              $(“p a.myClass”) # Select all links or  elements having class=”myClass” which is children of &lt;p&gt; elements              $(“a#myId.myClass”) # Selects links with an id=”myId” and class=”myClass”              $(“li:not(.myclass”) # Selects all elements matched by &lt;li&gt; which do not have class=”myclass”              $(“[href]”) # Selects all elements with an href attribute              $(“a[target=’_blank’]”) # Selects all  elements with a target attribute value equal to “_blank”              $(“a[target!=’_blank’]”) # Selects all  elements with a target attribute value NOT equal to “_blank”              $(“:button”) # Selects all button elements and input elements of type=”button”              $(“tr:even”) # Selects all even &lt;tr&gt; elements              $(“tr:odd”) # Selects all odd &lt;tr&gt; elements              $(“strong + em”) # Selects all elements matched by  which is followed by               $(“p ~ ul”) # Selects all elements matched by &lt;ul&gt; which is followed by &lt;p&gt;              $(“code, em, strong”) # Selects all elements which is matched by  or  or               $(“p strong, .myDivClass”) # Select all strong elements which are followed by &lt;p&gt; and having class=”myDivClass”              $(“:empty”) # Select all elements having no children              $(“p:empty”) # Select all paragraph elements having no children              $(“div[p]”) # Select all elements having &lt;div&gt; which contains &lt;p&gt;              $(“p[.myDivClass]”) # Select all paragraph elements having class=”myDivClass”              $(“:radio”) # Selects all the radio buttons in the form              $(“:checked”) # Selects all the checkbox in the form              $(“:input”) # Selects input element of the form like: input, textarea, select, button, etc              $(“:text”) # Selects all input text elements              $(“p:lt(3)”) # Selects all first three elements              $(“p:gt(2)”) # Selects all paragraph elements excluding first two or after third one              $(“div/p”) # Selects all paragraph elements which are under div tag              $(“div//code”) # Selects all  elements which are descendants of &lt;div&gt;              $(“//p//a”) # Selects all links that are descendants of paragraph              $(“:parent”) # Selects all elements which are parent of another element, including text              $(“li:contains(second)”) # Selects all elements matched by &lt;li&gt; that contain the text second      jQuery attributesSome jQuery methods used to get or set the value of attributes, property, html, etc are postulated below:attr() - get or set the specified attribute of the target element.prop() - get or set the specified property of the target element.html() - get or set the html content to the specified target element.val() - get or set the value of the specified target element.text() - get or set the text for the specified target element.jQuery attributes are almost uses with properties like className, id, tagName, href, title, src, rel. HTML tags are the h1, h2, p, img, div, head, body, bold(b), anchor(a), form, hr, br, input, li, ul, ol, link, option, strong, small, table, td, tr, th, u, tt, center, etc.jQuery attr() methodsattr() is used to get value of attributes and attr(name, value) is used to set the attribute with new value which will apply to all elements.&lt;!DOCTYPE html&gt;&lt;html&gt;    &lt;head&gt;        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js\"&gt;        &lt;/script&gt;        &lt;script&gt;            $(document).ready(function () {                     alert($(\"div\").attr(\"style\")); // Select div tag and get the value of style attributes                $(\"p\").attr(\"class\", \"greenColorStyle\"); // all paragraph tag's class will update to greenColorStyle                var title = $(\"p\").attr(\"title\"); // get value of paragraph's attributes title                $(\"#progTitle\").text(title); // inspect div id progTitle and replace text defined by variable title            });        &lt;/script&gt;    &lt;style type=\"text/css\"&gt;        .greenColorStyle {        color: green;        }    &lt;/style&gt;    &lt;/head&gt;    &lt;body&gt;    &lt;div style=\"color: red; \"&gt;This is a paragraph under div&lt;/div&gt;    &lt;p title=\"Programming Tutorials\"&gt;Get value of title attributes and replace div having id progTitle&lt;/p&gt;    &lt;div&gt;        &lt;p&gt;After page loads all paragraph should be in green color&lt;/p&gt;        &lt;p&gt;Another text&lt;/p&gt;    &lt;/div&gt;    &lt;div id=\"progTitle\"&gt;&lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;addClass(className)addClass(className) is used to apply defined styles to selected elements.&lt;html&gt;    &lt;head&gt;        &lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js\"&gt;        &lt;/script&gt;        &lt;script&gt;            $(document).ready(function () {                 $(\"#myDiv\").addClass(\"greenColorStyle\");            });        &lt;/script&gt;    &lt;style type=\"text/css\"&gt;        .greenColorStyle {        color: green;        }    &lt;/style&gt;    &lt;/head&gt;    &lt;body&gt;    &lt;div id=\"myDiv\"&gt;This is a paragraph under div&lt;/div&gt;    &lt;/body&gt;&lt;/html&gt;Some more attr methods are:  removeAttr()  Removes an attribute of matched elements  removeClass(class)  Removes the class of matched elements  hasClass(class)   check if class present then returns true  toggleClass(class)  add the class if absent and remove the class if present  html()   Get the html contents of an element  html(val)   set the html content with value of element  text()   get the combined text contents of elements  text(val)   set the text content of element  val()   get input value of first matched element  val(val)   set the value attribute of every matched elementjQuery TraversingjQuery traversing is used to find html elements based on their relation to other elements.jQuery traversing means to move over elements to find a particular or entire element.Here are some list of jQuery traversing methods:  add()Collects one or more matched elements which are passed inside the method to create an object which can be manipulated at the same time.&lt;html&gt;&lt;head&gt;\t&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js\"&gt;\t&lt;/script&gt;\t&lt;script&gt;\t\t$(document).ready(function () {     \t\t  var jqObj = $('div').add('p').add('span').css( \"background\", \"yellow\" );\t\t  jqObj.addClass('greenColorStyle');\t\t});\t&lt;/script&gt;   &lt;style type=\"text/css\"&gt;  \t.greenColorStyle {      color: green;    }   &lt;/style&gt;&lt;/head&gt;&lt;body&gt;  &lt;div id=\"myDiv\" style=\"text-align: center; padding: 20px\"&gt;This is a paragraph under div&lt;/div&gt;  &lt;p&gt;Some more text&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;  addBack()adds the previous set of elements with current set and maintains them in a stack which can be manipulated.# add together paragraph tags with myDiv div tag and implement bgGreen color to both$(\"div.myDiv\").find(\"p\").addBack().addClass(\"bgGreen\");  not()get all elements which do not match specific selectorTree TraversingDescendants  childrenGet all the child elements of the selected element.$(selector).children();  find()Get all the specified child elements of each selected element.Ancestors  closestfind the first ancestor of the selected selector.$(selector).closest(selector or element);  parentget the parent element of specified selector$(selector).parent();  parentsget all ancestor elements of specified selector$(selector).parents();  parentsUntil()get all ancestor elements between specified selector and arguments.$(selector).parentsUntil(selector, element)  offsetParent()get the first parent element of specified selector.Siblings  siblingsget all siblings of the specified selector.$(selector).siblings()  next()get next sibling element of the specified selector.$(selector).next()  nextAll()get all next sibling elements of the specified selector.$(selector).nextAll()  nextUntil()get all next sibling between specified selector and arguments.$(selector).nextUntil()  prev()get the previous sibling element of the specified selector.$(selector).prev(selector)  prevAll()get all the previous siblings of specified selector.$(selector).prevAll(selector, filter_element)  prevUntil()get all the previous siblings between specified selector and arguments.$(selector).prevUntil(selector, filter_element)filtering  first()get the first element of the specified selector  last()get the last element of the specified selector  eq()get the element with specified index number of the specified selector.$(selector).eq(index)  filter()remove or get the element which are matched with specified selector$(selector).is(selector or function or elements)  has()get all elements which have one or more elements within and are matched with specified selector.$(selector).has(selector)  is()it checks if one of the specified selector is matched with arguments..is(selector or function or elements)  map()traverse arrays and objects and results a new arrayjQuery.map(array, function(val, i) {    // do stuff here});  slice(start, end)get the subset of specified selector based on it’s argument index or start and stop value.$(selector).slice(start, end)jQuery EffectsjQuery helps us to add various types of visual effects on our webpage.jQuery effects are listed below:            Display Effects      Sliding Effects      Fading Effects      Other Effects                  show()      slideUp()      fadeIn()      delay()              hide()      slideDown()      fadeOut()      animate()              toggle()      slideToggle()      fadeToggle()      fadeTo()                            fadeTo()             Here are the description of some jQuery effect methods:show()Display the selected elementsUsage:$(\"#btn-hide\").click(function() {    $(\"p\").hide()});$(\"#btn-show\").click(function() {    $(\"p\").show()});hide()Hide the selected elementstoggle()This function is used to show to hide (toggle) the matched elementsUsage:    $(btn-toggle).click(function() {        $(\"p\").toggle()    })slideUp()This function is used to display the slideup effects which first hide the element and then show the element with sliding effects once it is completed execute the callback function, thus this function is used to slide up an element.Syntax$(selector).slideUp(speed,callback);  speed - valid speed values are slow, normal, fast  callback - which is optional parameter and this function is called once the animation is completed.Usage$(\"#btnUp\").click(function(){    $(\".target\").slideUp('slow', function(){         $(\"#div-id\").text('Slide Up Effect');    });});slideDown()This function is used to slide down the element.Syntax$(selector).slideDown(speed,callback);Usage$(\"#btnDown\").click(function(){    $(\".target\").slideDown( 'slow', function(){         $(\"#div2-id\").text('Slide Down Effect');    });});slideToggle()slideToggle() method is used to toggle between slideUp() and slideDown() which means if the element if slide down this function helps them slide up and vice versa.Syntax:$(selector).slideToggle(speed,callback);fadeIn()This function is used to show fades in effect from the hidden element to make it visible.$(selector).fadeIn(speed,easing,callback)  speed - valid speed values are slow, normal, fast  callback - which is optional parameter and this function is called once the animation is completed.  easing - This is optional. This feature shows the effect in various speed in various dimension. Various options are:          swing - moves faster in the middle dimension and slower at the start or end.      linear - moves in constant speed.      fadeOut()This function is used to show fades out effect from the visible element to make it hidden.$(selector).fadeOut(speed,easing,callback)fadeToggle()This method is used to toggle between fadeIn() and fadeOut() which will fades in the element if it is fades out and vice versa.Syntax$(selector).fadeOut(speed,easing,callback)fadeTo()This method is used to show the fading effect of an element partially in or out to make it transparent.Syntax$(selector).fadeTo(speed,opacity,callback);      speed - valid speed values are slow, fast, or milliseconds        opacity - The values of opacity are between 0 and 10 - fully transparent (hidden)1- fully opaque (shown)        callback - which is optional parameter and this function is called once the animation is completed.  delay()The delay() is an inbuilt method in jQuery which is used to set a timer to delay the execution of the next item in the queue.Syntax$(selector).delay(speed,queueName)The paremeters used are explained here:  speed - The values are milliseconds, “slow”, “fast”  queueName - The default value if “fx” and you can set the queue name here and is the optional parameter.Usage$(\"#mybtn\").click(function() {    $(\"#slow-delay-div\").delay(\"slow\").fadeIn();    $(\"#fast-delay-div\").delay(\"fast\").fadeIn();    $(\"#ms-div\").delay(1000).fadeIn();});animate()This method is used to create custom animations which gives special effects using style properties of the element.Specify the selector to get the reference of an element and call animate() to apply animation, this animate() function takes json object for style properties, speed of animation, and other options.Syntax$(selector).animate({ params },speed, callback);$(selector).animate({ stylePropertyName: 'value', duration, easing, callback }, { options })Usage&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;\t&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js\"&gt;    &lt;/script&gt;    &lt;script&gt;        $(document).ready(function () {\t\t\t$('#animateEx').animate({                        height: '50px',                        width: '50px'                    });\t\t\t});    &lt;/script&gt;\t&lt;style&gt;        .green {            background-color: green;            height: 350px;            width: 350px;        }    &lt;/style&gt;&lt;/head&gt;&lt;body&gt;\t&lt;h1&gt;Here is an example of jQuery animate()&lt;/h1&gt;\t&lt;div id=\"animateEx\" class=\"green\"&gt;\t&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;"
  },
  
  {
    "title": "Metaprogramming in Ruby",
    "url": "/posts/metaprogramming-in-ruby/",
    "categories": "Ruby, Metaprogramming",
    "tags": "ruby, metaprogramming",
    "date": "2019-09-02 09:23:58 +0545",
    





    
    "snippet": "Metaprogramming in RubyMetaprogramming is a programming concept which treats other programs as their data and computer programs are written in such a way that is executed at runtime instead of comp...",
    "content": "Metaprogramming in RubyMetaprogramming is a programming concept which treats other programs as their data and computer programs are written in such a way that is executed at runtime instead of compile time.It helps in reducing development time by minimizing the lines of codes, also efficiently manages the programs with new solutions without recompilation.Metaprogramming includes :  compile code generation or Runtime code generation (or both)  Aspect-Oriented Thinking or Aspect Oriented Programming  DRY ThinkingIt is advisable to mastering Metaprogramming before using it as it is very powerful.ExamplesMake a getter methods which return instance variables if they are not nil, if they are nil set it to some default value and return it.class Foo    def foo        @foo ||= 0    endendSuppose if you have multiple such getters then instead of writing them all we can use metaprogramming like this:class Foo    {foo: 0, bar: '', baz: []}.each do |method_name, default_value|        define_method method_name do            instance_var = :\"@#{method_name}\"            instance_variable_get(instance_var) ||            instance_variable_set(instance_var, default_value)        end    endendmodule GettersWithDefault    def getters_with_default(spec)        spec.each do |method_name, default_value|            define_method method_name do                instance_var = :\"@#{method_name}\"                instance_variable_get(instance_var) ||                instance_variable_set(instance_var, default_val)            end        end    endendclass Foo    include GettersWithDefault    getters_with_default foo: 0, bar: '', baz: {}endA common example of Metaprogrammingclass Post    def initialize(status)        @status = status    end    %w(published unpublished draft).each do |possible_status|        define_method(\"#{possible_status}?\") do            @status == possible_status        end    endendIt seems like it saves time, because we don’t need to write separate methods for published?, unpublished?, and draft?. However, there are tradeoffs. For example, metaprogramming like this makes searching for method definitions later difficult. It’s certainly faster to type, but it’s harder to find and read later. Since we spend so much more time reading code than writing it, code that’s easier to write than read is actually a bad tradeoff.Domain Specific LanguageA Domain Specific Language or DSL is a custom language that solves a specific domain or problem. In Ruby’s case, a DSL is written in Ruby but looks different from standard Ruby code. Some examples of Ruby DSL are Rails Routes, Rspec, Factory Girl, etc. Factory Girl has cmplicated internal code but it allows you to write expressive, declarative code.FactoryGirl.define do    sequence :github_username do |n|        \"github_#{n}\"    end    factory :user do        description \"Learn all about Git\"        github_username        trait :admin do            admin true        end    endendDSL Structuredescribe \"User\" do  # ...endFactoryGirl.define do    # ...endRails.application.routes.draw do  # ...endBe carefulIf there is a less-complicated solution to a problem, reach for that first. Metaprogramming is usually not a good first solution to a problem, and DSLs require a good understanding of the problem’s domain. Once you do understand the problem well, though, DSLs are a great option.Talk about MonkeypatchingCode Discoverty and ReadabilityOne problem with metaprogramming solutions are their obstruction of code discovery. When entering a new project or simply trying to re-familiarize onself with existing one, tracing code executiion in a text editor can be quite difficult if method definitions do not exist.For example we can assume that a User class exists with a set of metaprogrammed methods:class User    [        :password,        :email,        :first_name,        :last_name    ].each do |attribute|        define_method(:\"has_#{attribute}?\") do            self.send(attribute).nil?        end    endendAlthough a little contrived, this code is a list of simple convenience methods on a User class. This solution is easily extended to include additional attributes without a full method definition per attribute.However, these methods can not be found using grep, silver searcher, or other “find all” tools. Since the method has_password? is never explicitly defined in the code, it is not discoverable.A Work Around:To combat this issue, some developers choose to write a comment listing the defined method names above metaprogramming block. This simple solution can greatly help the readability of the code.class User    # has_password?, has_email?, has_first_name?, has_last_name? method definitions    [        :password,        :email,        :first_name,        :last_name    ].each do |attribute|        define_method(:\"has_#{attribute}?\") do            self.send(attribute).nil?        end    endendPerformanceDepending on the amount of times a piece of code is executed, performance considerations can be extremely important. “Hot code” is a term used to describe code that is called frequently during an application’s request cycle. Since not all code is created equally, understanding the performance implications of different metaprogramming approaches is imperative when writing or modifying hot code."
  },
  
  {
    "title": "How to create Rails application with MongoDB?",
    "url": "/posts/Rails-app-with-MongoDB/",
    "categories": "MongoDB, Ruby on Rails",
    "tags": "mongodb, rails_setup",
    "date": "2019-08-30 09:23:58 +0545",
    





    
    "snippet": "Setup Rails 5 with mongoid gemAt first we need to install MongoDB in our system, the steps to install MongoDB is descripted in my previous blog. Confirm mongoDB is installed by browsing http://loca...",
    "content": "Setup Rails 5 with mongoid gemAt first we need to install MongoDB in our system, the steps to install MongoDB is descripted in my previous blog. Confirm mongoDB is installed by browsing http://localhost:27017/ and you will get following message:It looks like you are trying to access MongoDB over HTTP on the native driver port.Create a Rails application with the keyword “–skip-active-record” so that ActiveRecord is not included in the generated app.rails new myapp --skip-active-recordEdit your GemfileRemove this Gem if existsgem 'sqlite3'And add following Gems:gem 'mongoid', '~&gt; 6.2.0'gem 'bson_ext'Generate configuration file to support MongoDB which generates config/mongoid.ymlrails g mongoid:configThere is a file called /config/mongoid.yml’ which contains database configuration and it is required.The Rails generators for ‘model’, ‘scaffold’ etc have been overridden by Mongoid. Any models, scaffolds etc that you create will create classes that include the Mongoid::Document module instead of inheriting from ApplicationRecord in the models folder.Associationrails generate scaffold article title:stringrails generate scaffold comment body:string article_id:string # Here article_id required when implementing has_many association but not required in case of embedds many and even records are not saved inside Comment document which is included inside Article document.Association embeds_many with embedded_inclass Article  include Mongoid::Document  field :title, type: String  embeds_many :commentsendclass Comment  include Mongoid::Document  field :title, type: String  field :article_id, type: String  embedded_in :articleendarticle = Article.new(title: \"Embeds Many association on MongoDB\")article.comments.build(title: 'Embeds Many association will connect child records inside parent record')# if you check the mongo console `db.articles.find()` you will notice Comments records are also included inside the Article record and no seperate comment document is created inside comment collection.{ \"_id\" : ObjectId(\"5d6f75b567ef9b0d9e8373b4\"), \"title\" : \"Embeds Many association on MongoDB\", \"comments\" : [ { \"_id\" : ObjectId(\"5d6f75bd67ef9b0d9e8373b5\") }, { \"_id\" : ObjectId(\"5d6f75ef67ef9b0d9e8373b6\"), \"title\" : \"Embeds Many association will connect child records inside parent record\" } ] }Association has_many with belongs_toclass Article  include Mongoid::Document  field :title, type: String  has_many :commentsendclass Comment  include Mongoid::Document  field :title, type: String  field :article_id, type: String  belongs_to :articleend`db.articles.find()`{ \"_id\" : ObjectId(\"5d6f6a8067ef9b06fed2c32e\"), \"title\" : \"first article\" }`db.comments.find()`{ \"_id\" : ObjectId(\"5d6f6e2567ef9b0c888373b1\"), \"title\" : \"comment title\", \"article_id\" : ObjectId(\"5d6f6a8067ef9b06fed2c32e\") }In this senario, the records are saved inside two independed mongoDB collection Articles and Comments. And inside Comment Document we will have article_id whose value is the corressponding Article Id. But in embeds_many technique the child records do not save inside the Comment collection but inside Article Collection included inside Article document."
  },
  
  {
    "title": "Notes on MongoDB",
    "url": "/posts/MongoDB-notes/",
    "categories": "MongoDB, Data Operation",
    "tags": "mongodb, data_operation",
    "date": "2019-08-30 09:23:58 +0545",
    





    
    "snippet": "Introduction to MongoDBMongoDB is a open source document-oriented NoSQL database used for high volume data storage. If database is not already created switch to the database and insert data into it...",
    "content": "Introduction to MongoDBMongoDB is a open source document-oriented NoSQL database used for high volume data storage. If database is not already created switch to the database and insert data into it, this way database is created.Each record in a MongoDB collection is a document. MongoDB collections are like table and documents are like rows of the relational databases.Create Databaseuse NewDatabase # switched to db NewDatabasedb.products.insert({name: 'product', price: 20}) # Create a collection name as products with new document as a recordView database and collections&gt; show dbs;adminconfiglocalNewDatabase&gt; use NewDatabaseswitched to db NewDatabase&gt; show collectionsproductsDelete Database&gt; db.dropDatabase(){ \"dropped\" : \"NewDatabase\", \"ok\" : 1 }Crud operationsInsert a Single Documentdb.collection.insertOne() inserts a single document into a collection.MongoDB adds the _id field with an ObjectId value to the new document.db.customers.insertOne(    {        profile_name: 'customer name',        email: 'email@example.com',        age: 32,        tags: [\"regular\"],        full_name: { first_name: 'firstname', mid_name: 'midname', last_name: 'last_name' }    })when you run the above command, you will get following output{\t\"acknowledged\" : true,\t\"insertedId\" : ObjectId(\"5d6ccbfda82b6d69714cebeb\")}Insert multiple documentsdb.collection.insertMany()db.customers.insertMany(    [        {          profile_name: 'customer name 2',          email: 'email10@example.com',          age: 22,          tags: [\"regular\"],          full_name: { first_name: 'firstname2', mid_name: 'midname2', last_name: 'last_name2' }        },        {          profile_name: 'customer name 3',          email: 'email11@example.com',          age: 22,          tags: [\"regular\"],          full_name: { first_name: 'firstname3', mid_name: 'midname3', last_name: 'last_name3' }        },        {          profile_name: 'customer name 4',          email: 'email12@example.com',          age: 22,          tags: [\"regular\"],          full_name: { first_name: 'firstname4', mid_name: 'midname4', last_name: 'last_name4' }        }    ])=&gt; output when executing above query{\t\"acknowledged\" : true,\t\"insertedIds\" : [\t\tObjectId(\"5d6ccdf9a82b6d69714cebec\"),\t\tObjectId(\"5d6ccdf9a82b6d69714cebed\"),\t\tObjectId(\"5d6ccdf9a82b6d69714cebee\")\t]}View Record&gt; db.customers.find({profile_name: 'customer name'}){ \"_id\" : ObjectId(\"5d6ccbfda82b6d69714cebeb\"), \"profile_name\" : \"customer name\", \"email\" : \"email@example.com\", \"age\" : 32, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname\", \"mid_name\" : \"midname\", \"last_name\" : \"last_name\" } }multiple matched records&gt; db.customers.find({age: 22}){ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebee\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email4@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559364\"), \"profile_name\" : \"customer name 2\", \"email\" : \"email10@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559365\"), \"profile_name\" : \"customer name 3\", \"email\" : \"email11@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname3\", \"mid_name\" : \"midname3\", \"last_name\" : \"last_name3\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559366\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email12@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }  db.customers.find({profile_name: “customer name 4”})  { \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebee\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email4@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559366\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email12@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }    db.customers.find({profile_name: “customer name 4”, “email”: “email4@example.com”})  { \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebee\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email4@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }    db.customers.find({profile_name: “customer name 4”}).limit(1)  { \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebee\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email4@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }  Show all records  db.customers.find()  { \"_id\" : ObjectId(\"5d6ccbfda82b6d69714cebeb\"), \"profile_name\" : \"customer name\", \"email\" : \"email@example.com\", \"age\" : 32, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname\", \"mid_name\" : \"midname\", \"last_name\" : \"last_name\" } }{ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebec\"), \"profile_name\" : \"customer name 2\", \"email\" : \"email2@example.com\", \"age\" : 30, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2\" } }{ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebed\"), \"profile_name\" : \"customer name 3\", \"email\" : \"email3@example.com\", \"age\" : 36, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname3\", \"mid_name\" : \"midname3\", \"last_name\" : \"last_name3\" } }{ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebee\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email4@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559364\"), \"profile_name\" : \"customer name 2\", \"email\" : \"email10@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559365\"), \"profile_name\" : \"customer name 3\", \"email\" : \"email11@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname3\", \"mid_name\" : \"midname3\", \"last_name\" : \"last_name3\" } }{ \"_id\" : ObjectId(\"5d6cd0eb68f1285dc9559366\"), \"profile_name\" : \"customer name 4\", \"email\" : \"email12@example.com\", \"age\" : 22, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname4\", \"mid_name\" : \"midname4\", \"last_name\" : \"last_name4\" } }  Upsert Option (updateOne(), updateMany(), replaceOne())Replace a record except an ID fielddb.customers.replaceOne({email: 'email@example.com'}, {\"profile_name\" : \"customer name\", \"email\" : \"email@example.com\", \"age\" : 15, \"tags\" : [ \"nonregular\" ], \"full_name\" : { \"first_name\" : \"firstname\", \"mid_name\" : \"midname\", \"last_name\" : \"last_name\" }}){ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }When you search the record which is just updated, you will notice update has been made:db.customers.find({email: 'email@example.com'}){ \"_id\" : ObjectId(\"5d6ccbfda82b6d69714cebeb\"), \"profile_name\" : \"customer name\", \"email\" : \"email@example.com\", \"age\" : 15, \"tags\" : [ \"nonregular\" ], \"full_name\" : { \"first_name\" : \"firstname\", \"mid_name\" : \"midname\", \"last_name\" : \"last_name\" } }Update a record execpt an ID fieldFirst track the record you want to update&gt; db.customers.find({email: 'email2@example.com'}){ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebec\"), \"profile_name\" : \"customer name 2\", \"email\" : \"email2@example.com\", \"age\" : 30, \"tags\" : [ \"regular\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2\" } }&gt;Apply Update command&gt; db.customers.updateOne({email: 'email2@example.com'}, {$set: {\"profile_name\": 'customer name 2 updated', \"full_name.last_name\": \"last_name2 updated\", tags: [\"regular updated\"]}}){ \"acknowledged\" : true, \"matchedCount\" : 1, \"modifiedCount\" : 1 }Now check the record and you will notice the record is updated&gt; db.customers.find({email: 'email2@example.com'}){ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebec\"), \"profile_name\" : \"customer name 2 updated\", \"email\" : \"email2@example.com\", \"age\" : 30, \"tags\" : [ \"regular updated\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2 updated\" } }Add lastModified field when you update the record which will add new column&gt; db.customers.updateOne({email: 'email2@example.com'}, {$set: {\"profile_name\": 'customer name 2 updated', \"full_name.last_name\": \"last_name2 updated\", tags: [\"regular updated\"]}, $currentDate: { lastModified: true }}){ \"_id\" : ObjectId(\"5d6ccdf9a82b6d69714cebec\"), \"profile_name\" : \"customer name 2 updated\", \"email\" : \"email2@example.com\", \"age\" : 30, \"tags\" : [ \"regular updated\" ], \"full_name\" : { \"first_name\" : \"firstname2\", \"mid_name\" : \"midname2\", \"last_name\" : \"last_name2 updated\" }, \"lastModified\" : ISODate(\"2019-09-02T08:55:35.114Z\") }``1The update action involves following operations:uses the $set operator to update the value of the size.uom field to \"cm\" and the value of the status field to \"P\",uses the $currentDate operator to update the value of the lastModified field to the current date. If lastModified field does not exist, $currentDate will create the field. See $currentDate for details.#### UpdateManyUpdate all the documents where age is greater than 22db.customers.updateMany(    {        “age”: { $gt: 22 }    },    {        $set: { “tags”: “Multiple Update”, active: “true” },        $currentDate: { lastModified: true }    })&gt;&gt; { \"acknowledged\" : true, \"matchedCount\" : 2, \"modifiedCount\" : 2 }Now it's time to check updated record:  db.customers.find({“age”: {$gt: 22}}){ “_id” : ObjectId(“5d6ccdf9a82b6d69714cebec”), “profile_name” : “customer name 2 updated”, “email” : “email2@example.com”, “age” : 30, “tags” : “Multiple Update”, “full_name” : { “first_name” : “firstname2”, “mid_name” : “midname2”, “last_name” : “last_name2 updated” }, “lastModified” : ISODate(“2019-09-02T12:56:37.700Z”), “active” : “true” }{ “_id” : ObjectId(“5d6ccdf9a82b6d69714cebed”), “profile_name” : “customer name 3”, “email” : “email3@example.com”, “age” : 36, “tags” : “Multiple Update”, “full_name” : { “first_name” : “firstname3”, “mid_name” : “midname3”, “last_name” : “last_name3” }, “active” : “true”, “lastModified” : ISODate(“2019-09-02T12:56:37.700Z”) }```Some other options:db.collection.findOneAndReplace().db.collection.findOneAndUpdate().db.collection.findAndModify().db.collection.save().db.collection.bulkWrite()Delete document (deleteOne(), deleteMany())Delete only one record no matter multiple records for this profile name exists&gt; db.customers.deleteOne({profile_name: 'profile name'})o/p =&gt; { \"acknowledged\" : true, \"deletedCount\" : 1 }Delete many records at once&gt; db.customers.deleteMany({profile_name: 'profile name 1'})o/p =&gt; { \"acknowledged\" : true, \"deletedCount\" : 4 }Bulk Write with bulkWrite()try {    db.customers.bulkWrite([        { insertOne: { \"document\": {                        profile_name: 'foo bar',                        email: 'foo@shivrajbadu.com.np',                        age: 29,                        tags: [\"regular\"],                        full_name: { first_name: 'shiv', mid_name: 'raj', last_name: 'badu' }                    }                }            },            { insertOne: { \"document\": {                    profile_name: 'foo baz',                    email: 'baz@shivrajbadu.com.np',                    age: 25,                    tags: [\"regular\"],                    full_name: { first_name: 'foo', mid_name: 'bar', last_name: 'baz' }                    }                }            },            { updateOne: {                    \"filter\" : { email: 'email10@example.com' },                    \"update\" : { $set: {\"profile_name\": 'new name', tags: [\"irregular\"], \"full_name.first_name\": \"fn\", \"full_name.mid_name\": \"mn\", \"full_name.last_name\": \"ln\" } }                }            },            { deleteOne: {                    \"filter\": { profile_name: 'customer name' }                }            },            { replaceOne: {                 \"filter\": { email: 'email@example.com' },                \"replacement\": { \"profile_name\": 'profile name', \"email\": 'email@example.com', \"age\": 14, \"tags\": [\"irregular\"], \"full_name\": { \"first_name\": \"fn\", \"last_name\": \"ln\", \"mid_name\": \"mn\" }  }             } }        ]);} catch (e) {    print(e);}Text SearchTo perform text search use text index and $text operator, text indexes can include any field whose value is a string or an array of string elements. To perform text search queries, you must have a text index on your collection. A collection can only have one text search index, but that index can conver multiple fields.If index is not found you will get following error message:Error: error: {\t\"ok\" : 0,\t\"errmsg\" : \"text index required for $text query\",\t\"code\" : 27,\t\"codeName\" : \"IndexNotFound\"}So, you need to create Index firstdb.customers.createIndex({    profile_name: \"text\",    email: \"text\"})db.customers.find({    $text: {        $search: \"myemail@shivrajbadu.com.np\"    }})db.customers.aggregate(    [        { $match: { $text: { $search: \"first name\" } } }    ])To get exact match result of searched textdb.customers.aggregate(    [        { $match: { $text: { $search: \"\\\"customer name replaced\\\"\" } } }    ])Referencesuser document             contact document                            articles document-------------             -----------------                          -----------------{                         {                                            {    _id: &lt;ObjectId1&gt;,        _id: &lt;ObjectId2&gt;,                            _id: &lt;ObjectId3&gt;,    username: 'xyz'           user_id: &lt;ObjectId1&gt;,                       user_id: &lt;ObjectId1&gt;,}                             phone: '9852525252',                        title: 'first article',                              email: 'contact@shivrajbadu.com.np'         body: 'article body'                          }                                            }One-to-One Relationships with Embedded DocumentsContact document contains a reference to the User document.User Document{    _id: \"unique_id\",    username: 'uniquename'}Contact Document{    _id: “ObjectId(“5d6df862e1b6226e35c6c519”)”,    _user_id: “unique_id”,    phone: “8585858585”,    email: “contact@shivrajbadu.com.np”}### One-to-Many Relationships with Embedded DocumentsIn the normalized data model, the articles documents contain a reference to the user document.User Document{    _id: “unique_id”,    username: ‘uniquename’}Article Document{    _id: \"ObjectId(\"5d6df9b5e1b6226e35c6c522\")\",    _user_id: \"unique_id\",    title: \"This is a title.\",    body: \"This is a description.\"}{    _id: \"ObjectId(\"9e7df9b5e1b6226e35c6c435\")\",    _user_id: \"unique_id\",    title: \"This is another title.\",    body: \"This is description for another title.\"}When implement one to many relationships, many child records will have many child document records so multiple queries need to be issued to resolve the references, we can also use another solution to make single query as shown below:{    _id: \"unique_id\",    username: 'uniquename',    articles: [        {            _id: \"ObjectId(\"5d6df9b5e1b6226e35c6c522\")\",            _user_id: \"unique_id\",            title: \"This is a title.\",            body: \"This is a description.\"        },        {            _id: \"ObjectId(\"9e7df9b5e1b6226e35c6c435\")\",            _user_id: \"unique_id\",            title: \"This is another title.\",            body: \"This is description for another title.\"        }    ]}"
  },
  
  {
    "title": "How to install MongoDB on Ubuntu 18.04?",
    "url": "/posts/MongoDB-installation/",
    "categories": "MongoDB, Installation",
    "tags": "mongodb, installation",
    "date": "2019-08-29 09:23:58 +0545",
    





    
    "snippet": "Install MongoDB Community Edition on Unbuntu 18.04 (Bionic)Import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.ascwget -qO - https://www.mongodb.org/static/pgp/serv...",
    "content": "Install MongoDB Community Edition on Unbuntu 18.04 (Bionic)Import the MongoDB public GPG Key from https://www.mongodb.org/static/pgp/server-4.2.ascwget -qO - https://www.mongodb.org/static/pgp/server-4.2.asc | sudo apt-key add -Create the list file /etc/apt/sources.list.d/mongodb-org-4.2.list for your ubuntu versionecho \"deb [ arch=amd64 ] https://repo.mongodb.org/apt/ubuntu bionic/mongodb-org/4.2 multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-4.2.listReload local package databasesudo apt-get updateInstall MongoDB packagessudo apt-get install -y mongodb-orgOptional. Although you can specify any available version of MongoDB, apt-get will upgrade the packages when a newer version becomes available. To prevent unintended upgrades, you can pin the package at the currently installed version:echo \"mongodb-org hold\" | sudo dpkg --set-selectionsecho \"mongodb-org-server hold\" | sudo dpkg --set-selectionsecho \"mongodb-org-shell hold\" | sudo dpkg --set-selectionsecho \"mongodb-org-mongos hold\" | sudo dpkg --set-selectionsecho \"mongodb-org-tools hold\" | sudo dpkg --set-selectionsDirectories are at/var/lib/mongodb/var/log/mongodbConfiguration files are at/etc/mongod.confStart MongoDBsudo service mongod startTo verify MongoDB is started successfully, check the following content on /var/log/mongodb/mongod.log[initandlisten] waiting for connections on port 27017Processsudo service mongod stopsudo service mongod restartUse MongoDB using command shellmongoUninstall MongoDB stepssudo service mongod stopsudo apt-get purge mongodb-org* # remove mongoDB packages# remove data directoriessudo rm -r /var/log/mongodb sudo rm -r /var/lib/mongodb"
  },
  
  {
    "title": "Polymorphism in Ruby",
    "url": "/posts/polymorphism-in-ruby/",
    "categories": "Ruby, Polymorphism",
    "tags": "ruby, polymorphism",
    "date": "2019-08-28 09:23:58 +0545",
    





    
    "snippet": "Polymorphism in RubyThe term polymorphism means having many forms. In Ruby, polymorphism is carried out by using Inheritance. Polymorphism is achieved by using method overriding.class Animal    def...",
    "content": "Polymorphism in RubyThe term polymorphism means having many forms. In Ruby, polymorphism is carried out by using Inheritance. Polymorphism is achieved by using method overriding.class Animal    def eat # this method will overrides on other inherited classes        puts \"Animal eats grasses, water, milk, etc\"    endendclass Cat &lt; Animal    def eat        puts \"Cat eats milk &amp; water\"    endendclass Cow &lt; Animal    def eat        puts \"Cow eats grasses &amp; water\"    endendanimal = Animal.newanimal.eat=&gt; Animal eats grasses, water, milk, etcanimal = Cat.newanimal.eat=&gt; Cat eats milk &amp; wateranimal = Cow.newanimal.eat=&gt; Cow eats grasses &amp; water"
  },
  
  {
    "title": "Inheritance in Ruby",
    "url": "/posts/inheritance-in-ruby/",
    "categories": "Ruby, Inheritance",
    "tags": "ruby, inheritance",
    "date": "2019-08-28 09:23:58 +0545",
    





    
    "snippet": "Inheritance in RubyInheritance is the feature of OOP in which characteristics &amp; behaviours of one class inherits into another class. The class which is inheriting behaviour is called subclass a...",
    "content": "Inheritance in RubyInheritance is the feature of OOP in which characteristics &amp; behaviours of one class inherits into another class. The class which is inheriting behaviour is called subclass and class it inherits from is called superclass. Inheritance can also be used to remove duplication in your code and helps to achieve DRY “Don’t Repeat Yourself” principle.Class Inheritanceclass Animal    def eat # this method will overrides on other inherited classes        puts \"Animal eats grasses, water, etc\"    endendclass Cat &lt; Animalendclass Cow &lt; AnimalendHere Animal is the superclass and Cat, Cow is the subclass.cat = Cat.newcow = Cow.newcat.eat # =&gt; Animal eats grasses, water, etccow.eat # =&gt; Animal eats grasses, water, etcMethod overridingclass Animal    def eat        puts 'Animal eats grasses, water, etc'    endendclass Cat &lt; Animal    attr_accessor :food_name    def initialize(food_name)        @food_name = food_name    end    def eat        puts \"Cat eats #{food_name}\"    endendclass Dog &lt; Animalendcat = Cat.new(\"Milk and water\")cat.eat# =&gt; Milk and waterdog = Dog.newdog.eat# =&gt; Animal eats grasses, water, etcsupersuper is the inbuilt function of Ruby, which is used to call the methods up the inheritance hierarchy.class Animal  def eat    \"Animal\"  endendclass Cat &lt; Animal  def eat    super + \" - cat - eats milk and water.\"  endendcat = Cat.newcat.eat        # =&gt; \"Animal - cat - eats milk and water.\"Module Mixins in RubyModules are a way of grouping together methods, classes, and constants. Modules provide a namespace and prevent name clashes, and it implement the mixin facility. Mixins is like multiple inheritence.module ModuleName    def module_method        puts \"I am a module method\"    endendclass ClassName    include ModuleNameendobj = ClassName.newobj.module_method# =&gt; I am a module method"
  },
  
  {
    "title": "Features and functions of Active Record Model",
    "url": "/posts/ruby-on-rails-active-record-models-notes/",
    "categories": "Ruby on Rails, Active Record cheatsheet",
    "tags": "ruby on rails, active_record",
    "date": "2019-08-27 07:12:58 +0545",
    





    
    "snippet": "Notes on various features and functions of Active Record ModelQuery Methodsobj = User  .where(email: 'info@mydomain.np')  .where('id = 2')  .where('id = ?', 2)  .order(:tag_line)  .order(tag_line: ...",
    "content": "Notes on various features and functions of Active Record ModelQuery Methodsobj = User  .where(email: 'info@mydomain.np')  .where('id = 2')  .where('id = ?', 2)  .order(:tag_line)  .order(tag_line: :desc)  .order(\"tag_line DESC\")  .reorder(:tag_line) # Replaces any existing order defined on the relation with the specified order.  .where(active: true)  .rewhere(active: false) # Allows to change a previously set where condition for a given attribute, instead of appending to that condition.  .offset(1)  .limit(2)  .uniqSome other query methodsitems = Employer  .select(:id)  .select([:id, :name])  .group(:title)   # GROUP BY name  .group('title AS grouped_title, age')  .having('SUM(salary) &gt; 25000')  # needs to be chained with .group  .includes(:user)  .includes(user: [:articles])  .references(:comments)Finder methodsitem = ModelName.find(id)item = ModelName.find_by_email(email)item = ModelName.where(email: email).firstModel  .first  .last  .exists?(5)  .exists?(name: \"ShivRaj\")  .find_nth(4, [offset])Persistenceitem.new_record?item.persisted?item.destroyed?item.serialize_hash # Returns a serialized hash of your objectitem.saveitem.save!      # It does same as save, but raises an Exceptionitem.update  name: 'ShivRaj'  # Save the record immediatelyitem.update! name: 'ShivRaj'item.update_column  :name, 'ShivRaj'  # It skips validations and callbacksitem.update_columns  name: 'ShivRaj'item.update_columns! name: 'ShivRaj'item.touch                 # It updates :updated_atitem.touch :published_atitem.destroyitem.delete  # It skips callbacksModel.create     # It does same task which is done by new and saveModel.create!    # It does same as create but raises an ExceptionAttribute Assignmentitem.attributes                         # &lt;Hash&gt;item.attributes = { name: 'ShivRaj' }   # Merges attributes in but it Doesn't save.item.assign_attributes name: 'ShivRaj'  # Merges attributes in but it Doesn't save.Validationsitem.valid?item.invalid?Dirtyitem.changed?item.changed             # ['name']item.changed_attributes  # { 'name' =&gt; 'ShivRaj' } - original valuesitem.changes             # { 'name' =&gt; ['ShivRaj', 'PushpaRaj'] }item.previous_changes    # available after #saveitem.restore_attributesitem.name = 'ShivRaj'item.name_was         # 'ShivRaj'item.name_change      # [ 'ShivRaj', 'PushpaRaj' ]item.name_changed?    # trueitem.name_changed?(from: 'ShivRaj', to: 'PushpaRaj')CalculationsPerson.countPerson.count(:age)    # counts non-nil'sPerson.average(:age)Person.maximum(:age)Person.minimum(:age)Person.sum('2 * age')Person.calculate(:count, :all)Person.distinct.countPerson.group(:city).countDynamic attribute-based finders# Returns one recordPerson.find_by_name(name)Person.find_last_by_name(name)Person.find_or_create_by_name(name)Person.find_or_initialize_by_name(name)# Returns a list of recordsPerson.find_all_by_name(name)# Add a bang to make it raise an exceptionPerson.find_by_name!(name)# You may use `scoped` instead of `find`Person.scoped_by_user_namebelongs to Association  belongs_to :author,  :dependent      =&gt; :destroy    # or :delete  :class_name     =&gt; \"Seller\"  :select         =&gt; \"*\"  :counter_cache  =&gt; true  :counter_cache  =&gt; :custom_counter  :include        =&gt; \"Product\"  :readonly       =&gt; true  :conditions     =&gt; 'published = true'  :touch          =&gt; true  :touch          =&gt; :sellers_last_updated_at  :primary_key    =&gt; \"name\"  :foreign_key    =&gt; \"author_name\"Has many Associationbelongs_to :parent, :foreign_key =&gt; 'parent_id' class_name: 'Folder'has_many :folders, :foreign_key =&gt; 'parent_id', class_name: 'Folder'has_many :comments,    :order      =&gt; \"posted_on\"has_many :comments,    :include    =&gt; :authorhas_many :people,      :class_name =&gt; \"Person\"has_many :people,      :conditions =&gt; \"deleted = 0\"has_many :tracks,      :order      =&gt; \"position\"has_many :comments,    :dependent  =&gt; :nullifyhas_many :comments,    :dependent  =&gt; :destroyhas_many :tags,        :as         =&gt; :taggablehas_many :reports,     :readonly   =&gt; truehas_many :subscribers, :through    =&gt; :subscriptions, class_name: \"User\", :source =&gt; :userhas_many :subscribers, :finder_sql =&gt;    'SELECT DISTINCT people.* ' +    'FROM people p, post_subscriptions ps ' +    'WHERE ps.post_id = #{id} AND ps.person_id = p.id ' +    'ORDER BY p.first_name'Many-to-many Has many through Association If you have a join model:class Programmer &lt; ActiveRecord::Base  has_many :assignments  has_many :projects, :through =&gt; :assignmentsend  class Project &lt; ActiveRecord::Base  has_many :assignments  has_many :programmers, :through =&gt; :assignmentsend  class Assignment  belongs_to :project  belongs_to :programmerendMany-to-many (HABTM) Associationhas_and_belongs_to_many :projectshas_and_belongs_to_many :projects, :include =&gt; [ :milestones, :manager ]has_and_belongs_to_many :nations, :class_name =&gt; \"Country\"has_and_belongs_to_many :categories, :join_table =&gt; \"prods_cats\"has_and_belongs_to_many :categories, :readonly =&gt; truehas_and_belongs_to_many :active_projects, :join_table =&gt; 'developers_projects', :delete_sql =&gt;\"DELETE FROM developers_projects WHERE active=1 AND developer_id = #{id} AND project_id = #{record.id}\"Polymorphic associationsclass Post  has_many :attachments, as: :parentend class Image  belongs_to :parent, polymorphic: trueend And in migrations:create_table :images do |t|  t.references :post, polymorphic: trueendValidationclass Person &lt; ActiveRecord::Base  # Presence  validates :name,     presence: true   # Acceptance  validates :terms,    acceptance: true  # Confirm  validates :email,    confirmation: true  # Unique  validates :slug,     uniqueness: true  validates :slug,     uniqueness: { case_sensitive: false }  validates :holiday,  uniqueness: { scope: :year, message: 'yearly only' }  # Format  validates :code,     format: /regex/  validates :code,     format: { with: /regex/ }  # Length  validates :name,     length: { minimum: 2 }  validates :bio,      length: { maximum: 500 }  validates :password, length: { in: =&gt; 6..20 }  validates :number,   length: { is: =&gt; 6 }  # Include/exclude  validates :gender,   inclusion: %w(male female)  validates :gender,   inclusion: { in: %w(male female) }  validates :lol,      exclusion: %w(xyz)  # Numeric  validates :points,   numericality: true  validates :played,   numericality: { only_integer: true }  # ... greater_than, greater_than_or_equal_to,  # ... less_than, less_than_or_equal_to  # ... odd, even, equal_to  # Validate the associated records to ensure they're valid as well  has_many :books  validates_associated :books  # Length (full options)  validates :content, length: {    minimum:   300,    maximum:   400,    tokenizer: lambda { |str| str.scan(/\\w+/) },    too_short: \"must have at least %{count} words\",    too_long:  \"must have at most %{count} words\" }  # Multiple  validates :login, :email, presence: true  # Conditional  validates :description, presence: true, if: :published?  validates :description, presence: true, if: lambda { |obj| .. }  validates :title, presence: true, on: :save   # :save | :create | :updateendCustom validationsclass Person &lt; ActiveRecord::Base  validate :foo_cannot_be_nil  def foo_cannot_be_nil    errors.add(:foo, 'cannot be nil')  if foo.nil?  endendErrorsrecord.errors.valid?      # → falserecord.errors             # → { :name =&gt; [\"can't be blank\"] }record.errors.messages    # → { :name =&gt; [\"can't be blank\"] }record.errors[:name].any?Mass updates# Updates article having id 8Article.update 8, name: \"\", age: 34Article.update [2,3], [{name: \"Shiv\"}, {name: \"Raj\"}]Joining# Basic joinsEmployer.joins(:companies).where(companies: { type: 'private' })Employer.joins(:companies).where('companies.type' =&gt; 'private' )# Multiple associationsBlog.joins(:category, :comments)# Nested associationsBlog.joins(comments: :guest)# SQLAuthor.joins(  'INNER JOIN posts ' +  'ON posts.author_id = authors.id ' +  'AND posts.published = \"t\"')Where interpolationwhere('name = ?', 'Shiv')where(['name = :name', { name: 'Shiv' }])Serializeclass User &lt; ActiveRecord::Base  serialize :preferencesend user = User.create(  preferences: {    'background' =&gt; 'black',    'display' =&gt; 'large'  })You can also specify a class option as the second parameter that’ll raise an exception if a serialized object is retrieved as a descendant of a class not in the hierarchy.# Only Hash allowed!class User &lt; ActiveRecord::Base  serialize :preferences, Hashend # Reading it raises SerializationTypeMismatchuser = User.create(preferences: %w(one two three))User.find(user.id).preferencesOverriding accessorsclass Song &lt; ActiveRecord::Base  # Uses an integer of seconds to hold the length of the song  def length=(minutes)    write_attribute(:length, minutes.to_i * 60)  end  def length    read_attribute(:length) / 60  endend"
  },
  
  {
    "title": "Git Modify Author and Committer",
    "url": "/posts/git-modify-author-committer/",
    "categories": "Git, Modify Author And Committer",
    "tags": "git",
    "date": "2019-06-13 07:23:58 +0545",
    





    
    "snippet": "How to change all commits to have the same newly added author and committer?Go to appropriate branch and project directory and run the following command on console.  git filter-branch -f --env-filt...",
    "content": "How to change all commits to have the same newly added author and committer?Go to appropriate branch and project directory and run the following command on console.  git filter-branch -f --env-filter \"    GIT_AUTHOR_NAME='ShivRaj'     GIT_AUTHOR_EMAIL='shivrajbadu@gmail.com'    GIT_COMMITTER_NAME='ShivRaj'    GIT_COMMITTER_EMAIL='shivrajbadu@gmail.com'  \" HEADAbove command run successfully with following outputRewrite cd130b5306f93f52a1ef7cce7fd8c25ad5a68b14 (1/1) (0 seconds passed, remaining 0 predicted)    Ref 'refs/heads/master' was rewritten"
  },
  
  {
    "title": "Conditional Validations",
    "url": "/posts/conditional-validations-rails/",
    "categories": "Ruby on Rails, Conditional Validations",
    "tags": "ruby on rails, validation",
    "date": "2019-01-07 08:01:18 +0545",
    





    
    "snippet": "Senerio:  User may not provide Name when creating profile, so user name is not compulsory  But if Name is provided then minimum character should be 3 &amp; max character should be 10Solutions:# Nor...",
    "content": "Senerio:  User may not provide Name when creating profile, so user name is not compulsory  But if Name is provided then minimum character should be 3 &amp; max character should be 10Solutions:# Normally we do like this and is the best way in this situationvalidates :name, length: { minimum: 5, maximum: 15 },                          allow_blank: truevalidates :name, length: { minimum: 5, maximum: 15 },                if: :length_of_name_is_not_zerovalidates :name, length: { minimum: 5, maximum: 15 },                unless: Proc.new {|obj| obj.name.length == 0}def length_of_name_is_not_zero  return false if self.name.length.eql?(0)  trueendAnother example of conditional validationattr_accessor :stu_field_validatevalidates :no_of_students, :presence =&gt; { :if =&gt; \"student_no_validate?\" }def student_no_validate? self.stu_field_validate.present? and ['no_of_students'].include?(self.stu_field_validate)endPass field value as&lt;%= f.hidden_field :stu_field_validate, :value =&gt; \"no_of_students\" %&gt;"
  },
  
  {
    "title": "Blocks, Lambdas and Proc",
    "url": "/posts/blocks-proc-and-lambdas/",
    "categories": "Ruby, Block Lambda Proc",
    "tags": "ruby, block, lambda, proc",
    "date": "2019-01-07 08:01:18 +0545",
    





    
    "snippet": "Lambdas and ProcLambdas and Proc are block executing statement.Lambdas and Proc both are object of Proc.Lambdas and Proc are executed by call().Lambda declarationx = lambda { p \"This is lambda\" }x....",
    "content": "Lambdas and ProcLambdas and Proc are block executing statement.Lambdas and Proc both are object of Proc.Lambdas and Proc are executed by call().Lambda declarationx = lambda { p \"This is lambda\" }x.call=&gt; \"This is lambda\"obj = lambda do |x, y|  x+yendobj.call(2,3)=&gt; 5# if required arguments are not supplied lambda throws argument errorsobj.call(2,3,5)ArgumentError (wrong number of arguments (given 3, expected 2))Proc declarationx = Proc.new {p \"this is proc\"}x.call=&gt; \"this is proc\"obj = Proc.new do |x, y|  x+yendobj.call(2,3)=&gt; 5# if required arguments are not supplied it won't throw argument error like lambdaobj.call(2,3,5)=&gt; 5class Block  def hello(*args, &amp;block)    yield *args  end  proc = Proc.new do |*args|    puts *args.class    arr = *args    sum = 0    arr.flatten.each do |num|      sum = sum + num    end    puts sum  end  obj = Block.new  obj.hello([1,10,15], &amp;proc)end=&gt; Array26BlocksRuby blocks are anonymous functions are passed into methods. They are enclosed between {} brackets or in do/end statement.It accepts multiple arguments as |arg1, …, argn|. Blocks are used with each.It allows to save code and use it later.#### single line blocks[20,30,40].each {|n| puts n}# here code inside {} are block#### multi-line blocks[20,30,40].each do |n|  puts nendRuby yield keywordyield is a keyword that calls and run the code inside the blockdef block_fun  yieldendblock_fun { puts \"Block is executing\" }"
  },
  
  {
    "title": "Engine on Rails",
    "url": "/posts/engines-on-rails/",
    "categories": "Ruby on Rails, Engines",
    "tags": "ruby on rails, engines",
    "date": "2019-01-07 08:01:18 +0545",
    





    
    "snippet": "Engines are small applications which provides functionality to their host applications. A Rails application is a engine with Rails::Application class inheriting a lot of behaviour from Rails::Engin...",
    "content": "Engines are small applications which provides functionality to their host applications. A Rails application is a engine with Rails::Application class inheriting a lot of behaviour from Rails::Engine. So Rails application and engine are alomost same thing and share common structure with slight differences.Engines also related with plugins, both shares lib/ directory structure, and both generated using rails plugin new generator. Engine is considered as full plugin using --full in generator while --mountable option includes features of full and some others. An engine can be a plugin and plugin can be an engine.Some example of engines are: Devise for authenticating parent application, Thredded for forum functionlity, Spree for ecommerce, Refinery CMS for CMS application.Create a Base application (e.g: base_app)Create engine e.g: engine_app with following command:  rails new plugin engine_app --mountableCustomize engine_app.gemspec file and edit, homepage, summary, description, etc as per your requirements.Go to base_app -  Gemfile  gem 'engine_app', path: 'lib/engine_app'  bundle installWhen everything is Ok, we will get some message like:Using engine_app 0.0.1 from source at lib/engine_appHow to make plugins -  routes easy?Go to routes.rb file of base app and paste the code:mount EngineApp::Engine, at: '/engine_app'orGo to plugins app/lib/engine_app/engine.rbisolate_namespace EngineAppinitializer \"engine_app\", before: :load_config_initializers do |app|      Rails.application.routes.append do          mount EngineApp::Engine, at: '/engine_app'      endendNow it is possible to run rake routes and we can see all engines routese.g: engine_app /engine_app EngineApp::EngineHow to check Plugins Rails Console?Go to main app&gt; rails console&gt; EngineApp::Article.newHow to make MVC in base and inside engines?To make model controller inside base go to base dir or else go to engine dirRails Engine MigrationsBase app has no knowledge of Engine migrations, we need to customize it or manually we need to run command:i) Manually we can run following command:rake engine_app:install:migrationsii) go to lib/engine.rb file and paste the following code initializer \"engine_app\", before: :load_config_initializers do |app|     config.paths[\"db/migrate\"].expanded.each do |expanded_path|         Rails.application.config.paths[\"db/migrate\"] &lt;&lt; expanded_path     end endNow rake db:migrate will work from base applicationHow to access plugins and base app’s controller action?For e.g: link_to 'Home', root_path, will not work if we want to access enginesresources because engine won’t able to understand root_path for it. So we need following codes:link_to 'Home', main_app.root_pathlink_to 'Plugin Home', engine_app.articles_pathHow to layout in base and engines?We have to call layouts inside application controller or wherever required and it can be accessed by:layouts 'application' # will call base layoutslayouts 'engine_app/application' # will call plugin layoutsHow to include gems inside engines?Go to .gemspec file e.g: engine_app.gemspecGem::Specification.new do |s|  s.add_dependency \"devise\"  s.add_dependency 'authority', '~&gt; 3.1'end"
  },
  
  {
    "title": "Pow alternatives prax !",
    "url": "/posts/pow-alternatives-prax/",
    "categories": "Ruby, Pow Alternatives Prax",
    "tags": "ruby, linux, prax, pow_alternate, rack gem",
    "date": "2019-01-05 09:01:18 +0545",
    





    
    "snippet": "Pow is zero-config Rack server for Mac OS X. Your application will run on myapp.test without modifying /etc/hosts.Those who use GNU/Linux and installed Ruby and Rack gem Prax is usefull. It is a we...",
    "content": "Pow is zero-config Rack server for Mac OS X. Your application will run on myapp.test without modifying /etc/hosts.Those who use GNU/Linux and installed Ruby and Rack gem Prax is usefull. It is a web server which start rack application in background and proxy all requests to that application.Configurations  git clone git://github.com/ysbaddaden/prax.git /opt/prax  cd /opt/prax/ &amp;&amp; ./bin/prax install  sudo /etc/init.d/prax start  # Go to application and run the command  # cd apps/yourappname  prax link  # open your application with this command  prax open  # or  google-chrome http://yourappname.dev/  # see the list of linking application using  prax listIf you are using (RVM) Ruby version manager, follow below steps:cd $HOMEtouch .praxconfigPaste this code in .praxconfig file# detect `$rvm_path`if [ -z \"${rvm_path:-}\" ] &amp;&amp; [ -x \"${HOME:-}/.rvm/bin/rvm\" ]then rvm_path=\"${HOME:-}/.rvm\"fiif [ -z \"${rvm_path:-}\" ] &amp;&amp; [ -x \"/usr/local/rvm/bin/rvm\" ]then rvm_path=\"/usr/local/rvm\"fi# load environment of current project rubyif  [ -n \"${rvm_path:-}\" ] &amp;&amp;  [ -x \"${rvm_path:-}/bin/rvm\" ] &amp;&amp;  rvm_project_environment=`\"${rvm_path:-}/bin/rvm\" . do rvm env --path2&gt;/dev/null` &amp;&amp;  [ -n \"${rvm_project_environment:-}\" ] &amp;&amp;  [ -s \"${rvm_project_environment:-}\" ]then  echo \"RVM loading: ${rvm_project_environment:-}\"  \\. \"${rvm_project_environment:-}\"else  echo \"RVM project not found at: $PWD\"fiWhen your host example.dev do not work, then you need to restart your application using prax:# Go to home directory and cd into .praxcd .prax# Go to your application directorycd example.dev# for first time run the command `prax start`, later you can restart itprax restart"
  },
  
  {
    "title": "Ruby - Arrays !",
    "url": "/posts/ruby-arrays/",
    "categories": "Ruby, Arrays",
    "tags": "ruby, arrays",
    "date": "2018-09-02 09:01:18 +0545",
    





    
    "snippet": "An array is an ordered collection of elements that can be of any type. Each element in an array is referred to by an index. Array can have objects like integer, string, float, Fixnum, Hash, Symbol....",
    "content": "An array is an ordered collection of elements that can be of any type. Each element in an array is referred to by an index. Array can have objects like integer, string, float, Fixnum, Hash, Symbol.Creating / Initialization of an Arrayarr = Array.newSet array sizearr = Array.new(50)puts arr.size # 50puts arr.length # 50Assign a value of an arrayarr1 = Array.new([1,2,3,4,5])arr2 = [1,2,3,4,5]Accessing elementesarr1[3] =&gt; 4arr1[50] =&gt; nilarr1[-3] =&gt; 3arr1[0, 4] =&gt; [1,2,3,4] # 0 is the indexing value and 4 is 4 items including indexing value 0arr1[1..3] =&gt; [2,3,4]arr1.at(1) =&gt; 2arr1.fetch(4) =&gt; 5arr1.first =&gt; 1arr1.last =&gt; 5arr1.take(3)=&gt; [1, 2, 3]arr1.drop(3)=&gt; [4, 5]arr = [1,2,3,4,5]arr.count =&gt; 5arr.empty? # =&gt; falsearr.include?(2) =&gt; trueCheck if two array objects are equal# checks the valuearr1.eql? arr2=&gt; true# checks the references of the object, object idarr1.equal? arr2=&gt; false# This is because object_id for both object is differentarr1.object_id = 11544620arr2.object_id = 11617600arr1 = arr2arr1.equal? arr2 =&gt; true# Now object_id for both object is samearr1.object_id = 11617600arr2.object_id = 11617600Check if there are same elements in both arraysarray1 = [1,2,3]array2 = [2,3,1]array1.to_set == array2.to_set=&gt; truearray1 = [1,2,3,4]array2 = [1,2,3]array1.to_set == array2.to_set=&gt; false#### In Ruby &gt;= 2.6 we can use array1.intersection(array2) method, if both are same it returns empty array [][ 1, 2, 3 ].difference([ 3, 2, 1 ])[ 1, 2, 3 ].difference([ 1, 2, 3 ])=&gt; []Word array create an array in which each entry is a single word.count  = %w{one two three four five}This is equivalent tocount = [\"one\", \"two\", \"three\", \"four\", \"five\"]Nested Array: Array can contains other arraysstaffs_info = [  [\"Ram\", \"0012\", \"Manager\"],  [\"Shyam\", \"0013\", \"HR Manager\"],  [\"Hari\", \"0014\", \"Receptionist\"]]Accessing value of nested arraystaffs_info[0][1]=&gt; \"0012\"Adding Data to Arraycount &lt;&lt; \"six\" =&gt; [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\"]count.push(\"seven\")=&gt; [\"one\", \"two\", \"three\", \"four\", \"five\", \"six\", \"seven\"]# insert first position of an arraycount.unshift(\"zero\")=&gt; [\"zero\", \"one\", \"two\", \"three\", \"four\", \"five\"]# insert at any positioncount.insert(5, \"between 4 &amp; 5\")=&gt; [\"zero\", \"one\", \"two\", \"three\", \"four\", \"between 4 &amp; 5\", \"five\"]# insert multivalues oncecount.insert(3, 'bet 2-3 1', 'bet2-3 2', 'bet2-3 3')=&gt; [\"zero\", \"one\", \"two\", \"bet 2-3 1\", \"bet2-3 2\", \"bet2-3 3\", \"three\", \"four\", \"between 4 &amp; 5\", \"five\"]Adding Data to nested Arraystaffs_info[0] &lt;&lt; \"Rs. 80,000\"=&gt; [\"Ram\", \"0012\", \"Manager\", \"Rs. 80,000\"]staffs_info=&gt; [[\"Ram\", \"0012\", \"Manager\", \"Rs. 80,000\"], [\"Shyam\", \"0013\", \"HR Manager\"], [\"Hari\", \"0014\", \"Receptionist\"]]Array pop / Remove items from an arrayarr = [1,2,3,4,5]arr.pop=&gt; 5arr=&gt; [1,2,3,4]# array.unshift(0) will add o to start of an array while array.shift will remove first elementarr = [1,2,3,4,5]arr.shift =&gt; 1arr[2,3,4,5]# delete an item at particular index use delete_at(index_position)arr = [1,2,3,4,5]arr.delete_at(2)=&gt; 3arr[1,2,4,5]# compact() is used to remove nil value from an arrayarr = [nil, 1, 2, 3, nil, 4, nil, 5]arr.compact[1,2,3,4,5]arr=&gt; [nil, 1, 2, 3, nil, 4, nil, 5]arr.compact!=&gt; [1,2,3,4,5]arr=&gt; [1,2,3,4,5]arr = [1,1,2,2,3,3,4,4,5,6,7]arr.uniq=&gt; [1, 2, 3, 4, 5, 6, 7]Iterating over an Arrayarr = [1,2,3,4,5]arr.each {|item| p item+10}=&gt; it prints 11,12,13,14,15arr = [1,2,3,4,5]arr.reverse_each {|item| p item+10}=&gt; it prints 15,14,13,12,11arr = [1,2,3,4,5]arr.map {|item| p item+10}=&gt; it prints 11,12,13,14,15arr=&gt; 1,2,3,4,5arr.map! {|item| p item+10}arr=&gt; [11, 12, 13, 14, 15]Selecting items from an ArrayNon-destructive Selectionarr = [1,2,3,4,5,6,7,8]arr.select {|a| a &gt; 3}# =&gt; [4,5,6,7,8]arr.reject {|a| a &lt; 3}# =&gt; [3,4,5,6,7,8]arr.drop_while {|a| a &lt; 5}# =&gt; [5,6,7,8]arr=&gt; [1,2,3,4,5,6,7,8] Destructive SelectionDestructive methods are select! and reject!arr.select! {|a| a &gt; 3}=&gt; [4, 5, 6, 7, 8]arr=&gt; [4, 5, 6, 7, 8]&gt; arr = [1,2,3,4,5,6,7,8]=&gt; [1, 2, 3, 4, 5, 6, 7, 8] &gt; arr.delete_if { |a| a &lt; 4 }=&gt; [4, 5, 6, 7, 8] &gt; arr=&gt; [4, 5, 6, 7, 8]arr = [1, 2, 3, 4, 5, 6, 7, 8]arr.keep_if { |a| a &lt; 4 }[1,2,3]arr=&gt; [1, 2, 3]            Public methods like &amp;,      , &amp;&amp;,             a = [1, 2, 3, 4]b = [3, 4, 5, 6]Set intersection:a &amp; b=&gt; [3, 4]a | b=&gt; [1, 2, 3, 4, 5, 6]a || b=&gt; [1, 2, 3, 4]a &amp;&amp; b=&gt; [3, 4, 5, 6]Concatenating two arrays:[\"a\", \"b\"] + [\"c\", \"d\"]=&gt; [\"a\", \"b\", \"c\", \"d\"]Difference of arrays:[\"a\", \"b\", \"c\", \"d\", \"e\"] - [\"c\", \"d\"]=&gt; [\"a\", \"b\", \"e\"]Arrarys can be chained together and returns an array (arr « obj -&gt; arr)[\"a\", \"b\"] &lt;&lt; 10 &lt;&lt; [\"c\", \"d\"]=&gt; =&gt; [\"a\", \"b\", 10, [\"c\", \"d\"]]array &lt;=&gt; another_array -&gt; -1, 0, +1 or nila = [1,2]b = [3,4]a &lt;=&gt; b =&gt; -1a = [1, 2]b = [1, 2]a &lt;=&gt; b =&gt; 0 a = [1, 2, 3] b = [1, 2] a &lt;=&gt; b =&gt; 1 a = [1, 2, 3] b = [1, 2, :v] a&lt;=&gt;b =&gt; nilarr == another_arr -&gt; bool[1,3] == [1,3] #=&gt; true[1,3] == [1,3,4] #=&gt; false            bsearch {      x      block} -&gt; elm      Binary search finds a value from this array which meets the given condition in O(log n) where n is the size of the array.arr = [0,1,2,3,4,5]arr.bsearch {|x| x &gt;= 2}=&gt; 2Clear an Arrayarr = [1,2,3,4,5]arr.clear# =&gt; []a = [1,2,3,4,5]a.collect {|x| x.to_s+\"!\"}=&gt; [\"1!\", \"2!\", \"3!\", \"4!\", \"5!\"]a.collect.with_index {|x,i| p i}=&gt; [0, 1, 2, 3, 4]a.map.with_index {|x,i| p i}=&gt; [0, 1, 2, 3, 4]Combinationa = [1,2,3,4,5]a.combination(1).to_a#=&gt; [[1], [2], [3], [4], [5]]a.combination(2).to_a=&gt; [[1, 2], [1, 3], [1, 4], [1, 5], [2, 3], [2, 4], [2, 5], [3, 4], [3, 5], [4, 5]]a.combination(3).to_a=&gt; [[1, 2, 3], [1, 2, 4], [1, 2, 5], [1, 3, 4], [1, 3, 5], [1, 4, 5], [2, 3, 4], [2, 3, 5], [2, 4, 5], [3, 4, 5]]compact[1,2,nil,'a','b',4].compact#=&gt; [1,2,'a','b',4]concat[1,2].concat([5,6])#=&gt; [1,2,5,6]cycleCalls the given block for each element n times or forever if nil is given.Does nothing.a=[\"a\",\"b\",\"c\"]a.cycle {|x| puts x} # infinite loopa.cycle(2) {|x| puts x} # =&gt; a b c a b c Array fillarr = [1,2,3]arr.fill('a')=&gt; [\"a\", \"a\", \"a\"]flattenarr = [[1,2], 3,4,[5]]arr.flatten# =&gt; [1,2,3,4,5]replacearr = ['a', 'b']arr.replace([1,2])=&gt; [1,2]sorta = [5,4,6,8]a.sort=&gt; [4, 5, 6, 8]Conversionto_s =&gt; returns the stringto_h =&gt; returns hash i.e. [key, value] pairs&gt; [[1,:b], [2,:c]].to_h=&gt; {1=&gt;:b, 2=&gt;:c}to_a =&gt; returns selfto_ary =&gt; returns selftranspose matrixa = [[1,2], [3,4], [5,6]]a.transpose=&gt; [[1,3,5], [2,4,6]]"
  },
  
  {
    "title": "Ruby - Switch Case Statement !",
    "url": "/posts/ruby-switch-case/",
    "categories": "Ruby, Switch Case",
    "tags": "ruby, switch_case",
    "date": "2018-09-01 07:12:18 +0545",
    





    
    "snippet": "Ruby uses the case expression with one or more when conditions. After execution it returns one of the when statement or default else case.case gets.chompwhen '1'  puts \"You have entered 1\"when '2' ...",
    "content": "Ruby uses the case expression with one or more when conditions. After execution it returns one of the when statement or default else case.case gets.chompwhen '1'  puts \"You have entered 1\"when '2'  puts \"You have entered 2\"else  puts 'You have entered number other than 1 &amp; 2'end"
  },
  
  {
    "title": "Ruby - Loops & Iterators !",
    "url": "/posts/ruby-loops-and-iterators/",
    "categories": "Ruby, Loops & iterators",
    "tags": "ruby, loops",
    "date": "2018-09-01 04:19:12 +0545",
    





    
    "snippet": "Loop is the process in which set of instructions or block of codes are repeated in a specified number of times under certain condition is satisfied. for, while, do while are example of loops.while ...",
    "content": "Loop is the process in which set of instructions or block of codes are repeated in a specified number of times under certain condition is satisfied. for, while, do while are example of loops.while loopRuby while loop is used to execute a program until condition is true, once condition fails execution is terminated from loop. While loop is used when number of needed iterations is not fixed.count = 0while count &lt; 5 do  p count  count = count+1enddo while loop# syntaxloop do  # some code here  break if &lt;condition&gt;end# Examplei = 1while true  puts i  i = i + 1  break if i &gt; 5endi = 1loop do  puts i  i = i + 1  break if i &gt; 5endfor loopfor loop is used to run block of code in a specific number of times when number of needed iterations is known.for num in 1..100  puts numend Range loopRuby each method is used to iterator over individual item in an array.(1..100).each do |num|  puts numend# loop through an array using each[1, 2, 3].each do |i|  puts iend# loop through hash using eachhash_var = {name: 'Car', color: 'Red', model: '2018'}hash_var.each do |key, value|  puts \"#{key} =&gt; #{value}\"end# find index in loop using each_with_index[10, 11, 12].each_with_index do |val, key|  p keyend=&gt; 0   1   2Times loop5.times {|i| puts \"number #{i}\"}skip iterations with the next keyword10.times do |i|  res = i % 2  next unless res==0  puts iendstop a loop early using breakarr = [2,4,6,8,10,12]arr.each do |el|  break if el &gt; 10  puts elend"
  },
  
  {
    "title": "Ruby - Control Flow Statement !",
    "url": "/posts/ruby-if-else-unless-statements/",
    "categories": "Ruby, Control Statement",
    "tags": "ruby, control_structure",
    "date": "2018-08-30 09:23:58 +0545",
    





    
    "snippet": "if Statement in Rubyif, elsif and else block in Ruby controls decision based on the condition to true/false resulting in the different execution of the code.key = 10if key &gt; 15  puts 'Key is gre...",
    "content": "if Statement in Rubyif, elsif and else block in Ruby controls decision based on the condition to true/false resulting in the different execution of the code.key = 10if key &gt; 15  puts 'Key is greater than 15'elsif key &lt; 8  puts 'key is less than 8'else  puts 'key is between 8 and 15'end unless statement is inverse of if statement. unless statement is executed if expression is not truenum = 10unless num == 9  puts \"Selected number is not 10\"endTernary OperatorTernary operator is short hand for if else expression. Two symbols ? : are used.x = 2x &gt; 5 ? 'Greater' : 'Smaller' "
  },
  
  {
    "title": "Ruby String !",
    "url": "/posts/ruby-string/",
    "categories": "Ruby, String",
    "tags": "ruby, string",
    "date": "2018-08-30 09:23:58 +0545",
    





    
    "snippet": "About StringString holds and manipulates an arbitrary sequence of bytes which is group of characters. String in ruby is defined using single quote and double quote as:strvar = 'this is string'strva...",
    "content": "About StringString holds and manipulates an arbitrary sequence of bytes which is group of characters. String in ruby is defined using single quote and double quote as:strvar = 'this is string'strvar1 = \"this is string #{some_dynamic_var}\"Find string lengthsize() and length() are used to find string length\"string\".size=&gt; 6String Interpolationstr = \"String\"puts \"This Is #{str}\"Ruby calls to_s() on the string interpolation block which is used to convert object itself into string.Extract a Substringstr = \"longstring\"str[0,4]# longstr[4,6]# stringstr[0..-2]# longstrinstr[0..3] = ''# stringinclude? is used to find if string contains another stringstr = \"My name is Mr. ABC\"str.include?(\"ABC\")# trueindex() can be used to find the start position / index position of the stringstr = \"My name is Mr. ABC\"str.index(\"ABC\")# 15In Ruby String add more string like this:str = \"string\"str.rjust(18, \"0\") =&gt; \"000000000000string\"str.ljust(18, \"0\") =&gt; \"string000000000000\" Case in Stringvar1 = \"str\"var2 = \"Str\"var1.upcase == var2.upcase=&gt; truevar1.casecmp?(var2) # casecmp? Case-insensitive version of String=&gt; trueTrim a String &amp; Remove a White Spacestr = \"   string   \"str.strip=&gt; \"string\"Trim left and right stringstr = \"   string   \"str.lstrip =&gt; \"string   \"str.rstrip =&gt; \"   string\" String prefix and suffixstart_with?, end_with?str = \"a red car\"str.start_with?(\"a\")# truestr.start_with?(\"car\")# trueRuby 2.5 has two methods delete_prefix &amp; delete_suffixstr = \"a red car\"str.delete_prefix(\"a red\") =&gt; \" car\" str.delete_suffix(\"red car\") =&gt; \"a \"Convert string to array of charactersstr = \"string\"str.split(\"\")=&gt; [\"s\", \"t\", \"r\", \"i\", \"n\", \"g\"]Convert arrary to stringarr = [\"s\", \"t\", \"r\", \"i\", \"n\", \"g\"]arr.join=&gt; \"string\"arr.join(\"-\") =&gt; \"s-t-r-i-n-g\"Count specific characters\"lophophorous\".count(\"o\")=&gt; 4#### Convert string to integer\"str\".to_i=&gt; 0\"50\".to_i=&gt; 50check string is a numbermatch() is introduced in Ruby 2.4\"123\".match?(/\\A-?\\d+\\Z/)=&gt; true\"123sadf\".match?(/\\A-?\\d+\\Z/)=&gt; falseAppend Charactersstr = \"\"str &lt;&lt; \"Ruby\"str &lt;&lt; \" \"str &lt;&lt; \"Rails\"# \"Ruby Rails\"Note: When you use += for string concatenation, this way new string will be created every time which is not good for performance.Loop through characters\"hello world\".each_char {|ch| puts ch}\"hello world\".chars =&gt; [\"h\", \"e\", \"l\", \"l\", \"o\", \" \", \"w\", \"o\", \"r\", \"l\", \"d\"] String Case\"hello\".upcase=&gt; \"HELLO\"\"HELLO\".downcase=&gt; \"hello\"Multiline Stringsb = &lt;&lt;-STRINGhelloworldSTRINGa = %Q(helloworld) =&gt; \"hello\\nworld\\n\"Gsub() replace textstr = \"The color of car is red\"str.gsub(\"red\", \"blue\")=&gt; \"The color of car is blue\"str = \"helloooo\"str.gsub(\"o\", '')=&gt; \"hell\"str = \"my id is 5\"str.gsub(/\\d+/, '1001')=&gt; \"my id is 1001\" str.gsub(/\\w+/) {|w| w.capitalize} =&gt; \"My Id Is 5\"Remove last character of a string\"hello\".chomp(\"o\")=&gt; hellRemove first and last character if first and last letter satisfied some valuestr = \"{'a','b','c'}\"str[1..-1] if str.chars.first == '{'str[0...-1] if str.chars.last == '}'Change string encodings\"string\".encoding =&gt; #&lt;Encoding:UTF-8&gt;\"string\".force_encoding(\"UTF-8\")Find out number of occurrence of each character in a given stringstr = \"hello world\"arr = str.split(\"\")arr.uniq.each {|x| p \"Count of #{x} = #{str.count(x)}\" if x != \" \"output:\"Count of h = 1\"\"Count of e = 1\"\"Count of l = 3\"\"Count of o = 2\"\"Count of w = 1\"\"Count of r = 1\"\"Count of d = 1\""
  },
  
  {
    "title": "Get user input in Ruby!",
    "url": "/posts/ruby-get-user-input/",
    "categories": "Ruby, Get user input",
    "tags": "ruby, get_user_input",
    "date": "2018-08-30 09:23:58 +0545",
    





    
    "snippet": "Getting user inputgets keyword is used to get the user input as a string.#!/usr/bin/rubyputs 'what is your name?'name = gets.chompputs \"How are you #{name}\"String#chomp method returns string after ...",
    "content": "Getting user inputgets keyword is used to get the user input as a string.#!/usr/bin/rubyputs 'what is your name?'name = gets.chompputs \"How are you #{name}\"String#chomp method returns string after removing extra line."
  },
  
  {
    "title": "Ruby Put and Print Commands !",
    "url": "/posts/ruby-puts-print-command/",
    "categories": "Ruby, Puts vs. Print",
    "tags": "ruby, print",
    "date": "2018-08-30 05:12:38 +0545",
    





    
    "snippet": "puts vs printputs and print both are used to display the result of evaluating Ruby code.Major difference between these two are: puts adds a newline after executing but print does not add new line.p...",
    "content": "puts vs printputs and print both are used to display the result of evaluating Ruby code.Major difference between these two are: puts adds a newline after executing but print does not add new line.puts \"one two\"one two=&gt; nilprint \"one two\"one two =&gt; nilprint [1,2,nil][1, 2, nil] =&gt; nilputs [1,2,nil]12=&gt; nil "
  },
  
  {
    "title": "Sql Having & GroupBy!",
    "url": "/posts/sql-having/",
    "categories": "SQL, Having - Group by",
    "tags": "sql, having_group_by",
    "date": "2018-08-29 09:23:58 +0545",
    





    
    "snippet": "Having &amp; Group Clause  Having is used to restrict the rows affected by the Group By clause as it iis similar to Where clause.  Having applies to summarized group records, whereas Where applies ...",
    "content": "Having &amp; Group Clause  Having is used to restrict the rows affected by the Group By clause as it iis similar to Where clause.  Having applies to summarized group records, whereas Where applies to individual records.  Only the groups that meets Having criteria will be returned.  Having requires that the GROUP BY clause is present so both are in the same query.  Group By in sql is used to return distinct rows based on table column supplied on group by or group() method.We have following records in our database, now we want to group record based on i) title ii) id, title combination, and filter further to get count result for articles id greater than 2.&lt;Article id: 1, title: \"\", created_at: \"2019-04-23 05:44:22\", updated_at: \"2019-04-23 05:44:23\"&gt;&lt;Article id: 2, title: \"WND\", created_at: \"2019-04-23 05:46:20\", updated_at: \"2019-04-23 05:46:20\"&gt;&lt;Article id: 3, title: \"a\", created_at: \"2019-04-25 07:35:49\", updated_at: \"2019-04-25 07:35:50\"&gt;&lt;Article id: 4, title: \"a\", created_at: \"2019-04-25 07:35:52\", updated_at: \"2019-04-25 07:35:52\"&gt;]Article.group(:title).countSELECT COUNT(*) AS count_all, \"articles\".\"title\" AS articles_title FROM \"articles\" GROUP BY \"articles\".\"title\"=&gt; {\"\"=&gt;1, \"WND\"=&gt;1, \"a\"=&gt;2}Article.group(:id, :title).countSELECT COUNT(*) AS count_all, \"articles\".\"id\" AS articles_id, \"articles\".\"title\" AS articles_title FROM \"articles\" GROUP BY \"articles\".\"id\", \"articles\".\"title\"=&gt; {[1, \"\"]=&gt;1, [2, \"WND\"]=&gt;1, [3, \"a\"]=&gt;1, [4, \"a\"]=&gt;1}Article.group(:title).having(\"articles.id&gt;2\").countSELECT COUNT(*) AS count_all, \"articles\".\"title\" AS articles_title FROM \"articles\" GROUP BY \"articles\".\"title\" HAVING (articles.id&gt;2)=&gt; {\"a\"=&gt;2}#group_by is used to group the record and #transform_values can be used to count each grouped records.Institution.first.items.includes(:vendor).group_by{|item| item.vendor.name}.transform_values {|values| values.count}=&gt; {\"Vendor 1\"=&gt;1, \"Vendor 2\"=&gt;3, \"Vendor 3\"=&gt;3, \"Vendor 4\"=&gt;1, \"Vendor 5\"=&gt;10 }"
  },
  
  {
    "title": "Ruby Variables!",
    "url": "/posts/Ruby-variables/",
    "categories": "Ruby, Variable",
    "tags": "ruby, variable",
    "date": "2018-08-29 09:23:58 +0545",
    





    
    "snippet": "What is Ruby variables?Variables are like containers used to store information for later use. Values can be stored in the form of Integer, String, Boolean, Float, Decimal, Array, Hashes, etc.Variab...",
    "content": "What is Ruby variables?Variables are like containers used to store information for later use. Values can be stored in the form of Integer, String, Boolean, Float, Decimal, Array, Hashes, etc.Variable can be declared as:num = 10str = \"this is string\"char = 'A'arr = [0,1,2,3,4]hash_var = {name: 'Shiv Raj', code: '00145', address: 'Nepal', \"email\" =&gt; 'shivrajbadu@gmail.com'}bool_var = falseMany languages like C, Java are strong or static variable typing. That means you must define a variable type when declaring them e.g. if it is integer you must write int var_name, if string you must write varchar var_name. But Ruby is dynamically typed language which means we do not need to define type of the variable, and once variable is declared, you can later on change the variable type in the code, these are the advantage of dynamically typed lanaugage.Way to know Ruby Variable TypeUse kind_of? method of Object class.num = 10num.kind_of?(Integer) # trueTo get the class method name used by the variablenum.class=&gt; Fixnumstr = 'this is string'str.class=&gt; String  Variable type can be changed just by assigning new valuex = 10x.class # Fixnumx = \"Ten\"x.class # String  Convert the values of the variablesx=10=&gt; 10x.to_f=&gt; 10.0x.to_s=&gt; \"10\"x.to_s(2) # convert to base 2 binary=&gt; 1010x.to_s(16) # convert to hexadecimal=&gt; \"a\"x.to_s(8) # convert to octal=&gt; \"12\""
  },
  
  {
    "title": "Ruby Variables Scope!",
    "url": "/posts/Ruby-variables-scope/",
    "categories": "Ruby, Variable Scope",
    "tags": "ruby, variable",
    "date": "2018-08-29 09:23:58 +0545",
    





    
    "snippet": "Scope of Ruby variables  Global VariableGlobal Variables can be accessed inside classes and it’s methods. Global variable are available everywhere. It is defined by prefacing the variable name with...",
    "content": "Scope of Ruby variables  Global VariableGlobal Variables can be accessed inside classes and it’s methods. Global variable are available everywhere. It is defined by prefacing the variable name with $ symbol. Before initialization it has value nil.$global_variable = 'This is a global variable !'class Example  def test_global    puts $global_variable  endend# instantiation and callobj = Example.newobj.test_global # This is a global variable !  Instance VariableInstance Variable is accessible in any instance method in a particular instance of a class. It is defined by prefacing the variable name with @ symbol.class Vehicle  def initialize(name, color)    @name = name    @color = color  end  def full_info    puts \"Name of vehicle is: #{@name} with color #{@color} !\"  endend# instantiatevehicle = Vehicle.new('Car', 'Red');# method callvehicle.full_info # Name of vehicle is: Car with color Red !  Local VariableLocal variable has local scope which be accessed inside the code where they are declared, that is when local variable is decared inside method or loop it cannot be used outside of method or loop. It is defined by small letter or begin with underscore.class LocalVariable  def fun    local_var1 = 'one'    _LocalVar2 = 'two'    puts local_var1 + _LocalVar2  endend# instantiation and callLocalVariable.new.fun # onetwo  Class VariableA class variable is a variable that is shared amongst all instances of the class. Class variable are declared with @@ sign. Class variable are called on the class itself. Class variables are like global variable but inside the class scope.class Vehicle  @@name = 'Honda'  def self.name    puts @@name  endendVehicle.name  Ruby ConstantRuby constant are the values whose value cannot be changed once it is assigned. Constant declared within a class are available anywhere within the context of class, and when declared outside of class are assined with a global scope. Constants are written in uppercase letter with underscore to seperate different word.PROJECT_VALUE=100"
  },
  
  {
    "title": "Write Comments on Ruby!",
    "url": "/posts/Comments-in-Ruby/",
    "categories": "Ruby, Comments",
    "tags": "ruby, comments",
    "date": "2018-08-28 09:23:58 +0545",
    





    
    "snippet": "How to write comments on RubyComments in Ruby can be written in two ways:  Single line commentSingle line comment followed by # symbol  # This is a single line comment.  Multi line commentsMultilin...",
    "content": "How to write comments on RubyComments in Ruby can be written in two ways:  Single line commentSingle line comment followed by # symbol  # This is a single line comment.  Multi line commentsMultiline comments starts with =begin and ends with =end=begin  This is multiline comments.  One can write number of lines as per need.=end"
  },
  
  {
    "title": "Backup and Restore PostgreSQL Database",
    "url": "/posts/backup-and-restore-postgresql-databases-on-ubuntu-16-04/",
    "categories": "PostgreSQL, Backup Restore",
    "tags": "postgresql, backup_restore",
    "date": "2018-08-27 09:23:58 +0545",
    





    
    "snippet": "Backup Databasepg_dump is the PostgreSQL utility to backup the database.To backup a single database, run below command in command line interface as superuser.~ sudo pg_dump -U postgres -h localhost...",
    "content": "Backup Databasepg_dump is the PostgreSQL utility to backup the database.To backup a single database, run below command in command line interface as superuser.~ sudo pg_dump -U postgres -h localhost name-of-database &gt; name-of-backup-fileRestore DatabaseBackup file created can be useful to restore your system.To restore it is essential to create a empty database and then you can restore database using following command:psql new_Database_name &lt; path_to_backup_fileHere is the full script to restore the single databasesudo su - postgrespostgres@usr-Aspire-E5-575G:~$ psqlpostgres=# CREATE DATABASE new_database_name TEMPLATE template0;postgres@usr-Aspire-E5-575G:~$ psql new_database_name &lt; /home/siv/name-of-backup-file"
  },
  
  {
    "title": "How to install Ruby on Windows?",
    "url": "/posts/Ruby-Install-Win-And_Run_App/",
    "categories": "Ruby, Operate on WindowsOS",
    "tags": "ruby, windowsOS",
    "date": "2018-08-06 04:23:58 +0545",
    





    
    "snippet": "When you are on Windows machineYou can install BitnamiRubyStack Installers or RubyInstaller. But BitnamiRubyStack always doesnot have latest ruby supported for Win.When you are dealing with RubyEnc...",
    "content": "When you are on Windows machineYou can install BitnamiRubyStack Installers or RubyInstaller. But BitnamiRubyStack always doesnot have latest ruby supported for Win.When you are dealing with RubyEncoder you might need the same Ruby Version in which application is build. So here is the tips how you can switch your ruby versions in your Win Machine.Let say by BitnamiRubyStack your ruby version is already installed to 2.0.x ver and it is deafault version used in your system. Also you had already installed Ruby 2.4.x version using RubyInsaller but it is not the default just installed. So what you need to do is:- Uninstall default ruby 2.0- load the installed ruby 2.4.x bin executable path- check from any dir say c:/&gt;ruby -v , it should display ruby 2.4.2- Now everything is Ok to move aheadYou may get following errors:ERR:C:\\Users\\Siv\\Desktop\\2314\\newtest&gt;bundle installC:/Ruby24-x64/lib/ruby/2.4.0/rubygems/dependency.rb:308:in `to_specs': Could not find 'bundler' (&gt;= 0) among 13 total gem(s) (Gem::MissingSpecError)Checked in 'GEM_PATH=C:/Users/Siv/.gem/ruby/2.4.0;C:/Ruby24-x64/lib/ruby/gems/2.4.0', execute `gem env` for more information        from C:/Ruby24-x64/lib/ruby/2.4.0/rubygems/dependency.rb:320:in `to_spec'        from C:/Ruby24-x64/lib/ruby/2.4.0/rubygems/core_ext/kernel_gem.rb:65:in `gem'To Resolve just rungem install bundlerERR:C:\\Users\\Siv\\Desktop\\2314\\newtest&gt;rails sC:/Ruby24-x64/lib/ruby/2.4.0/rubygems/dependency.rb:308:in `to_specs': Could not find 'railties' (&gt;= 0) among 14 total gem(s) (Gem::MissingSpecError)Checked in 'GEM_PATH=C:/Users/Siv/.gem/ruby/2.4.0;C:/Ruby24-x64/lib/ruby/gems/2.4.0', execute `gem env` for more information        from C:/Ruby24-x64/lib/ruby/2.4.0/rubygems/dependency.rb:320:in `to_spec'        from C:/Ruby24-x64/lib/ruby/2.4.0/rubygems/core_ext/kernel_gem.rb:65:in `gem'        from C:/Ruby200/bin/rails:22:in `&lt;main&gt;'Just run following commandsbundle installAdd the gemPlease add the following to your Gemfile to avoid polling for changes:gem 'wdm', '&gt;= 0.1.0' if Gem.win_platform?"
  },
  
  {
    "title": "Set up docker on Ruby on Rails application",
    "url": "/posts/setup-docker-on-ror/",
    "categories": "Docker, Ruby on Rails",
    "tags": "docker, ruby on rails",
    "date": "2018-07-25 09:23:58 +0545",
    





    
    "snippet": "Docker for Rails: From Development to Deployment (Complete Guide)Docker simplifies building, shipping, and running applications in isolated containers. This guide covers:✅ Running a Rails app with ...",
    "content": "Docker for Rails: From Development to Deployment (Complete Guide)Docker simplifies building, shipping, and running applications in isolated containers. This guide covers:✅ Running a Rails app with Docker✅ Deploying to Docker Hub✅ Running the app on another machine with zero setup1. Setting Up a Rails App with DockerPrerequisites  Docker Desktop installed  Docker Hub accountStep 1: Create a Rails Apprails new docker_rails_demo --database=postgresqlcd docker_rails_demoAdd a Welcome Pagerails generate controller Welcome indexUpdate config/routes.rb:Rails.application.routes.draw do  root 'welcome#index'endStep 2: Docker Configurationignore while pushing the code.dockerignorelist env vars.env1. Write script for installation. Dockerfile# Use official Ruby image (updated to a valid version)FROM ruby:3.2.6-slim# Install dependenciesRUN apt-get update -qq &amp;&amp; apt-get install -y \\    build-essential \\    libpq-dev \\    nodejs \\    postgresql-client \\    &amp;&amp; rm -rf /var/lib/apt/lists/*# Set working directoryWORKDIR /siv_rails_app_dockerdemo# Install gemsCOPY Gemfile Gemfile.lock ./RUN bundle install# Copy application codeCOPY . .# Add a script to be executed every time the container startsCOPY entrypoint.sh /usr/bin/RUN chmod +x /usr/bin/entrypoint.shENTRYPOINT [\"entrypoint.sh\"]# Expose portEXPOSE 3001# Start the main processCMD [\"rails\", \"server\", \"-b\", \"0.0.0.0\"]2. entrypoint.sh#!/bin/bashset -e# Remove a potentially pre-existing server.pid for Railsrm -f /siv_rails_app_dockerdemo/tmp/pids/server.pid# Then exec the container's main process (what's set as CMD in the Dockerfile)exec \"$@\"3. docker-compose.yml  version: \"3.8\"  services:    db:      image: postgres:15      environment:        POSTGRES_PASSWORD: password      volumes:        - postgres_data:/var/lib/postgresql/data      ports:        - \"5432:5432\"    web:      build: .      command: bash -c \"rm -f tmp/pids/server.pid &amp;&amp; rails server -b 0.0.0.0\"      volumes:        - .:/siv_rails_app_dockerdemo      ports:        - \"3001:3000\"      depends_on:        - db      environment:        DATABASE_URL: postgres://postgres:password@db:5432/dockerrails_development  volumes:    postgres_data:Step 3: Run the App# Build containersdocker-compose build# Start servicesdocker-compose upAccess: http://localhost:3001Initialize the Databasedocker-compose exec web rails db:createdocker-compose exec web rails db:migrate2. Deploying to Docker Hub1. Build &amp; Tag the Imagedocker build -t shivrajbadu/docker_rails_demo:1.0 .2. Push to Docker Hubdocker logindocker push shivrajbadu/docker_rails_demo:1.03. Running the App on Another Machine1. Create a New Directorymkdir ~/docker_rails_deploy &amp;&amp; cd ~/docker_rails_deploy2. Add docker-compose.ymlversion: \"3.8\"services:  db:    image: postgres:15    environment:      POSTGRES_PASSWORD: password    volumes:      - postgres_data:/var/lib/postgresql/data  web:    image: shivrajbadu/docker_rails_demo:1.0    command: bash -c \"rm -f tmp/pids/server.pid &amp;&amp; rails server -b 0.0.0.0\"    depends_on:      - db    environment:      DATABASE_URL: postgres://postgres:password@db:5432/docker_rails_demo_production      RAILS_ENV: production    ports:      - \"3002:3000\"volumes:  postgres_data:3. Start the Appdocker-compose upAccess: http://localhost:3002Initialize DB (If Needed)docker-compose exec web rails db:create db:migrateKey Docker Commands Cheat Sheet            Command      Description                  docker-compose --build       Build dependencies              docker-compose up      Start containers. Run the services and processes.              docker-compose down      Stop containers              docker-compose logs web      View Rails logs              docker-compose exec web bash      Enter container shell              docker-compose exec web rails c      Open Rails console              docker ps      List running containers, processes              docker push &lt;image&gt;      Push to Docker Hub              docker container ls      to see container list              docker-compose stop      to stop all the processes for later restart              docker-compose run website rake db:migrate db:seed RAILS_ENV=production      website means name of service to run rails app              docker-compose run --rm website rake db:create db:migrate      website is name of service              sudo docker run -i -t &lt;image/id&gt; /bin/bash      enter into bash shell              docker rmi 24a77bfbb9ee -f      forcefully remove image              docker rm $(docker ps -a -q)      remove all the containers      Note: If you don’t have Docker running on your local machine, you need to replace localhost in the above URL with the IP address of the machine Docker is running on. If you’re using Docker Machine, you can run below cmd to find out the IP.docker-machine ip “${DOCKER_MACHINE_NAME}”"
  },
  
  {
    "title": "Introduction - Ruby!",
    "url": "/posts/introduction-ruby/",
    "categories": "Ruby, Introduction",
    "tags": "ruby, introduction",
    "date": "2018-04-23 09:23:58 +0545",
    





    
    "snippet": "Ruby is a dynamic, open source, server-side scripting, interpreted, reflective, object-oriented, general purpose programming language. It was designed by Yukihiro Matsumato in Japan in the mid-1990...",
    "content": "Ruby is a dynamic, open source, server-side scripting, interpreted, reflective, object-oriented, general purpose programming language. It was designed by Yukihiro Matsumato in Japan in the mid-1990s.Features of Ruby  Ruby was influenced by Perl, Smalltalk, Eiffel, Ada, and Lisp.  Ruby has an automatic  memory management and dynamic type system.  Ruby can run on multiple platforms such as the various versions of Windows, MAC OS and UNIX.  Ruby is free of charge but requires a liscence.  Ruby can be used to write Common Gateway Interface (CGI) scripts.  Ruby can be embedded into HTML.  Ruby written applications can be easily maintainable and scalable.  Ruby can be used for the development of Internet and Intranet applications.  Ruby supports many GUI tools such as Tcl/Tk, GTK, OpenGL.  Ruby can easily connect to DB2, MySQL, Oracle, Sybase.Compile Ruby programCreate a file hello_world.rb and write below code in it  #! /usr/bin/ruby  puts \"Hello World !!\";To run above code first go to the directory where hello_world.rb exists, then run the commandruby hello_world.rb# output isHello World !!In irb interactive command line mode you can run and test following code:&gt;&gt; puts \"Hello, world !!\"Hello, world !!=&gt; nil"
  },
  
  {
    "title": "Association in Ruby on Rails!",
    "url": "/posts/association-in-ruby-on-rails/",
    "categories": "Ruby on Rails, Association",
    "tags": "ruby on rails, association",
    "date": "2018-04-21 09:23:58 +0545",
    





    
    "snippet": "Ruby on Rails AssociationWhen column on database table grows, the column needs to be put into new table if the records emphasizes on data redundancy and data dependency and data normalization takes...",
    "content": "Ruby on Rails AssociationWhen column on database table grows, the column needs to be put into new table if the records emphasizes on data redundancy and data dependency and data normalization takes place. So, various tables are created which are linked or connected with one another via foreign keys. When connection takes place between two associated Active Record Models it is called as an Association.Various Types of Association  One-to-One  One-to-Many  Many-to-Many  Polymorphic One-to-ManyOne-to-OneIn this type of association, the data records contains one instance of another model.Model look like this:class User &lt; ApplicationRecord           class CitizenshipNumber &lt; ApplicationRecord    has_one :citizenship_number             belongs_to :user                        end                                      endTable look like this:  users                      profiles  -----                      ---------  id  (primary key)           id  username                    name  password                    user_id (foreign key)One-to-ManyIn this type of association, instance of first model can have zero or more than one instances of second model and second model belongs to only first model.Model look like this:class User &lt; ApplicationRecord         class Post &lt; ApplicationRecord    has_many :posts                       belongs_to :userend                                    endTable look like this:  users                      posts  -----                      ---------  id  (primary key)           id  username                    title  password                    user_id (foreign key)Many to ManyIt can be handled in two ways: has and belongs to many and has many through relationships.Has and belongs to manyIn this type of association, has_and_belongs_to_many methods is called from both the models in order to create many to many connection with another model. Rails migration need to be created in the following format in order to create join table.rails g migration CreateJoinTableUserPost user postwhich generates migration file like this:class CreateJoinTableUserPost &lt; ActiveRecord::Migration[5.0]  def change    create_join_table :users, :posts do |t|      t.index [:user_id, :post_id]    end  endendModel look like this:class User &lt; ApplicationRecord         class Post &lt; ApplicationRecord  has_and_belongs_to_many :posts         has_and_belongs_to_many :usersend                                    endTable look like this:  users                users_posts                posts  -----                -----------                ---------  id  (primary key)    id                         id  username             user_id (foreign key)      name  password             post_id (foreign key)      Has many throughIn this type of many-to-many association, unlike join intermediate table was created in has_and_belongs_to_many, but join intermediate model is created which points both the associated parent model.Model look like this:class User &lt; ApplicationRecord                class Post &lt; ApplicationRecord  has_many :posts, through: user_posts           has_many :users, through: user_posts  has_many :user_posts                           has_many :user_postsend                                           endclass UserPost &lt; ApplicationRecord    belongs_to :users    belongs_to :postsendTable look like this:  users                user_posts                 posts  -----                -----------                ---------  id  (primary key)    id                         id  username             user_id (foreign key)      name  password             post_id (foreign key)      Polymorphic One-to-ManyIn this type of association, one model is belongs to many different models on a single association. Let’s say Post has video, Article has Video, UserProfile has video, Blog has video. So all these models Post, Article, UserProfile, Blog needs to be handled by polymorphic interface called as Videoable.Model look like this:class Post &lt; ApplicationRecord         class Article &lt; ApplicationRecord  has_many :video, as: :videoable         has_many :video, as: :videoableend                                    endclass UserProfile &lt; ApplicationRecord  class Blog &lt; ApplicationRecord  has_many :video, as: :videoable         has_many :video, as: :videoableend                                    endclass Videoable &lt; ApplicationRecord    belongs_to :videoable, polymorphic: trueendGenerally polymorphic table needs type column (videoable_type: string) and foreign_key column (videoable_id: integer)Table look like this:  videos               Post      Article   Blog    UserProfile      -----                -----     -------   -----   -----------  id  (primary key)     id        id        id      id  videoable_id          title     title     title   full_name  videoable_type Migration:class CreateVideos &lt; ActiveRecord::Migration  def change    create_table :videos do |t|      t.integer :videoable_id      t.string :videoable_type      t.timestamps    end    add_index :videos, :videoable_id  endend# orclass CreateVideos &lt; ActiveRecord::Migration  def change    create_table :videos do |t|      t.references :videoable, polymorphic: true, index: true      t.timestamps    end  endend"
  },
  
  {
    "title": "Introduction - Ruby on Rails!",
    "url": "/posts/introduction-ruby-on-rails/",
    "categories": "Ruby on Rails, Introduction",
    "tags": "ruby on rails, introduction",
    "date": "2018-04-21 09:23:58 +0545",
    





    
    "snippet": "Rails is a server-side web application development framework written in the Ruby programming language. Ruby on Rails help developer to write small to large web applications quickly.Ruby on Rails is...",
    "content": "Rails is a server-side web application development framework written in the Ruby programming language. Ruby on Rails help developer to write small to large web applications quickly.Ruby on Rails is popular among other frameworks because:  Rails provides amazing tools like scaffolding, which helps in developing web applications in very less time.  Ruby on Rails is 100% free as it is open source framework.  It is based on MVC (Model-View-Controller) pattern which is popular among web developers.  RubyGems are the libraries which are available publicly and well documented.  Ruby is easy to learn.  Rails supports integrated testing.  Saves money and time.  Code can be easily maintained.  Huge number of helping communitiesThere are many biggest applications developed from Ruby on Rails like Github, ThemeForest, Groupon, Pixlr, Shopify, Airbnb, etcRails Design Principles:      MVC (Model, View, Controller)    MVC pattern splits an application into three modules a Model, View and Controller. There is “separation of the concerns” among Models, Views and Controllers as each parts has it’s own responsibility.    Model    Model is the layer which interact with the database to retrieve and store the data. You can define the classes in model layer which is used by the application. e.g: Article model is created when you want to develop article functionality. Model also maintains the relationship between the objects and the database and handles validation, association, transaction, etc.    Active Record is the Model in MVC which represents business logic. Business object can be created with the help of Active Record and those object carries persistent data.    View    View layer is the presentation layer which is used to return relevant HTML to be rendered on the users browser.ActionView is the View in Rails MVC which is a part of ActionPack library.    Controller    The controller interacts with the model to retrieve and store data. The retrived data from model will pass to the view. The view returns the resulting HTML to the controller and the controller send this back to the users browser.ActionController is the controller in MVC which handles browser request and acts as channel between Model and View. This is a part of ActionPack library.        DRY - Don’t Repeat Yourself    In this principle, developer have to reduce repetition of codes so that code should be more maintainable, more extensible, less buggy.        Convention over Configuration    This principle allows developer to use default logics and rules used by the framework so that application can be developed in very less time using very few lines of code.For example: rails g Article command will create an Article class and articles table unless developer configure another name. So this convention of framework configuration helps in Rapid application development.  Rails Web MVC Architecture"
  }
  
]

